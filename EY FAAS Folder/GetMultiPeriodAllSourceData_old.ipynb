{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f7db4d24-5933-4c00-9a1e-177c8fb5ff90",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "period_value_ld_df = spark.sql(\"SELECT SUBSTRING(legacyperiod, 1, 6) as period_value_ld from qtdf_assu_faas_frh_uc.frheng1.temp_legacysddates\")\n",
    "period_value_ld_list = [row.period_value_ld for row in period_value_ld_df.collect()]\n",
    "period_value_ld = \",\".join([\"'\" + value + \"'\" for value in period_value_ld_list])\n",
    "print(period_value_ld)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cde1e8c1-3a42-4220-b8a1-637dc74016f2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "a = spark.sql(\"SELECT count(*) FROM qtdf_assu_faas_frh_uc.frheng1.trialbalance WHERE EntityID_fk=6709 and AccountingStdID_fk=1 and IsActive = true  AND Createddate <= '2024-05-20 13:56:10' and SUBSTRING(Period,1,6) IN (\"+period_value_ld+\")\")\n",
    "\n",
    "a.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "042784bf-25be-423d-85c9-ce9f57b7070f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Notebook Tag\n",
    " | Type  | Value | \n",
    " | -----------  | ----------- | \n",
    " | **Description** | Notebook takes data from TB, Adjustment , Disclosure and SCOA Table and writes data for multiperiod which is input data for staging calculations|\n",
    " | **Input Data**| Raw tables from catalog and sql database |\n",
    " | **Output Data**| Combined Data of Dates, Adjustments and Disclosures |  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2665fbc8-331a-44c7-b9bb-226c078f1736",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bd687435-975d-4538-887c-7adde79925b9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col,expr,explode,when,lit,concat,current_timestamp,months_between,date_diff,coalesce,substring,instr, abs,window,row_number,date_add, to_date, left, right, max,split,concat_ws,length,datediff,month, monotonically_increasing_id, regexp_extract, first, date_format\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import (StructType,StructField,StringType,IntegerType,TimestampType,DoubleType,DateType,DecimalType,BooleanType,FloatType)\n",
    "from pyspark.sql.window import Window\n",
    "from functools import reduce\n",
    "from collections import defaultdict\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "from pyspark.sql.functions import from_json\n",
    "import logging\n",
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7a7e3357-3940-4200-9738-ebf3d7d84d31",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#dbutils.widgets.removeAll()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "43f759e6-9b8e-4a9b-9914-ec9c564753bf",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 2. Parameters & Variables\n",
    "##### Widgets to pass as parameters from ADF pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a2ec45be-a8ff-405e-9069-ea4606811436",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# dbutils.widgets.text(\"EntityID\", \"50\")\n",
    "# dbutils.widgets.text(\"Bunit\", \"0\")\n",
    "# dbutils.widgets.text(\"CategoryID\", \"1488\")\n",
    "# dbutils.widgets.text(\"FinancialYear\", \"2025\")\n",
    "# dbutils.widgets.text(\"AccountingStandardID\",\"5\")  \n",
    "# # dbutils.widgets.text(\"PeriodValue\",\"Year-202204-202303\")  # to test extended period less than 12 months Dimension entity id : 3052, catgeoryid: 487, acc startard id : 1, finacial year : 2023\n",
    "# # dbutils.widgets.text(\"PeriodValue\",\"Year-202204-202303\")  # to test standard period Dimension entity id : 3050, catgeoryid: 395, acc startard id : 41, finacial year : 2023\n",
    "# # dbutils.widgets.text(\"PeriodValue\",\"Year-202110-202301\")  # to test extended period more than 12 months Dimension entityid: 3051, acc stadtrd id :41 , category id : 437, year : 2023\n",
    "# # dbutils.widgets.text(\"PeriodValue\",\"Year-202302-202401\")  # to test not an extended period Dimension entityid: 3051, acc stadtrd id :41 , category id : 438, year : 2024\n",
    "# dbutils.widgets.text(\"PeriodValue\",\"Year-202404-202503\") # to test reporting control table, entity id : 50 , categoryid : 1488, financial year : 2025, Acc id : 5,Amount : Million,RoundTypeLevel: Child level,\n",
    "# dbutils.widgets.text(\"ComparitivePeriodValue\",'')\n",
    "# dbutils.widgets.text(\"AmountsIn\",\"FCU\")\n",
    "# # dbutils.widgets.text(\"AmountsIn\",\"Million\") \n",
    "# dbutils.widgets.text(\"Roundoff\",\"0\")\n",
    "# # dbutils.widgets.text(\"RoundTypeLevel\",\"GL code level\")\n",
    "# dbutils.widgets.text(\"RoundTypeLevel\",\"Child level\")\n",
    "# dbutils.widgets.text(\"IsDrillDown\",\"1\")\n",
    "# # dbutils.widgets.text(\"Flag\", \"1\")\n",
    "\n",
    "# # dbutils.widgets.text(\"Dim\", '{\"BusinessUnit\": \"BU1,BU2\"}') # not an extended\n",
    "# # dbutils.widgets.text(\"Dim\", '{\"BusinessUnit\": \"BU1,BU2\", \"Segmentupdate\": \"Segm1\"}')\n",
    "# # dbutils.widgets.text(\"Dim\", '{\"ServiceLinechange\": \"SL1\", \"Department\": \"Dept2\"}') # standard period for 105 and 103\n",
    "# # dbutils.widgets.text(\"Dim\", '{\"Geography\": \"India,Delhi\"}') # less than 12 months\n",
    "# # dbutils.widgets.text(\"Dim\", '{\"Geography\": \"India,Delhi\", \"ProfitCentre\": \"PF1\"}') # more than 12 months\n",
    "\n",
    "# EntityID = dbutils.widgets.get(\"EntityID\")\n",
    "# Bunit = dbutils.widgets.get(\"Bunit\")\n",
    "# CategoryID = dbutils.widgets.get(\"CategoryID\")\n",
    "# FinancialYear = dbutils.widgets.get(\"FinancialYear\")\n",
    "# AccountingStandardID = dbutils.widgets.get(\"AccountingStandardID\")\n",
    "# PeriodValue = dbutils.widgets.get(\"PeriodValue\")\n",
    "# ComparitivePeriodValue_param = dbutils.widgets.get(\"ComparitivePeriodValue\")\n",
    "# AmountsIn = dbutils.widgets.get(\"AmountsIn\")\n",
    "# Roundoff = dbutils.widgets.get(\"Roundoff\")\n",
    "# RoundTypeLevel = dbutils.widgets.get(\"RoundTypeLevel\")\n",
    "# IsDrillDown = dbutils.widgets.get(\"IsDrillDown\")\n",
    "# # Flag = dbutils.widgets.get(\"Flag\")\n",
    "# # Dim = dbutils.widgets.get(\"Dim\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "302284c3-06a0-4e8d-96cf-24298382b5dc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../../../BackgroundServices/src/FactIncrementalLoad/Configuration_dynamic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f06e31e0-232e-48ec-95e2-2cebb558345c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "engagement = dbutils.widgets.text(\"engagement\",\"TenantDB1Config1\")\n",
    "environment = dbutils.widgets.text(\"environment\",\"Development\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "046b484d-cbde-4677-a224-cb9dd78bac59",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "configuration = Configuration(engagement)\n",
    "# print(configuration)\n",
    "adls_path = f\"abfss://{configuration.container}@{configuration.storageaccountname}.dfs.core.windows.net\"\n",
    "folder_path = \"/DataUpload/Multiproc/\"\n",
    "catalog_name = 'dtdf_assu_faas_uc'\n",
    "catalog_schema_name = 'dtdf_assu_faas_uc.frh'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9b45c65f-6ae4-4f55-93f7-39b49467ad0f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # Need to remove\n",
    "# def sql_read_table(table_name: str):    \n",
    "#     \"\"\"\n",
    "#     Read a Delta table into a PySpark DataFrame.\n",
    "\n",
    "#     :param path: The path to the Delta table.\n",
    "#     :param format: The format of the table.\n",
    "#     :return: A PySfgpark DataFrame containing the data from the Delta table.\n",
    "#     \"\"\"\n",
    "#     username = \"apsSqlDIGIQAAdmin\"\n",
    "#     password = \"4&4skBzqt7Q32@k5afHmG2$!\"\n",
    "#     host = \"apsqdigsql01.database.windows.net\"\n",
    "#     try:\n",
    "#         df = spark.read.format(\"sqlserver\") \\\n",
    "#     .option(\"host\", \"apsqdigsql01.database.windows.net\") \\\n",
    "#     .option(\"port\", \"1433\") \\\n",
    "#     .option(\"user\", username) \\\n",
    "#     .option(\"password\", password) \\\n",
    "#     .option(\"database\", \"Dev1ConfigDB\") \\\n",
    "#     .option(\"dbtable\", table_name) \\\n",
    "#     .load()\n",
    "#         print(f\"SQL table {table_name} data read is successful!\")\n",
    "#         return df\n",
    "#     except Exception as e:\n",
    "#         print(f\"An error occurred: {str(e)}\")\n",
    "#         error_message = f\"An error occurred while reading table {table_name}: {str(e)}\"\n",
    "#         logging.error(error_message)\n",
    "#         raise Exception(\"[Error] Reading source table data Failed ...\", e)\n",
    "#         return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "41100783-50b8-4d7a-9a14-95106edd0bd2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 3. Read Data from source tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f14eac05-d0cf-46ae-8c58-640bece310dc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "trialbalance_tbl = \"trialbalance\"\n",
    "castingadjustment_tbl = \"castingadjustmenttable\"\n",
    "legacyadjustment_tbl = \"legacyadjustment\"\n",
    "adjustments_tbl = \"adjustments\"\n",
    "custom_dim_master_tbl = \"custom_dim_master\"\n",
    "dimensiontable_tbl = \"dimensiontable\"\n",
    "scoa_tbl = \"scoa\"\n",
    "scoamappingtable_tbl = \"scoamappingtable\"\n",
    "reportingparameters_tbl = \"reportingparameters\"\n",
    "executioncontrol_tbl = \"executioncontroltbl\"\n",
    "reportrunlog_tbl = \"reportrunlog\"\n",
    "drilldown_tbl = get_tbl_nm('multiperiod_drilldown_'+str(parameterID))\n",
    "staging_tbl = get_tbl_nm('multiperiod_staging_'+str(parameterID))\n",
    "\n",
    "trialbalance = get_tbl_nm(trialbalance_tbl)\n",
    "trialbalance_src_uc_df = spark.sql(f\"SELECT * FROM {trialbalance}\")\n",
    "castingadjustment = get_tbl_nm(castingadjustment_tbl)\n",
    "castingadjustment_src_uc_df = spark.sql(f\"SELECT * FROM {castingadjustment}\")\n",
    "legacyadjustment = get_tbl_nm(legacyadjustment_tbl)\n",
    "legacyadjustments_src_uc_df = spark.sql(f\"SELECT * FROM {legacyadjustment}\")\n",
    "adjustments = get_tbl_nm(adjustments_tbl)\n",
    "standaloneadjustments_src_uc_df = spark.sql(f\"SELECT * FROM {adjustments}\")\n",
    "custom_dim_master = get_tbl_nm(custom_dim_master_tbl)\n",
    "customdimmaster_src_uc_df = spark.sql(f\"SELECT * FROM {custom_dim_master}\")\n",
    "dimensiontable = get_tbl_nm(dimensiontable_tbl)\n",
    "dimensiontable_src_uc_df = spark.sql(f\"SELECT * FROM {dimensiontable}\")\n",
    "scoa = get_tbl_nm(scoa_tbl)\n",
    "scoa_src_uc_df = spark.sql(f\"SELECT * FROM {scoa}\")\n",
    "scoamappingtable = get_tbl_nm(scoamappingtable_tbl)\n",
    "masterscoamapping_src_uc_df = spark.sql(f\"SELECT * FROM {scoamappingtable}\")\n",
    "reportingparameters = get_tbl_nm(reportingparameters_tbl)\n",
    "reportingparameters_src_uc_df = spark.sql(f\"SELECT * FROM {reportingparameters}\")\n",
    "executioncontrol = get_tbl_nm(executioncontrol_tbl)\n",
    "executioncontrol_src_uc_df = spark.sql(f\"SELECT * FROM {executioncontrol}\")\n",
    "reportrunlog = get_tbl_nm(reportrunlog_tbl)\n",
    "reportrunlog_src_uc_df = spark.sql(f\"SELECT * FROM {reportrunlog}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "38561d2a-c088-4558-b336-6e6a5fcaef2a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# trialbalance_src_uc_df = spark.read.table(\"dtdf_assu_faas_uc.frh.trialbalance\")\n",
    "# castingadjustment_src_uc_df = spark.read.table(\"dtdf_assu_faas_uc.frh.castingadjustmenttable\")\n",
    "# legacyadjustments_src_uc_df = spark.read.table(\"dtdf_assu_faas_uc.frh.legacyadjustment\")\n",
    "# standaloneadjustments_src_uc_df = spark.read.table(\"dtdf_assu_faas_uc.frh.adjustments\")\n",
    "# customdimmaster_src_uc_df =  spark.read.table(\"dtdf_assu_faas_uc.frh.custom_dim_master\")\n",
    "# dimensiontable_src_uc_df = spark.read.table(\"dtdf_assu_faas_uc.frh.dimensiontable\")\n",
    "# scoa_src_uc_df = spark.read.table(\"dtdf_assu_faas_uc.frh.scoa\")\n",
    "# masterscoamapping_src_uc_df = spark.read.table(\"dtdf_assu_faas_uc.frh.scoamappingtable\")\n",
    "# reportingparameters_src_uc_df = spark.read.table(\"dtdf_assu_faas_uc.frh.reportingparameters\")\n",
    "# executioncontroltbl_src_uc_df = spark.read.table(\"dtdf_assu_faas_uc.frh.executioncontroltbl\")\n",
    "# reportrunlog_src_uc_df = spark.read.table(\"dtdf_assu_faas_uc.frh.reportrunlog\")\n",
    "periodicaldatesforextensionperiod_src_df = sql_read_table(\"master.PeriodicalDatesForExtensionPeriod\")\n",
    "entityperioddetails_src_df = sql_read_table(\"master.EntityPeriodDetails\")\n",
    "periodicaldates_src_df = sql_read_table(\"master.PeriodicalDates\")\n",
    "entitymaster_src_df = sql_read_table(\"master.entitymaster\")\n",
    "financialcycle_src_df = sql_read_table(\"master.financialcycle\")\n",
    "categorydropdown_src_df = sql_read_table(\"master.categorydropdown\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c1ecb3b4-43e3-4e32-8d19-33f5db23444c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 4. Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d96ad261-b87d-4506-901b-ea7dbe16c928",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "PeriodValuePrefix = PeriodValue.split(\"-\")[0][:1]\n",
    "if PeriodValuePrefix in ('Y','9','H','Q','M') :\n",
    "    PeriodValueInYHQM=1\n",
    "else :\n",
    "    PeriodValueInYHQM=0\n",
    "print(\"PeriodValuePrefix:\",PeriodValuePrefix)\n",
    "print(\"PeriodValueInYHQM:\",PeriodValueInYHQM)\n",
    "\n",
    "if ComparitivePeriodValue_param is None:\n",
    "    print(\"ComparitivePeriodValue is None\")\n",
    "elif len(ComparitivePeriodValue_param) == 0:\n",
    "    ComparitivePeriodValue = None\n",
    "    print(\"ComparitivePeriodValue is Empty\")\n",
    "else:\n",
    "    print(\"ComparitivePeriodValue is not empty\")\n",
    "    ComparitivePeriodValue = ComparitivePeriodValue_param\n",
    "print(ComparitivePeriodValue)\n",
    "\n",
    "entity_result = entitymaster_src_df.filter(entitymaster_src_df[\"entitymasterid\"] == EntityID).select(\n",
    "    \"fk_fiscalperiodid\",\n",
    "    \"fk_functionalcurrencyid\",\n",
    "    \"entityname\",\n",
    "    \"fk_fiscalperiodid\"\n",
    ").first()\n",
    "\n",
    "# display(entity_result)\n",
    "\n",
    "# Assigning values to variables\n",
    "FK_FiscalPeriodID = entity_result[\"fk_fiscalperiodid\"]\n",
    "SDCurrencyid = entity_result[\"fk_functionalcurrencyid\"]\n",
    "EntitySDName = entity_result[\"entityname\"]\n",
    "FinancialSDCycleID = entity_result[\"fk_fiscalperiodid\"]\n",
    "print(\"FK_FiscalPeriodID:\",FK_FiscalPeriodID)\n",
    "print(\"SDCurrencyid:\",SDCurrencyid)\n",
    "print(\"EntitySDName:\",EntitySDName)\n",
    "print(\"FinancialSDCycleID:\",FinancialSDCycleID)\n",
    "# # Selecting values from financial_cycle_df\n",
    "fiscal_cycle_result = financialcycle_src_df.filter(financialcycle_src_df[\"financialcycleid\"] == FK_FiscalPeriodID).select(\n",
    "    \"financialcycle\",\n",
    "    \"financialcycleid\"\n",
    ").first()\n",
    "\n",
    "# # Assigning values to variables\n",
    "FiscalPeriod = fiscal_cycle_result[\"financialcycle\"]\n",
    "FiscalPeriodID = fiscal_cycle_result[\"financialcycleid\"]\n",
    "print(\"FiscalPeriod:\",FiscalPeriod)\n",
    "print(\"FiscalPeriodID:\",FiscalPeriodID)\n",
    "\n",
    "CategorySDName = categorydropdown_src_df.filter(col(\"categoryid\") == CategoryID).select(\"categoryname\").first()\n",
    "CategorySDName = CategorySDName[\"categoryname\"]\n",
    "print(\"CategorySDName:\",CategorySDName)\n",
    "\n",
    "extendedmonthdiff_result = entityperioddetails_src_df \\\n",
    ".filter(col(\"Fk_EntityMasterID\") == EntityID) \\\n",
    "    .select(expr(\n",
    "        \"MONTH(endDate) - MONTH(startDate) + 1 + (YEAR(endDate) - YEAR(startDate)) * 12\"\n",
    "    ).alias(\"ExtendedMonthDiff\")).collect()\n",
    "    \n",
    "    \n",
    "if extendedmonthdiff_result:\n",
    "    extendedmonthdiff = extendedmonthdiff_result[0][\"ExtendedMonthDiff\"]\n",
    "    # Use extendedmonthdiff variable for further processing\n",
    "else:\n",
    "    extendedmonthdiff = 0\n",
    "print(\"extendedmonthdiff:\",extendedmonthdiff)\n",
    "\n",
    "extflag = entityperioddetails_src_df.filter((col(\"fk_entitymasterid\") == EntityID) & (col(\"isExtendedPeriod\") == 1)).count() > 0\n",
    "extflag = int(extflag)\n",
    "print(\"extflag:\",extflag)\n",
    "extvalue = PeriodValue.split('-')[0]\n",
    "\n",
    "print(\"extvalue:\",extvalue)\n",
    "extendedcycleyear_result = (\n",
    "    periodicaldatesforextensionperiod_src_df\n",
    "    .filter((col(\"Entityid\") == EntityID) & (col(\"Value\") == extvalue) & (col(\"FinancialCycleID\") == FiscalPeriodID))\n",
    "    .select(when(col(\"Year\").isNotNull(), col(\"Year\")).otherwise(\"\").alias(\"ExtendedCycleYear\"))\n",
    "    .first()\n",
    ")\n",
    "# \n",
    "\n",
    "if extendedcycleyear_result:\n",
    "    extendedcycleyear = extendedcycleyear_result['ExtendedCycleYear']\n",
    "    # Use extendedmonthdiff variable for further processing\n",
    "else:\n",
    "    extendedcycleyear = extendedcycleyear_result\n",
    "\n",
    "print(\"extendedcycleyear:\",extendedcycleyear)\n",
    "\n",
    "if extendedcycleyear is None:\n",
    "    extendedcycleyear = 0\n",
    "\n",
    "print(\"extendedcycleyear:\",extendedcycleyear)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dcf65531-30c6-4249-9849-b5d86985b148",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Get ParameterID, UploadChangedate and LastExecutionDate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d919a85c-59a8-4e26-9070-c0620c38cac9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "def get_parameterID(reportingparameters_src_uc_df):\n",
    "    try:\n",
    "        reportingparameters_df = reportingparameters_src_uc_df.where(\n",
    "                                (col(\"EntityID_fk\") == EntityID) &\n",
    "                                (col(\"CategoryId_fk\") == CategoryID) &\n",
    "                                (col(\"Year\") == FinancialYear) &\n",
    "                                (col(\"AccountingStandard_fk\") == AccountingStandardID) &\n",
    "                                (col(\"Period\") == PeriodValue) &\n",
    "                                (col(\"ComparativePeriodValue\") == ComparitivePeriodValue_param) &\n",
    "                                (col(\"AmountsIn\") == AmountsIn) &\n",
    "                                (col(\"RoundUpto\") == Roundoff)  &\n",
    "                                (col(\"RoundUptoLevel\") == RoundTypeLevel))\n",
    "        parameterID = reportingparameters_df.select(\"ParameterID\").first()[0]\n",
    "        print(\"parameterID: \",parameterID)\n",
    "        return parameterID\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "        error_message = f\"No ParameterID found for given {EntityID}\"\n",
    "        logging.error(error_message)\n",
    "        raise Exception(\"[Error] No ParameterID found for given {EntityID} ...\", e)\n",
    "        return None\n",
    "'''    \n",
    "def get_uploadchangedate(executioncontrol_src_uc_df,parameterID):\n",
    "    try:\n",
    "        executioncontrol_df = executioncontrol_src_uc_df.filter((col(\"ParameterID_fk\") == parameterID) & (col(\"Status\") ==  \"Pending\"))\n",
    "        if executioncontrol_df.count()==0:\n",
    "\n",
    "            return  datetime.strptime(\"9999-12-25\", \"%Y-%m-%d\") #datetime(\"9999-99-99 09:15:23.359000\")\n",
    "        \n",
    "        else:\n",
    "            print(\" THIS SCENERIO WILL NEVER COME\")\n",
    "                       \n",
    "    \n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "        error_message = f\"No UploadChangeDate found for given {EntityID}\"\n",
    "        logging.error(error_message)\n",
    "        raise Exception(\"[Error] No UploadChangeDate found for given {EntityID} ...\", e)\n",
    "        return None\n",
    "\n",
    "def insertReportRunLog(reportrunlog_src_uc_df,reportrunlog,parameterID,CategoryID):\n",
    "    #code changes\n",
    "    if reportrunlog_src_uc_df.where(F.col(\"ParameterID_fk\")==parameterID).count()==0:\n",
    "        print(\"new parameterid FIRST TIME \",parameterID)\n",
    "        spark.sql(f'''\n",
    "                    INSERT into {reportrunlog} (ParameterID_fk, Categoryid_fk, ExecutionTime,ExecutionDate,ExecutedBy,IsReRun)\n",
    "                        VALUES ({parameterID}, {CategoryID}, current_timestamp , current_timestamp,'System',true)\n",
    "                                    ''')\n",
    "    else:\n",
    "        print(\"parameterid already present\")\n",
    "        \n",
    "\n",
    "# def get_LastExecutionDate(reportrunlog_src_uc_df,parameterID):\n",
    "#     try:\n",
    "#         reportrunlog_df = reportrunlog_src_uc_df.filter(col(\"ParameterID_fk\") == parameterID)\n",
    "#         if reportrunlog_df.count()==0:\n",
    "#             print(\"inside if of get_LastExecutionDate fxn \")\n",
    "#             insertReportRunLog(reportrunlog_src_uc_df,reportrunlog,parameterID,CategoryID)\n",
    "#             LastExecutionDate = reportrunlog_df.select(\"ExecutionTime\").first()[0]\n",
    "#             return LastExecutionDate \n",
    "#             #datetime.strptime(\"2000-01-01\", \"%Y-%m-%d\") #datetime(\"2000-01-01 09:15:23.359000\")\n",
    "#         else:\n",
    "#             print(\"inside ELSE of get_LastExecutionDate fxn \")\n",
    "#             # reportrunlog_df.display()\n",
    "#             # reportrunlog_df = reportrunlog_df.withColumn(\"ExecutionDate\",lit(\"2024-04-15 09:15:23.359000\")) # Need to delete later once data is available \n",
    "#             max_execution_date_row = reportrunlog_df.select(\"ExecutionDate\").orderBy(col(\"ExecutionDate\").desc()).first()\n",
    "#             LastExecutionDate = max_execution_date_row['ExecutionDate']\n",
    "#             print(\"LastExecutionDate: \",LastExecutionDate)\n",
    "#             return LastExecutionDate\n",
    "#     except Exception as e:\n",
    "#         print(f\"An error occurred: {str(e)}\")\n",
    "#         error_message = f\"No LastExecutionDate found for given {EntityID}\"\n",
    "#         logging.error(error_message)\n",
    "#         raise Exception(\"[Error] No LastExecutionDate found for given {EntityID} ...\", e)\n",
    "#         return None\n",
    "\n",
    "def get_LastExecutionDate(reportrunlog_src_uc_df,executioncontrol_src_uc_df,parameterID):\n",
    "    try:\n",
    "        reportrunlog_df = reportrunlog_src_uc_df.filter(col(\"ParameterID_fk\") == parameterID)\n",
    "        if reportrunlog_df.count()==0:\n",
    "\n",
    "            print(\"inside if of get_LastExecutionDate fxn \")\n",
    "            executioncontrol_df = executioncontrol_src_uc_df.filter((col(\"ParameterID_fk\") == parameterID) & (col(\"Status\") ==  \"Pending\"))\n",
    "            LastExecutionDate = executioncontrol_df.select(\"ExecutionDateTime\").orderBy(col(\"ExecutionDateTime\").desc()).collect()[0][\"ExecutionDateTime\"]\n",
    "            print(\"LastExecutionDate: \",LastExecutionDate)\n",
    "            # insertReportRunLog(reportrunlog_src_uc_df,reportrunlog,parameterID,CategoryID)                  \n",
    "            return  LastExecutionDate #datetime(\"2000-01-01 09:15:23.359000\")\n",
    "        else:\n",
    "            print(\"inside ELSE of get_LastExecutionDate fxn \")\n",
    "\n",
    "            # reportrunlog_df.display()\n",
    "            # reportrunlog_df = reportrunlog_df.withColumn(\"ExecutionDate\",lit(\"2024-04-15 09:15:23.359000\")) # Need to delete later once data is available \n",
    "            max_execution_date_row = reportrunlog_df.select(\"ExecutionDate\").orderBy(col(\"ExecutionDate\").desc()).first()\n",
    "            LastExecutionDate = max_execution_date_row['ExecutionDate']\n",
    "            print(\"LastExecutionDate: \",LastExecutionDate)\n",
    "            return LastExecutionDate\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "        error_message = f\"No LastExecutionDate found for given {EntityID}\"\n",
    "        logging.error(error_message)\n",
    "        raise Exception(\"[Error] No LastExecutionDate found for given {EntityID} ...\", e)\n",
    "        return None\n",
    "    \n",
    "#parameterID = get_parameterID(reportingparameters_src_uc_df)\n",
    "\n",
    "#UploadChangeDate = get_uploadchangedate(executioncontrol_src_uc_df,parameterID) \n",
    "LastExecutionDate = get_LastExecutionDate(reportrunlog_src_uc_df,executioncontrol_src_uc_df,parameterID)\n",
    "#print(\"UploadChangeDate\",UploadChangeDate)\n",
    "print(\"LastExecutionDate\",LastExecutionDate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4c72be19-f925-44f2-812a-ec4915443573",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## REMOVE IT\n",
    "parameterID = 119\n",
    "executioncontrol_tbl = \"executioncontroltbl\"\n",
    "reportrunlog_tbl = \"reportrunlog\"\n",
    "executioncontrol = get_tbl_nm(executioncontrol_tbl)\n",
    "executioncontrol_src_uc_df = spark.sql(f\"SELECT * FROM {executioncontrol}\")\n",
    "reportrunlog = get_tbl_nm(reportrunlog_tbl)\n",
    "reportrunlog_src_uc_df = spark.sql(f\"SELECT * FROM {reportrunlog}\")\n",
    "# reportrunlog_df = reportrunlog_src_uc_df.filter(col(\"ParameterID_fk\") == parameterID)\n",
    "# if reportrunlog_df.count()==0:\n",
    "print(\"inside if of get_LastExecutionDate fxn \")\n",
    "executioncontrol_df = executioncontrol_src_uc_df.filter((col(\"ParameterID_fk\") == parameterID) & (col(\"Status\") ==  \"Pending\"))\n",
    "LastExecutionDate = executioncontrol_df.select(\"ExecutionDateTime\").orderBy(col(\"ExecutionDateTime\").desc()).collect()[0][\"ExecutionDateTime\"]\n",
    "print(\"LastExecutionDate: \",LastExecutionDate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6af28a41-5675-4527-937a-bacecb4b13aa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "alldimcols=[col_name for col_name in trialbalance_src_uc_df.columns if trialbalance_src_uc_df.select(col(col_name)).na.drop().count() > 0 if col_name.startswith(\"DimType\")]\n",
    "\n",
    "# udf to include dimesion columns dynamically\n",
    "def add_dimension_columns(alldimcols, existing_schema):\n",
    "    dim_cols = []\n",
    "    processed_columns = set()\n",
    "\n",
    "    for column in alldimcols:\n",
    "        if column not in processed_columns:\n",
    "            processed_columns.add(column)\n",
    "            if column not in [field.name for field in existing_schema.fields]:\n",
    "                dim_cols.append(StructField(column, IntegerType(), True))\n",
    "    \n",
    "    return StructType(existing_schema.fields + dim_cols)\n",
    "\n",
    "schema_daterangeconverttoperiod= StructType([ \\\n",
    "    StructField(\"currentperiod\",StringType(),True), \\\n",
    "    StructField(\"startdate\",DateType(),True), \\\n",
    "    StructField(\"enddate\", DateType(), True), \\\n",
    "    StructField(\"periodstart\",StringType(),True), \\\n",
    "    StructField(\"monthdiff\",IntegerType(),True), \\\n",
    "    StructField(\"periodmonthvalue\",StringType(),True), \\\n",
    "    StructField(\"tbperiodvalue\", StringType(), True)\n",
    "  ])\n",
    " \n",
    "df_daterangeconverttoperiod = spark.createDataFrame(data=[],schema=schema_daterangeconverttoperiod)\n",
    "\n",
    "schema_daterangecustomperiod = StructType([ \\\n",
    "    StructField(\"id\",IntegerType(),True),\n",
    "    StructField(\"currentperiod\",StringType(),True), \\\n",
    "    StructField(\"startdate\",DateType(),True), \\\n",
    "    StructField(\"enddate\", DateType(), True), \\\n",
    "    StructField(\"periodstart\",StringType(),True), \\\n",
    "    StructField(\"monthdiff\",IntegerType(),True), \\\n",
    "    StructField(\"periodmonthvalue\",StringType(),True), \\\n",
    "    StructField(\"tbperiodvalue\", StringType(), True),\\\n",
    "    StructField(\"year\",StringType(),True), \\\n",
    "    StructField(\"periodvalue\",StringType(),True), \\\n",
    "    StructField(\"customperiod\",StringType(),True), \\\n",
    "    StructField(\"customdaterange\", IntegerType(), True)\n",
    "  ])\n",
    " \n",
    "df_daterangecustomperiod = spark.createDataFrame(data=[],schema=schema_daterangecustomperiod)\n",
    "\n",
    "schema_daterangetoperiod = StructType([ \\\n",
    "    StructField(\"id\",IntegerType(),True),\n",
    "    StructField(\"currentperiod\",StringType(),True), \\\n",
    "    StructField(\"startdate\",DateType(),True), \\\n",
    "    StructField(\"enddate\", DateType(), True), \\\n",
    "    StructField(\"periodstart\",StringType(),True), \\\n",
    "    StructField(\"monthdiff\",IntegerType(),True), \\\n",
    "    StructField(\"periodmonthvalue\",StringType(),True), \\\n",
    "    StructField(\"tbperiodvalue\", StringType(), True),\\\n",
    "    StructField(\"year\",StringType(),True), \\\n",
    "    StructField(\"periodvalue\",StringType(),True), \\\n",
    "    StructField(\"customperiod\",StringType(),True), \\\n",
    "    StructField(\"customdaterange\", IntegerType(), True),\n",
    "    StructField(\"openperiodvalue\", StringType(), True), \\\n",
    "    StructField(\"opencyclevalue\", StringType(), True) \\\n",
    "  ])\n",
    "\n",
    "schema_daterangetoperiod1 = StructType([ \\\n",
    "    StructField(\"id\",IntegerType(),True),\n",
    "    StructField(\"cpid\",StringType(),True), \\\n",
    "    StructField(\"currentperiod\",StringType(),True), \\\n",
    "    StructField(\"startdate\",DateType(),True), \\\n",
    "    StructField(\"enddate\", DateType(), True), \\\n",
    "    StructField(\"periodstart\",StringType(),True), \\\n",
    "    StructField(\"monthdiff\",IntegerType(),True), \\\n",
    "    StructField(\"periodmonthvalue\",StringType(),True), \\\n",
    "    StructField(\"tbperiodvalue\", StringType(), True),\\\n",
    "    StructField(\"year\",StringType(),True), \\\n",
    "    StructField(\"periodvalue\",StringType(),True), \\\n",
    "    StructField(\"customperiod\",StringType(),True), \\\n",
    "    StructField(\"customdaterange\", IntegerType(), True),\n",
    "    StructField(\"openperiodvalue\", StringType(), True), \\\n",
    "    StructField(\"opencyclevalue\", StringType(), True) \\\n",
    "  ])\n",
    " \n",
    "df_daterangetoperiod = spark.createDataFrame(data = [],schema=schema_daterangetoperiod)\n",
    "\n",
    "schema_df_usergivenmulticomp = StructType([\n",
    "  StructField(\"compperiodname\", StringType(), True),  \n",
    "  StructField(\"compperiodvalue\", StringType(), True),\n",
    "  StructField(\"comptype\", StringType(), True),\n",
    "  StructField(\"compperiod\", StringType(), True),\n",
    "  StructField(\"Value\", StringType(), True)])\n",
    "df_usergivenmulticomp= spark.createDataFrame([],schema = schema_df_usergivenmulticomp)\n",
    "\n",
    "schema_df_compperioddetails = StructType([\n",
    "StructField(\"comptype\", StringType(), True),\n",
    "StructField(\"alias\", StringType(), True)])\n",
    "df_compperioddetails= spark.createDataFrame([],schema = schema_df_compperioddetails)\n",
    "\n",
    "\n",
    "schema_df_legacysddates = StructType([\n",
    "  #StructField('id', IntegerType(), True),\n",
    "  StructField('periodtype', StringType(), True),  \n",
    "  StructField('Value', StringType(), True),\n",
    "  StructField('pcurrentvalue', StringType(), True),\n",
    "  StructField('currentvalue', StringType(), True),\n",
    "  StructField('legacyperiod', StringType(), True),\n",
    "  StructField('categoryid', IntegerType(), True),\n",
    "  StructField('financialyear', IntegerType(), True),\n",
    "  StructField('openingperiodvalue', StringType(), True),\n",
    "  StructField('castingperiod', StringType(), True),\n",
    "  StructField('numericvalueofperiod', IntegerType(), True),\n",
    "  StructField('ftpperiod', IntegerType(), True),\n",
    "  StructField('periodformat', StringType(), True),\n",
    "  StructField('p', StringType(), True),\n",
    "  StructField('ppp', StringType(), True),\n",
    "  StructField('py', StringType(), True),\n",
    "  StructField('pypp', StringType(), True),\n",
    "  StructField('ppy', StringType(), True),\n",
    "  StructField('p3y', StringType(), True),\n",
    "  StructField('p4y', StringType(), True)\n",
    "  ])\n",
    "df_legacysddates= spark.createDataFrame([],schema = schema_df_legacysddates)\n",
    "\n",
    "allperiodftpscoa_schema = StructType([\n",
    "    StructField(\"SCOAID\", IntegerType(), nullable=True),\n",
    "    StructField(\"Member\", StringType(), nullable=True),\n",
    "    StructField(\"Path\", StringType(), nullable=True),\n",
    "    StructField(\"Column27\", StringType(), nullable=True),\n",
    "    StructField(\"AccountSubType\", StringType(), nullable=True),\n",
    "    StructField(\"UltimateParent\", StringType(), nullable=True)\n",
    "])\n",
    "\n",
    "SCOAMappingTable_schema = StructType([\n",
    "    StructField(\"EntityId\", IntegerType(), False),\n",
    "    StructField(\"Period\", StringType(), False),\n",
    "    StructField(\"Accountingtype\", IntegerType(), False),\n",
    "    StructField(\"EYGLCode\", StringType(), False),\n",
    "    StructField(\"UserGLCode\", StringType(), False),\n",
    "    StructField(\"CategoryID\", IntegerType(), True),\n",
    "    StructField(\"UltimateParent\", StringType(), False),\n",
    "    StructField(\"Unit\", StringType(), True),\n",
    "    StructField(\"CreatedBy\", StringType(), True),\n",
    "    StructField(\"ModifiedBy\", StringType(), True),\n",
    "    StructField(\"CreatedDate\", StringType(), True),\n",
    "    StructField(\"ModifiedDate\", StringType(), True),\n",
    "    StructField(\"PresentationLayer\", StringType(), True),\n",
    "    StructField(\"UserGLDescription\", StringType(), True)\n",
    "])\n",
    "\n",
    "legacydatesperiodtypelite_schema = StructType([\n",
    "    StructField(\"pcurrentvalue\", StringType(), True),\n",
    "    StructField(\"PeriodType\", StringType(), True)\n",
    "])\n",
    "\n",
    "legacydatesnotperiodtypelite_schema = StructType([\n",
    "    StructField(\"pcurrentvalue\", StringType(), True),\n",
    "    StructField(\"PeriodType\", StringType(), True)\n",
    "])\n",
    "\n",
    "\n",
    "priorityseq_schema = StructType([\n",
    "    StructField(\"Period\", StringType(), True),\n",
    "    StructField(\"stdperiod\", StringType(), True),\n",
    "    StructField(\"PeriodType\", StringType(), True),\n",
    "    StructField(\"closing\", StringType(), True),\n",
    "    StructField(\"legacyperiod\", StringType(), True),\n",
    "    StructField(\"numericvalueofperiod\", IntegerType(), True),\n",
    "    StructField(\"priorityseq\", IntegerType(), True),\n",
    "    StructField(\"order\", IntegerType(), True),\n",
    "    StructField(\"ftpperiod\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "\n",
    "prioritylevel2_existing_schema = StructType([\n",
    "    StructField(\"Period\", StringType(), True),\n",
    "    StructField(\"stdperiod\", StringType(), True),\n",
    "    StructField(\"PeriodType\", StringType(), True),\n",
    "    StructField(\"closing\", StringType(), True),\n",
    "    StructField(\"legacyperiod\", StringType(), True),\n",
    "    StructField(\"numericvalueofperiod\", IntegerType(), True),\n",
    "    StructField(\"priorityseq\", IntegerType(), True),\n",
    "    StructField(\"Order\", IntegerType(), True)\n",
    "])\n",
    "prioritylevel2_schema = add_dimension_columns(alldimcols, prioritylevel2_existing_schema)\n",
    "\n",
    "prioritylevel2_ext_schema = StructType([\n",
    "    StructField(\"Period\", StringType(), True),\n",
    "    StructField(\"stdperiod\", StringType(), True),\n",
    "    StructField(\"PeriodType\", StringType(), True),\n",
    "    StructField(\"closing\", StringType(), True),\n",
    "    StructField(\"legacyperiod\", StringType(), True),\n",
    "    StructField(\"numericvalueofperiod\", IntegerType(), True),\n",
    "    StructField(\"priorityseq\", IntegerType(), True),\n",
    "    StructField(\"Order\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "\n",
    "tballperiod_schema = StructType([\n",
    "    StructField(\"Period\", StringType(), True),\n",
    "    StructField(\"stdperiod\", StringType(), True),\n",
    "    StructField(\"PeriodType\", StringType(), True),\n",
    "    StructField(\"closing\", StringType(), True),\n",
    "    StructField(\"LegacyPeriod\", StringType(), True),\n",
    "    StructField(\"numericvalueofperiod\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "\n",
    "tblegacymatchperiod_existing_schema = StructType([\n",
    "    StructField(\"Period\", StringType(), True),\n",
    "    StructField(\"stdperiod\", StringType(), True),\n",
    "    StructField(\"PeriodType\", StringType(), True),\n",
    "    StructField(\"closing\", StringType(), True),\n",
    "    StructField(\"LegacyPeriod\", StringType(), True),\n",
    "    StructField(\"numericvalueofperiod\", IntegerType(), True)\n",
    "])\n",
    "tblegacymatchperiod_schema = add_dimension_columns(alldimcols, tblegacymatchperiod_existing_schema)\n",
    "\n",
    "ytddates_schema = StructType([\n",
    "    StructField(\"PeriodType\", StringType(), True),\n",
    "    StructField(\"CurrentValue\", StringType(), True),\n",
    "    StructField(\"FTPValue\", StringType(), True)\n",
    "])\n",
    "\n",
    "tbfinal_existing_schema = StructType([\n",
    "    StructField(\"PeriodType\", StringType(), True),\n",
    "    StructField(\"Period\", StringType(), True),\n",
    "    StructField(\"userglcode\", StringType(), True),\n",
    "    StructField(\"currency\", IntegerType(), True),\n",
    "    StructField(\"closingbalance\", DecimalType(28, 8), True)\n",
    "])\n",
    "\n",
    "\n",
    "tbfinal_schema = add_dimension_columns(alldimcols, tbfinal_existing_schema)\n",
    "\n",
    "tbfinalytd_existing_schema = StructType([\n",
    "    StructField(\"PeriodType\", StringType(), True),\n",
    "    StructField(\"Period\", StringType(), True),\n",
    "    StructField(\"userglcode\", StringType(), True),\n",
    "    StructField(\"currency\", IntegerType(), True),\n",
    "    StructField(\"closingbalance\", DecimalType(28, 8), True)\n",
    "])\n",
    "\n",
    "tbfinalytd_schema = add_dimension_columns(alldimcols, tbfinalytd_existing_schema)\n",
    "\n",
    "tbfinaldd_existing_schema = StructType([\n",
    "    StructField(\"PeriodType\", StringType(), True),\n",
    "    StructField(\"Period\", StringType(), True),\n",
    "    StructField(\"userglcode\", StringType(), True),\n",
    "    StructField(\"usergldescription\", StringType(), True),\n",
    "    StructField(\"currency\", IntegerType(), True),\n",
    "    StructField(\"closingbalance\", DecimalType(28, 8), True),\n",
    "    StructField(\"Source\", StringType(), True)\n",
    "])\n",
    "tbfinaldd_schema = add_dimension_columns(alldimcols, tbfinaldd_existing_schema)\n",
    "\n",
    "tbfinalddytd_existing_schema = StructType([\n",
    "    StructField(\"PeriodType\", StringType(), True),\n",
    "    StructField(\"Period\", StringType(), True),\n",
    "    StructField(\"userglcode\", StringType(), True),\n",
    "    StructField(\"usergldescription\", StringType(), True),\n",
    "    StructField(\"currency\", IntegerType(), True),\n",
    "    StructField(\"closingbalance\", DecimalType(28, 8), True),\n",
    "    StructField(\"Source\", StringType(), True)\n",
    "])\n",
    "tbfinalddytd_schema = add_dimension_columns(alldimcols, tbfinalddytd_existing_schema)\n",
    "\n",
    "disclosures_existing_schema = StructType([\n",
    "    StructField(\"PeriodType\", StringType(), True),\n",
    "    StructField(\"Period\", StringType(), True),\n",
    "    StructField(\"entityid\", IntegerType(), True),\n",
    "    StructField(\"ultimateparent\", StringType(), True),\n",
    "    StructField(\"categoryid\", IntegerType(), True),\n",
    "    StructField(\"member\", StringType(), True),\n",
    "    StructField(\"accounttype\", StringType(), True),\n",
    "    StructField(\"amount\", DecimalType(28, 8), True),\n",
    "    StructField(\"currencyid\", IntegerType(), True),\n",
    "    StructField(\"column27\", StringType(), True),\n",
    "    StructField(\"AccountSubType\", StringType(), True)\n",
    "])\n",
    "disclosures_schema = add_dimension_columns(alldimcols, disclosures_existing_schema)\n",
    "\n",
    "disclosuresytd_existing_schema = StructType([\n",
    "    StructField(\"PeriodType\", StringType(), True),\n",
    "    StructField(\"Period\", StringType(), True),\n",
    "    StructField(\"entityid\", IntegerType(), True),\n",
    "    StructField(\"ultimateparent\", StringType(), True),\n",
    "    StructField(\"categoryid\", IntegerType(), True),\n",
    "    StructField(\"member\", StringType(), True),\n",
    "    StructField(\"accounttype\", StringType(), True),\n",
    "    StructField(\"amount\", DecimalType(28, 8), True),\n",
    "    StructField(\"currencyid\", IntegerType(), True),\n",
    "    StructField(\"column27\", StringType(), True),\n",
    "    StructField(\"AccountSubType\", StringType(), True)\n",
    "])\n",
    "disclosuresytd_schema = add_dimension_columns(alldimcols, disclosuresytd_existing_schema)\n",
    "\n",
    "castingsdadjustment_existing_schema = StructType([\n",
    "    StructField(\"CastingAdjustmentsID\", IntegerType(), True),\n",
    "    StructField(\"Fk_AccountingStandardID\", IntegerType(), True),\n",
    "    StructField(\"Fk_ValidationID\", IntegerType(), True),\n",
    "    StructField(\"PeriodValue\", StringType(), True),\n",
    "    StructField(\"FinancialYear\", StringType(), True),\n",
    "    StructField(\"Fk_EntityID\", IntegerType(), True),\n",
    "    StructField(\"EntityName\", StringType(), True),\n",
    "    StructField(\"JournalNumber\", IntegerType(), True),\n",
    "    StructField(\"JournalDate\", TimestampType(), True),\n",
    "    StructField(\"UserGLCode\", StringType(), True),\n",
    "    StructField(\"UserGLDescription\", StringType(), True),\n",
    "    StructField(\"StandardGLCode\", StringType(), True),\n",
    "    StructField(\"StandardGLDescription\", StringType(), True),\n",
    "    StructField(\"DebitCredit\", StringType(), True),\n",
    "    StructField(\"Fk_CurrencyID\", IntegerType(), True),\n",
    "    StructField(\"Amount\", DecimalType(18, 4), True),\n",
    "    StructField(\"JournalType\", StringType(), True),\n",
    "    StructField(\"Narration\", StringType(), True),\n",
    "    StructField(\"Fk_BusinessUnitID\", IntegerType(), True),\n",
    "    StructField(\"CategoryID\", IntegerType(), True),\n",
    "    StructField(\"Roundoff\", IntegerType(), True),\n",
    "    StructField(\"AmountsIn\", StringType(), True),\n",
    "    StructField(\"ReportType\", StringType(), True),\n",
    "    StructField(\"CreatedDate\", TimestampType(), True),\n",
    "    StructField(\"CreatedBy\", StringType(), True),\n",
    "    StructField(\"ModifiedDate\", TimestampType(), True),\n",
    "    StructField(\"ModifiedBy\", StringType(), True),\n",
    "    StructField(\"IsUserGLCode\", BooleanType(), True),\n",
    "    StructField(\"RoundTypeLevel\", StringType(), True),\n",
    "    \n",
    "])\n",
    "castingsdadjustment_schema = add_dimension_columns(alldimcols, castingsdadjustment_existing_schema)\n",
    "castingsdadjustment_df = spark.createDataFrame([], schema=castingsdadjustment_schema)\n",
    "\n",
    "legacysddates_schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), False),\n",
    "    StructField(\"periodtype\", StringType(), True),\n",
    "    StructField(\"Value\", StringType(), True),\n",
    "    StructField(\"pcurrentvalue\", StringType(), True),\n",
    "    StructField(\"currentvalue\", StringType(), True),\n",
    "    StructField(\"legacyperiod\", StringType(), True),\n",
    "    StructField(\"categoryid\", IntegerType(), True),\n",
    "    StructField(\"financialyear\", IntegerType(), True),\n",
    "    StructField(\"openingperiodvalue\", StringType(), True),\n",
    "    StructField(\"castingperiod\", StringType(), True),\n",
    "    StructField(\"numericvalueofperiod\", IntegerType(), True),\n",
    "    StructField(\"ftpperiod\", IntegerType(), True),\n",
    "    StructField(\"periodformat\", StringType(), True),\n",
    "    StructField(\"p\", StringType(), True),\n",
    "    StructField(\"ppp\", StringType(), True),\n",
    "    StructField(\"py\", StringType(), True),\n",
    "    StructField(\"pypp\", StringType(), True),\n",
    "    StructField(\"ppy\", StringType(), True),\n",
    "    StructField(\"p3y\", StringType(), True),\n",
    "    StructField(\"p4y\", StringType(), True)\n",
    "])\n",
    "\n",
    "legacysddates_union_df = spark.createDataFrame([], legacysddates_schema)\n",
    "\n",
    "\n",
    "legacyadjustmentsdrilldown_schema = StructType([\n",
    "    StructField(\"AccountingStdID_fk\", IntegerType(), True),\n",
    "    StructField(\"Period\", StringType(), True),\n",
    "    StructField(\"EntityID_Fk\", IntegerType(), True),\n",
    "    StructField(\"JournalNumber\", IntegerType(), True),\n",
    "    StructField(\"JournalDate\", TimestampType(), True),\n",
    "    StructField(\"UserGLCode\", StringType(), True),\n",
    "    StructField(\"Member\", StringType(), True),\n",
    "    StructField(\"DebitCredit\", StringType(), True),\n",
    "    StructField(\"CurrencyID_Fk\", IntegerType(), True),\n",
    "    StructField(\"Amount\", DecimalType(20, 5), True),\n",
    "    StructField(\"JournalType\", StringType(), True),\n",
    "    StructField(\"Narration\", StringType(), True),\n",
    "    StructField(\"CreatedDate\", TimestampType(), True),\n",
    "    StructField(\"CreatedBy\", StringType(), True),\n",
    "    StructField(\"ModifiedDate\", TimestampType(), True),\n",
    "    StructField(\"ModifiedBy\", StringType(), True),\n",
    "    StructField(\"BUID_fk\", IntegerType(), True),\n",
    "    StructField(\"IsImported\", BooleanType(), True),\n",
    "    StructField(\"CategoryID\", IntegerType(), True),\n",
    "    StructField(\"IsMappedGLCode\", BooleanType(), True),\n",
    "    StructField(\"EndPeriod\", StringType(), True),\n",
    "    StructField(\"NotApplicable\", StringType(), True),\n",
    "    StructField(\"DimType1ID_fk\", IntegerType(), True),\n",
    "    StructField(\"DimType2ID_fk\", IntegerType(), True),\n",
    "    StructField(\"DimType3ID_fk\", IntegerType(), True),\n",
    "    StructField(\"DimType4ID_fk\", IntegerType(), True),\n",
    "    StructField(\"DimType5ID_fk\", IntegerType(), True),\n",
    "    StructField(\"DimType6ID_fk\", IntegerType(), True),\n",
    "    StructField(\"DimType7ID_fk\", IntegerType(), True),\n",
    "    StructField(\"DimType8ID_fk\", IntegerType(), True),\n",
    "    StructField(\"DimType9ID_fk\", IntegerType(), True),\n",
    "    StructField(\"DimType10ID_fk\", IntegerType(), True),\n",
    "    StructField(\"DimType11ID_fk\", IntegerType(), True),\n",
    "    StructField(\"DimType12ID_fk\", IntegerType(), True),\n",
    "    StructField(\"DimType13ID_fk\", IntegerType(), True),\n",
    "    StructField(\"DimType14ID_fk\", IntegerType(), True),\n",
    "    StructField(\"DimType15ID_fk\", IntegerType(), True),\n",
    "    StructField(\"DimType16ID_fk\", IntegerType(), True),\n",
    "    StructField(\"DimType17ID_fk\", IntegerType(), True),\n",
    "    StructField(\"DimType18ID_fk\", IntegerType(), True),\n",
    "    StructField(\"DimType19ID_fk\", IntegerType(), True),\n",
    "    StructField(\"DimType20ID_fk\", IntegerType(), True),\n",
    "    StructField(\"DimType21ID_fk\", IntegerType(), True),\n",
    "    StructField(\"DimType22ID_fk\", IntegerType(), True),\n",
    "    StructField(\"DimType23ID_fk\", IntegerType(), True),\n",
    "    StructField(\"DimType24ID_fk\", IntegerType(), True),\n",
    "    StructField(\"DimType25ID_fk\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Create an empty DataFrame with the defined schema\n",
    "legacyadjustmentsdrilldown_union_df = spark.createDataFrame([], legacyadjustmentsdrilldown_schema)\n",
    "\n",
    "mappedsdtbytd_existing_schema = StructType([\n",
    "    StructField(\"PeriodType\", StringType(), True),\n",
    "    StructField(\"Period\", StringType(), True),\n",
    "    StructField(\"UserGLCode\", StringType(), True),\n",
    "    StructField(\"Currency\", IntegerType(), True),\n",
    "    StructField(\"ClosingBalance\", DecimalType(28, 8), True),\n",
    "    StructField(\"CategoryID\", StringType(), True),\n",
    "    StructField(\"Member\", StringType(), True),\n",
    "    StructField(\"ytdtype\", IntegerType(), True)\n",
    "])\n",
    "mappedsdtbytd_schema = add_dimension_columns(alldimcols, mappedsdtbytd_existing_schema)\n",
    "\n",
    "mappedsdtbftp_existing_schema = StructType([\n",
    "    StructField(\"PeriodType\", StringType(), True),\n",
    "    StructField(\"Period\", StringType(), True),\n",
    "    StructField(\"UserGLCode\", StringType(), True),\n",
    "    StructField(\"Currency\", IntegerType(), True),\n",
    "    StructField(\"ClosingBalance\", DecimalType(28, 8), True),\n",
    "    StructField(\"categoryid\", StringType(), True),\n",
    "    StructField(\"member\", StringType(), True),\n",
    "    StructField(\"ytdtype\", IntegerType(), True)\n",
    "])\n",
    "mappedsdtbftp_schema = add_dimension_columns(alldimcols, mappedsdtbftp_existing_schema)\n",
    "\n",
    "mappedtbddytd_existing_schema = StructType([\n",
    "    StructField(\"PeriodType\", StringType(), True),\n",
    "    StructField(\"Period\", StringType(), True),\n",
    "    StructField(\"UserGLCode\", StringType(), True),\n",
    "    StructField(\"UserGLDescription\", StringType(), True),\n",
    "    StructField(\"closingbalance\", DecimalType(28, 8), True),\n",
    "    StructField(\"member\", StringType(), True),\n",
    "    StructField(\"Source\", StringType(), True),\n",
    "    StructField(\"ytdtype\", IntegerType(), True)\n",
    "])\n",
    "mappedtbddytd_schema = add_dimension_columns(alldimcols, mappedtbddytd_existing_schema)\n",
    "\n",
    "mappedtbddftp_existing_schema = StructType([\n",
    "    StructField(\"PeriodType\", StringType(), True),\n",
    "    StructField(\"Period\", StringType(), True),\n",
    "    StructField(\"UserGLCode\", StringType(), True),\n",
    "    StructField(\"UserGLDescription\", StringType(), True),\n",
    "    StructField(\"closingbalance\", DecimalType(28, 8), True),\n",
    "    StructField(\"member\", StringType(), True),\n",
    "    StructField(\"Source\", StringType(), True),\n",
    "    StructField(\"ytdtype\", IntegerType(), True)\n",
    "])\n",
    "mappedtbddftp_schema = add_dimension_columns(alldimcols, mappedtbddftp_existing_schema)\n",
    "\n",
    "allperioddata_existing_schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"PeriodType\", StringType(), True),\n",
    "    StructField(\"Period\", StringType(), True),\n",
    "    StructField(\"userglcode\", StringType(), True),\n",
    "    StructField(\"currency\", IntegerType(), True),\n",
    "    StructField(\"closingbalance\", DecimalType(28, 8), True),\n",
    "    StructField(\"categoryid\", IntegerType(), True),\n",
    "    StructField(\"member\", StringType(), True),\n",
    "    StructField(\"ytdtype\", StringType(), True),\n",
    "    StructField(\"usergldescription\", StringType(), True),\n",
    "    StructField(\"Source\", StringType(), True),\n",
    "    StructField(\"Value\", StringType(), True),\n",
    "    StructField(\"pcurrentvalue\", StringType(), True),\n",
    "    StructField(\"legacyperiod\", StringType(), True),\n",
    "    StructField(\"financialyear\", StringType(), True),\n",
    "    StructField(\"openingperiodvalue\", StringType(), True),\n",
    "    StructField(\"castingperiod\", StringType(), True),\n",
    "    StructField(\"p\", StringType(), True),\n",
    "    StructField(\"ppp\", StringType(), True),\n",
    "    StructField(\"py\", StringType(), True),\n",
    "    StructField(\"pypp\", StringType(), True),\n",
    "    StructField(\"ppy\", StringType(), True),\n",
    "    StructField(\"p3y\", StringType(), True),\n",
    "    StructField(\"p4y\", StringType(), True),\n",
    "    StructField(\"CastingAdjustmentsID\", IntegerType(), True),\n",
    "    StructField(\"Fk_AccountingStandardID\", IntegerType(), True),\n",
    "    StructField(\"Fk_ValidationID\", IntegerType(), True),\n",
    "    StructField(\"PeriodValue\", StringType(), True),\n",
    "    StructField(\"Fk_EntityID\", IntegerType(), True),\n",
    "    StructField(\"EntityName\", StringType(), True),\n",
    "    StructField(\"JournalNumber\", IntegerType(), True),\n",
    "    StructField(\"JournalDate\", TimestampType(), True),\n",
    "    StructField(\"StandardGLCode\", StringType(), True),\n",
    "    StructField(\"StandardGLDescription\", StringType(), True),\n",
    "    StructField(\"DebitCredit\", StringType(), True),\n",
    "    StructField(\"Fk_CurrencyID\", IntegerType(), True),\n",
    "    StructField(\"Amount\", DecimalType(18, 4), True),\n",
    "    StructField(\"JournalType\", StringType(), True),\n",
    "    StructField(\"Narration\", StringType(), True),\n",
    "    StructField(\"Fk_BusinessUnitID\", IntegerType(), True),\n",
    "    StructField(\"Roundoff\", IntegerType(), True),\n",
    "    StructField(\"AmountsIn\", StringType(), True),\n",
    "    StructField(\"ReportType\", StringType(), True),\n",
    "    StructField(\"CreatedDate\", TimestampType(), True),\n",
    "    StructField(\"CreatedBy\", StringType(), True),\n",
    "    StructField(\"ModifiedDate\", TimestampType(), True),\n",
    "    StructField(\"ModifiedBy\", StringType(), True),\n",
    "    StructField(\"IsUserGLCode\", IntegerType(), True),\n",
    "    StructField(\"RoundTypeLevel\", StringType(), True),\n",
    "    StructField(\"typeofdata\", IntegerType(), True)\n",
    "])\n",
    "allperioddata_schema = add_dimension_columns(alldimcols, allperioddata_existing_schema)\n",
    "allperioddata_df = spark.createDataFrame([], schema=allperioddata_schema)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8beeaa01-5666-448d-a758-2d61d68b57a4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_legacydates(periodicaldates_src_df,df_legacysddates,df_daterangetoperiod,schema_daterangetoperiod,df_usergivenmulticomp,df_daterangeconverttoperiod,df_daterangecustomperiod,df_compperioddetails,AccountingStandardID,PeriodValue):\n",
    "    try:\n",
    "        \n",
    "        AccountingType = AccountingStandardID\n",
    "        Start_TimestampMP = datetime.now()\n",
    "        NumericalPeriodTypeValue =None\n",
    "        customperiod = 0\n",
    "        startVal = None\n",
    "        print(\"custom period logic\")\n",
    "        list_col_daterangeconverttoperiod = [\"id\",\"currentperiod\",\"startdate\",\"enddate\",\"periodstart\",\"monthdiff\",\"periodmonthvalue\",\"tbperiodvalue\"]\n",
    "\n",
    "        list_col_daterangecustomperiod = [\"id\",\"currentperiod\",\"startdate\",\"enddate\",\"periodstart\",\"monthdiff\",\"periodmonthvalue\",\"tbperiodvalue\",\"year\",\"periodvalue\",\"customperiod\",\"customdaterange\"]\n",
    "\n",
    "        list_col_daterangetoperiod = [\"id\",\"currentperiod\",\"startdate\",\"enddate\",\"periodstart\",\"monthdiff\",\"periodmonthvalue\",\"tbperiodvalue\",\"year\",\"periodvalue\",\"customperiod\",\"customdaterange\",\"openperiodvalue\",\"opencyclevalue\"]\n",
    "\n",
    "        # Create Empty df_usergivenmulticomp and df_compperioddetails\n",
    "        list_col_usergivenmulticomp = [\"compperiodname\",\"compperiodvalue\",\"comptype\",\"compperiod\",\"Value\"]\n",
    "\n",
    "        list_col_legacysddates=[\"id\",\"periodtype\",\"Value\",\"pcurrentvalue\",\"currentvalue\",\"legacyperiod\",\"categoryid\",\"financialyear\",\"openingperiodvalue\",\"castingperiod\",\"numericvalueofperiod\",\"ftpperiod\",\"periodformat\",\"p\",\"ppp\",\"py\",\"pypp\",\"ppy\",\"p3y\",\"p4y\"]\n",
    "        \n",
    "        periodicaldates_df = periodicaldates_src_df.withColumnRenamed(\"Year\",\"pd_Year\")\n",
    "\n",
    "        print(\"change date range period to standard period format\")\n",
    "\n",
    "        #function to get date in YYYY-MM-DD format \n",
    "\n",
    "        def changeToDateFormat(inputDate):\n",
    "            date_list= inputDate.split(\"-\")\n",
    "            date_input = date_list[0]\n",
    "            month_input = date_list[1]\n",
    "            year_input = date_list[2]\n",
    "            dateValue = datetime.date(int(year_input), int(month_input), int(date_input))\n",
    "            return dateValue\n",
    "\n",
    "        def diff_month(d1, d2):\n",
    "            return (d1.year - d2.year) * 12 + d1.month - d2.month\n",
    "\n",
    "        #for periodic table\n",
    "        def getPeriodFirstValue(Period):\n",
    "            v1= Period.split(\" \")[0]\n",
    "            #v2= Period.split(\" \")[1]\n",
    "            return v1\n",
    "        getPeriodFirstValue_udf = F.udf(getPeriodFirstValue,StringType())\n",
    "\n",
    "        #get period column of df_daterangecustomperiod\n",
    "        def getperiodvalue(value,opening,closing):\n",
    "            periodvalue= value + \"-\" + opening + \"-\" + closing\n",
    "            return periodvalue\n",
    "\n",
    "        getperiodvalue_udf = F.udf(getperiodvalue,StringType())\n",
    "\n",
    "        #get period column of df_daterangecustomperiod\n",
    "        def getcustomperiod(periodMonthValue,monthDiff,opening,startDate,closing):\n",
    "            customperiod=None\n",
    "            if periodMonthValue =='M12' and monthDiff == 12:\n",
    "                customperiod= 'Year'\n",
    "            elif periodMonthValue == 'M12' and monthDiff == 6:\n",
    "                customperiod= 'H2'\n",
    "            elif periodMonthValue =='M12' and monthDiff == 3:\n",
    "                customperiod= 'Q4'\n",
    "            elif periodMonthValue =='M3' and monthDiff == 3:\n",
    "                customperiod= 'Q1'\n",
    "            elif periodMonthValue =='M6' and monthDiff == 6:\n",
    "                customperiod= 'H1'\n",
    "            elif periodMonthValue =='M6' and monthDiff == 3:\n",
    "                customperiod= 'Q2'\n",
    "            elif periodMonthValue =='M9' and monthDiff == 9:\n",
    "                customperiod= '9M'\n",
    "            elif periodMonthValue =='M9' and monthDiff == 3:\n",
    "                customperiod= 'Q3'\n",
    "            else:\n",
    "                customperiod = 'C-' + opening[:4] + startDate.strftime(\"%m\") + '-' + closing\n",
    "            return customperiod\n",
    "\n",
    "        getcustomperiod_udf = F.udf(getcustomperiod,StringType())\n",
    "\n",
    "        #get updated periodvalue column of df_daterangecustomperiod on the basis of customperiod\n",
    "        def getupdatedperiodvalue(periodvalue,customperiod):\n",
    "            if customperiod[:1]=='C':\n",
    "                periodvalue=periodvalue\n",
    "            else:\n",
    "                tempList = periodvalue.split(\"-\")\n",
    "                periodvalue = customperiod+'-'+tempList[1]+'-'+tempList[2]\n",
    "            return periodvalue\n",
    "\n",
    "        getupdatedperiodvalue_udf = F.udf(getupdatedperiodvalue,StringType())\n",
    "        #get updated customdaterange column of df_daterangecustomperiod on the basis of customperiod\n",
    "        def getupdatedcustomdaterange(customperiod):\n",
    "            if customperiod[:1]=='C':\n",
    "                customdaterange=1\n",
    "            else:\n",
    "                customdaterange=0\n",
    "            return customdaterange\n",
    "\n",
    "        getupdatedcustomdaterange_udf = F.udf(getupdatedcustomdaterange,IntegerType())\n",
    "\n",
    "        #get openperiodvalue of daterangetoperiod\n",
    "        def getopenperiodvalue(startDate):\n",
    "            openperiodvalue = startDate.strftime(\"%Y\")+startDate.strftime(\"%m\")\n",
    "            return openperiodvalue\n",
    "\n",
    "        getopenperiodvalue_udf = F.udf(getopenperiodvalue,StringType())\n",
    "\n",
    "        def getperiodMonthValue(endDate,periodstart):\n",
    "            periodMonthValue = 'M'+str((int(endDate.strftime(\"%m\")) - int(periodstart) + 12 ) % 12 + 1)\n",
    "            return periodMonthValue\n",
    "\n",
    "        getperiodMonthValue_udf = F.udf(getperiodMonthValue,StringType())\n",
    "\n",
    "        def gettbperiodvalue(endDate):\n",
    "            return endDate.strftime(\"%Y\")+endDate.strftime(\"%m\")\n",
    "\n",
    "\n",
    "        gettbperiodvalue_udf = F.udf(gettbperiodvalue,StringType())\n",
    "\n",
    "        #this if condition, all required df for data in range format\n",
    "        if PeriodValueInYHQM==0:\n",
    "            print(\"inside if\")\n",
    "            startVal=4  # from master.financialcycle\n",
    "            #INSERT INTO #daterangeconverttoperiod\n",
    "            startDate= changeToDateFormat(PeriodValue[:10])\n",
    "            endDate= changeToDateFormat(PeriodValue[11:])\n",
    "            periodStart = str(startVal)  \n",
    "            monthDiff=0\n",
    "            periodMonthValue=None\n",
    "            tbPeriodValue=None\n",
    "            data_row_single = [[PeriodValue,startDate,endDate,periodStart,monthDiff,periodMonthValue,tbPeriodValue]]\n",
    "            '''--------------------- df_daterangeconverttoperiod ---------------'''\n",
    "            df_daterangeconverttoperiodTemp1= spark.createDataFrame(data=data_row_single,schema=schema_daterangeconverttoperiod)\n",
    "           \n",
    "            df_daterangeconverttoperiod = df_daterangeconverttoperiod.union(df_daterangeconverttoperiodTemp1)\n",
    "            #display(df_daterangeconverttoperiod)\n",
    "            if ComparitivePeriodValue is not None:\n",
    "                print(\"inside if ComparitivePeriodValue is not None\")\n",
    "                list_ComparitivePeriodValue = ComparitivePeriodValue.split(\",\")  #comparativeperiod\n",
    "                rows=[]\n",
    "                for compPer in list_ComparitivePeriodValue:\n",
    "                    startDate= changeToDateFormat(compPer[:10])\n",
    "                    endDate= changeToDateFormat(compPer[11:])\n",
    "                    print(startDate,endDate)\n",
    "                    rows.append([compPer,startDate,endDate,periodStart,monthDiff,periodMonthValue,tbPeriodValue])\n",
    "                df_daterangeconverttoperiodTemp2= spark.createDataFrame(data=rows,schema=schema_daterangeconverttoperiod)\n",
    "                \n",
    "                df_daterangeconverttoperiod = df_daterangeconverttoperiod.union(df_daterangeconverttoperiodTemp2)\n",
    "                    \n",
    "            else:\n",
    "                print(\"ComparitivePeriodValue is  None\")\n",
    "        \n",
    "            df_daterangeconverttoperiod = df_daterangeconverttoperiod.withColumn(\"periodmonthvalue\",getperiodMonthValue_udf(F.col(\"enddate\"),F.col(\"periodstart\")))\\\n",
    "                                        .withColumn(\"monthdiff\", F.months_between(F.col(\"enddate\"),F.col(\"startdate\")).cast(IntegerType()))\\\n",
    "                                        .withColumn(\"monthdiff\", F.col(\"monthdiff\")+1)\\\n",
    "                                        .withColumn(\"tbperiodvalue\",gettbperiodvalue_udf(F.col(\"enddate\")))\n",
    "            \n",
    "            # adding id column in df_daterangeconverttoperiod\n",
    "            df_daterangeconverttoperiod = df_daterangeconverttoperiod.withColumn(\"monotonically_increasing_id\",F.monotonically_increasing_id()+1).withColumn(\"DummyColForPartition\",F.lit(\"DummyColForPartition\"))\n",
    "            #display(df1218)\n",
    "            window = Window.partitionBy(F.col(\"DummyColForPartition\")).orderBy(col('monotonically_increasing_id'))\n",
    "            df_daterangeconverttoperiod = df_daterangeconverttoperiod.withColumn('id', F.row_number().over(window)).select(list_col_daterangeconverttoperiod)\n",
    "            \n",
    "            #df_daterangeconverttoperiod calculation end \n",
    "            \n",
    "            \n",
    "            '''--------------------- End of df_daterangeconverttoperiod ---------------'''\n",
    "            '''--------------------- daterangecustomperiod ---------------'''\n",
    "            #get daterangecustomperiod temp table \n",
    "            #creating new temp col in perioddates for joining condition, will drop tihs col after join\n",
    "            periodicaldates_df = periodicaldates_df.withColumn(\"PeriodColFirstValue\", getPeriodFirstValue_udf(F.col(\"Period\")))\n",
    "\n",
    "            df_daterangecustomperiod_new = df_daterangecustomperiod.select(list_col_daterangeconverttoperiod).union(df_daterangeconverttoperiod)\\\n",
    "                                            .withColumn(\"year\",F.lit(None))\\\n",
    "                                            .withColumn(\"periodvalue\",F.lit(None))\\\n",
    "                                            .withColumn(\"customperiod\",F.lit(None))\\\n",
    "                                            .withColumn(\"customdaterange\",F.lit(None))\n",
    "            #df_daterangecustomperiod= spark.createDataFrame(df_daterangecustomperiod.rdd,schema = schema_daterangecustomperiod)\n",
    "            df_daterangecustomperiod = df_daterangecustomperiod.union(df_daterangecustomperiod_new)\n",
    "            \n",
    "            joined_df_pt_pd = (df_daterangecustomperiod.join(periodicaldates_df,how=\"inner\",on= [(periodicaldates_df.Closing==df_daterangecustomperiod.tbperiodvalue) & (df_daterangecustomperiod.periodmonthvalue == periodicaldates_df.PeriodColFirstValue) & (periodicaldates_df.FinancialCycleID==FiscalPeriodID)]))\\\n",
    "                    .withColumn(\"year\",F.col(\"pd_Year\"))\\\n",
    "                    .withColumn(\"periodvalue\",getperiodvalue_udf(F.col(\"Value\"),F.col(\"Opening\"),F.col(\"Closing\")))\\\n",
    "                    .withColumn(\"customperiod\",getcustomperiod_udf(F.col(\"periodmonthvalue\"),F.col(\"monthdiff\"),F.col(\"opening\"),F.col(\"startdate\"),F.col(\"closing\")))\\\n",
    "                    .withColumn(\"customdaterange\",F.lit(0))\n",
    "            df_daterangecustomperiod = joined_df_pt_pd.select(list_col_daterangecustomperiod)\n",
    "            \n",
    "            periodicaldates_df = periodicaldates_df.drop(\"PeriodColFirstValue\") #droppping dervied col \n",
    "            # get updated customperiod and customdaterange on the basis of customperiod\n",
    "            df_daterangecustomperiod =df_daterangecustomperiod.withColumn(\"periodvalue\",getupdatedperiodvalue_udf(F.col(\"periodvalue\"),F.col(\"customperiod\"))).withColumn(\"customdaterange\",getupdatedcustomdaterange_udf(F.col(\"customperiod\")))\n",
    "            \n",
    "            '''--------------------- daterangetoperiod ---------------'''\n",
    "            #daterangetoperiod\n",
    "            df_daterangetoperiod_new = df_daterangetoperiod.select(list_col_daterangecustomperiod).union(df_daterangecustomperiod).withColumn(\"openperiodvalue\",F.lit(None)).withColumn(\"opencyclevalue\",F.lit(None))\n",
    "            \n",
    "            df_daterangetoperiod = df_daterangetoperiod.union(df_daterangetoperiod_new).withColumn(\"cpid\",F.concat(F.lit(\"CP\"),F.col(\"id\")-1))\n",
    "\n",
    "            joined_df_ptb_pd = df_daterangetoperiod.join(periodicaldates_df,how=\"inner\", on=[(df_daterangetoperiod.periodmonthvalue==periodicaldates_df.Value)& (df_daterangetoperiod.year==periodicaldates_df.pd_Year) ]).where( (F.col(\"financialcycleid\")==F.lit(FiscalPeriodID)) )\\\n",
    "                                .withColumn(\"openperiodvalue\",getopenperiodvalue_udf(F.col(\"startdate\")))\\\n",
    "                                .withColumn(\"opencyclevalue\",F.col(\"Opening\")).dropDuplicates(list_col_daterangecustomperiod)\\\n",
    "                                \n",
    "            print(\"------------df_daterangetoperiod after joined_df_ptb_pd---\")\n",
    "            df_daterangetoperiod =joined_df_ptb_pd.select(list_col_daterangetoperiod[:1]+[\"cpid\"]+list_col_daterangetoperiod[1:]).orderBy(\"id\")\n",
    "            \n",
    "            PeriodValue = df_daterangetoperiod.select(\"periodvalue\").where(F.col(\"id\")==1).collect()[0][0]\n",
    "            print(\"update value of PeriodValue to:\",PeriodValue)\n",
    "            \n",
    "\n",
    "            #update monthdiff column When inYHQM format\n",
    "        df_daterangetoperiod = df_daterangetoperiod.withColumn( \"monthdiff\", when(col(\"periodmonthvalue\").substr(1, 1) == 'Y', 12)\\\n",
    "                .when(col(\"periodmonthvalue\").substr(1, 1) == '9', 9)\n",
    "                .when(col(\"periodmonthvalue\").substr(1, 1) == 'H', 6)\n",
    "                .when(col(\"periodmonthvalue\").substr(1, 1) == 'Q', 3)\n",
    "                .when(col(\"periodmonthvalue\").substr(1, 1) == 'M', 1).otherwise(F.col(\"monthdiff\"))).orderBy(\"id\")        \n",
    "\n",
    "        # adding id column in df_daterangetoperiod in PeriodValueInYHQM==1 case\n",
    "        if PeriodValueInYHQM==1:  \n",
    "            df_daterangetoperiod = df_daterangetoperiod.withColumn(\"monotonically_increasing_id\",F.monotonically_increasing_id()+1).withColumn(\"DummyColForPartition\",F.lit(\"DummyColForPartition\"))\n",
    "            window = Window.partitionBy(F.col(\"DummyColForPartition\")).orderBy(col('monotonically_increasing_id'))\n",
    "            df_daterangetoperiod = df_daterangetoperiod.withColumn('id', F.row_number().over(window)).withColumn(\"cpid\",F.concat(F.lit(\"CP\"),F.col(\"id\")-1)).select(list_col_daterangetoperiod[:1]+[\"cpid\"]+list_col_daterangetoperiod[1:])\n",
    "        \n",
    "        df_daterangetoperiod.coalesce(1).write.format(\"parquet\").mode(\"overwrite\").save(adls_path+folder_path+\"daterangetoperiod\")  \n",
    "        df_daterangetoperiod = spark.read.parquet(adls_path+folder_path+\"daterangetoperiod\",schema = schema_daterangetoperiod1)\n",
    "\n",
    "        # local variable calculation\n",
    "        PeriodValuePrefix = PeriodValue[:1]\n",
    "        PeriodType = PeriodValuePrefix\n",
    "        endperiod = PeriodValue[-6:]\n",
    "        if PeriodValueInYHQM ==0:\n",
    "            print(\"inside if\")\n",
    "            NumericalValueofPeriodType = -int((df_daterangetoperiod.select(\"monthdiff\").where(F.col(\"id\")==1).collect()[0][0]))  \n",
    "        else:\n",
    "            if PeriodType== 'Y':\n",
    "                NumericalValueofPeriodType= -12\n",
    "            elif PeriodType== '9':\n",
    "                NumericalValueofPeriodType= -9\n",
    "            elif PeriodType== 'H':\n",
    "                NumericalValueofPeriodType= -6\n",
    "            elif PeriodType== 'Q':\n",
    "                NumericalValueofPeriodType= -3\n",
    "            elif PeriodType== 'M':\n",
    "                NumericalValueofPeriodType= -1\n",
    "\n",
    "        print(\"addition for custom date range\")\n",
    "\n",
    "        # from datetime import datetime\n",
    "        def getYearMonthFromDate(input_date):\n",
    "            return input_date.strftime(\"%Y\")+input_date.strftime(\"%m\")\n",
    "            \n",
    "        YearEndinDateFormat  = datetime.strptime(FinancialYear + '-' +  FiscalPeriod[-3:] + '-' + '01', '%Y-%b-%d').date()\n",
    "        PeriodEndInDateFormat = datetime.strptime(endperiod[:4] + '-' + endperiod[-2:] + '-' + '01' , '%Y-%m-%d').date()\n",
    "        print(\"YearEndinDateFormat:\",YearEndinDateFormat)\n",
    "        print(\"PeriodEndInDateFormat:\",PeriodEndInDateFormat)\n",
    "\n",
    "        CurrentPeriod\t=\tNone\n",
    "        CurrentPeriod_YTD\t=\tNone\n",
    "        PreviousPeriod\t=\tNone\n",
    "        PrevPreviousPeriod\t=\tNone\n",
    "        PreviousYear\t=\tNone\n",
    "        PrevPreviousYear\t=\tNone\n",
    "        PrevYearCurrPeriod\t=\tNone\n",
    "        PrevYearCurrPeriod_YTD\t=\tNone\n",
    "        PrevYearPrevPeriod\t=\tNone\n",
    "        PPrevPreviousYear\t=\tNone\n",
    "        P4PreviousYear\t=\tNone\n",
    "\n",
    "        CurrentPeriod = endperiod\n",
    "        CurrentPeriod_YTD = endperiod\n",
    "        if PeriodValuePrefix !=9:\n",
    "            PreviousPeriod = getYearMonthFromDate(PeriodEndInDateFormat + relativedelta(months = int(NumericalValueofPeriodType)))\n",
    "        else:\n",
    "            PreviousPeriod = getYearMonthFromDate(PeriodEndInDateFormat  + relativedelta(months = -12))\n",
    "        PrevPreviousPeriod = getYearMonthFromDate(PeriodEndInDateFormat + relativedelta(months = 2 * int(NumericalValueofPeriodType) ))\n",
    "        PreviousYear\t=\t getYearMonthFromDate(YearEndinDateFormat  + relativedelta(months = -12))\n",
    "        PrevPreviousYear\t=getYearMonthFromDate(YearEndinDateFormat  + relativedelta(months = -24))\n",
    "        PrevYearCurrPeriod\t=getYearMonthFromDate(PeriodEndInDateFormat + relativedelta(months = -12 ))\n",
    "        PrevYearCurrPeriod_YTD\t=getYearMonthFromDate(PeriodEndInDateFormat + relativedelta(months = -12 ))\n",
    "        PrevYearPrevPeriod\t=\t getYearMonthFromDate(PeriodEndInDateFormat + relativedelta(months = -12 + int(NumericalValueofPeriodType) ))\n",
    "        PPrevPreviousYear\t=\tgetYearMonthFromDate(YearEndinDateFormat  + relativedelta(months = -36))\n",
    "        P4PreviousYear\t=\tgetYearMonthFromDate(YearEndinDateFormat  + relativedelta(months = -48))\n",
    "\n",
    "        print(\"comparative Period calc\")\n",
    "\n",
    "        CompYearEndinDateFormat = None     #DATE,\n",
    "        CompPeriodEndInDateFormat  = None     # DATE,\n",
    "        CompFinancialyear  = None            # string\n",
    "        gapincompperiod   = None             # INT,\n",
    "        countofcomp    = None                # INT;\n",
    "\n",
    "        # df_complist :Empty\n",
    "        schema_complist= StructType([ StructField(\"compperiod\",StringType(),True) ])\n",
    "        df_complist = spark.createDataFrame(data=[],schema=schema_complist)\n",
    "\n",
    "        if PeriodValueInYHQM == 1:\n",
    "            print('period format is not daterange')\n",
    "            if ComparitivePeriodValue is not None:\n",
    "                list_ComparitivePeriodValue  = ComparitivePeriodValue.split(\",\")\n",
    "                data_in_row_list=[]\n",
    "                for cp in list_ComparitivePeriodValue:\n",
    "                    data_in_row_list.append([cp])\n",
    "                print(\"data_in_row_list\",data_in_row_list)\n",
    "                df_comp_temp = spark.createDataFrame(data=data_in_row_list,schema=schema_complist)\n",
    "                df_complist = df_complist.union(df_comp_temp)\n",
    "        else:\n",
    "            print('period format is daterange')\n",
    "            df_complist = df_complist.union(df_daterangetoperiod.withColumn(\"complist_compperiod\",F.when(F.col(\"customdaterange\")==1,F.concat_ws('-',F.col(\"periodmonthvalue\"),F.col(\"openperiodvalue\"),F.col(\"tbperiodvalue\") ) )\\\n",
    "                            .otherwise(F.concat_ws('-',F.col(\"customperiod\"),F.col(\"openperiodvalue\"),F.col(\"tbperiodvalue\"))) ).where(F.col(\"id\")!=1).select(\"complist_compperiod\"))\n",
    "\n",
    "        \n",
    "        countofcomp = df_complist.count()\n",
    "        print(\"countofcomp\",countofcomp)\n",
    "\n",
    "        #required functions\n",
    "        def getcompperiodvalue(PeriodValueInYHQM,compperiodvalue,customdaterange,periodmonthvalue,customperiod,openperiodvalue,tbperiodvalue):\n",
    "            if PeriodValueInYHQM == 1:\n",
    "                compperiodvalue=compperiodvalue\n",
    "            else:\n",
    "                if customdaterange==1:\n",
    "                    compperiodvalue=periodmonthvalue + '-' + openperiodvalue + '-' + tbperiodvalue\n",
    "                else:\n",
    "                    compperiodvalue=customperiod + '-' + openperiodvalue + '-' + tbperiodvalue\n",
    "            return compperiodvalue\n",
    "\n",
    "        getcompperiodvalue_udf  = F.udf(getcompperiodvalue,StringType())\n",
    "\n",
    "\n",
    "        def getcomptype(PeriodValueInYHQM,compperiodvalue,tbperiodvalue):\n",
    "            if PeriodValueInYHQM == 1:\n",
    "                compperiod = compperiodvalue[-6:]\n",
    "            else:\n",
    "                compperiod= tbperiodvalue\n",
    "            return compperiod\n",
    "\n",
    "        getcomptype_udf  = F.udf(getcomptype,StringType())\n",
    "        def getValue(compperiodvalue):\n",
    "            return compperiodvalue.split(\"-\")[0]\n",
    "\n",
    "        getValue_udf  = F.udf(getValue,StringType())\n",
    "\n",
    "        if ComparitivePeriodValue is not None:\n",
    "            if countofcomp == 1:\n",
    "                df_compperioddetails = df_compperioddetails.union(spark.createDataFrame([('CP1', 'PreviousPeriod'),('CP1', 'PrevYearCurrPeriod_YTD'),('CP1', 'PrevYearCurrPeriod'),('CP1', 'PreviousYear')],schema = schema_df_compperioddetails))\n",
    "\n",
    "            elif countofcomp == 2:\n",
    "                df_compperioddetails = df_compperioddetails.union(spark.createDataFrame([('CP1', 'PreviousPeriod'),('CP1', 'PreviousYear'),('CP2', 'PrevPreviousYear')],schema = schema_df_compperioddetails))\n",
    "\n",
    "            elif countofcomp ==3:\n",
    "                df_compperioddetails = df_compperioddetails.union(spark.createDataFrame([('CP1', 'PreviousPeriod'),('CP1', 'PreviousYear'),('CP2', 'PrevPreviousYear'), ('CP3', 'PPrevPreviousYear')],schema = schema_df_compperioddetails))\n",
    "\n",
    "            elif countofcomp ==4:\n",
    "                df_compperioddetails = df_compperioddetails.union(spark.createDataFrame([('CP1', 'PreviousPeriod'),('CP1', 'PreviousYear'),('CP2', 'PrevPreviousYear'),('CP3', 'PPrevPreviousYear'),('CP4','P4PreviousYear')],schema = schema_df_compperioddetails))\n",
    "            elif countofcomp ==5:\n",
    "                df_compperioddetails = df_compperioddetails.union(spark.createDataFrame([('CP1', 'PreviousPeriod'),('CP1', 'PreviousYear'),('CP2', 'PrevPreviousYear'),('CP3', 'PPrevPreviousYear'),('CP4','P4PreviousYear'), ('CP5','P5PreviousYear')],schema = schema_df_compperioddetails))\n",
    "            elif countofcomp ==6:\n",
    "                df_compperioddetails = df_compperioddetails.union(spark.createDataFrame([('CP1', 'PreviousPeriod'),('CP1', 'PreviousYear'),('CP2', 'PrevPreviousYear'),('CP3', 'PPrevPreviousYear'),('CP4','P4PreviousYear'), ('CP5','P5PreviousYear'),('CP6','P6PreviousYear')],schema = schema_df_compperioddetails))\n",
    "            elif countofcomp ==7:\n",
    "                df_compperioddetails = df_compperioddetails.union(spark.createDataFrame([('CP1', 'PreviousPeriod'),('CP1', 'PreviousYear'),('CP2', 'PrevPreviousYear'),('CP3', 'PPrevPreviousYear'),('CP4','P4PreviousYear'), ('CP5','P5PreviousYear'),('CP6','P6PreviousYear'),('CP7','P7PreviousYear')],schema = schema_df_compperioddetails))\n",
    "            elif countofcomp ==8:\n",
    "                df_compperioddetails = df_compperioddetails.union(spark.createDataFrame([('CP1', 'PreviousPeriod'),('CP1', 'PreviousYear'),('CP2', 'PrevPreviousYear'),('CP3', 'PPrevPreviousYear'),('CP4','P4PreviousYear'), ('CP5','P5PreviousYear'),('CP6','P6PreviousYear'),('CP7','P7PreviousYear'),('CP8','P8PreviousYear')],schema = schema_df_compperioddetails))\n",
    "            print(\"df_compperioddetails\")\n",
    "            \n",
    "            comparative_periods_df = spark.createDataFrame([(ComparitivePeriodValue,)], [\"value\"]).withColumn(\"comparative_periods\", split(col(\"value\"), ',')).selectExpr(\"explode(comparative_periods) as compperiodvalue\")\n",
    "            #ctecomp\n",
    "            df_ctecomp = comparative_periods_df.withColumn(\"comptype\", F.concat(F.lit('CP'), F.row_number().over(Window.orderBy(monotonically_increasing_id()))))\n",
    "            print(\"---ctecomp---\")\n",
    "            \n",
    "            df_joined_cte_pt_dp = df_ctecomp.join(df_compperioddetails,how=\"inner\",on=\"comptype\").join(df_daterangetoperiod,how=\"left\",on=df_ctecomp.comptype==df_daterangetoperiod.cpid)\\\n",
    "            .withColumn(\"compperiodvalue\",getcompperiodvalue_udf(F.lit(PeriodValueInYHQM),F.col(\"compperiodvalue\"),F.col(\"customdaterange\"),F.col(\"periodmonthvalue\"),F.col(\"customperiod\"),F.col(\"openperiodvalue\"),F.col(\"tbperiodvalue\")) )\\\n",
    "            .withColumn(\"compperiod\",getcomptype_udf(F.lit(PeriodValueInYHQM),F.col(\"compperiodvalue\"),F.col(\"tbperiodvalue\")))\\\n",
    "            .withColumn(\"Value\",getValue_udf(F.col(\"compperiodvalue\"))).select([\"alias\",\"compperiodvalue\",'comptype', 'compperiod', 'Value'])\n",
    "            print(\"df_joined_cte_pt_dp\")\n",
    "            \n",
    "            df_usergivenmulticomp = df_usergivenmulticomp.union(df_joined_cte_pt_dp)\n",
    "            df_usergivenmulticomp = df_usergivenmulticomp.withColumn(\"monotonically_increasing_id\",F.monotonically_increasing_id())\n",
    "            window = Window.orderBy(col('monotonically_increasing_id'))\n",
    "            df_usergivenmulticomp = df_usergivenmulticomp.withColumn('id', row_number().over(window)).select([\"id\"]+list_col_usergivenmulticomp)\n",
    "            print(\"df_usergivenmulticomp final dataframe\")\n",
    "            \n",
    "\n",
    "            compperiodvalue = df_usergivenmulticomp.where(df_usergivenmulticomp.id==1).select(\"compperiodvalue\").collect()[0][0]\n",
    "            \n",
    "            if FiscalPeriod=='Jan-Dec':\n",
    "                CompFinancialyear = str(compperiodvalue[-6:-2])\n",
    "            else:\n",
    "                CompFinancialyear = str(int(compperiodvalue.split(\"-\")[1][:4])+1)\n",
    "\n",
    "            ComparitiveSelectionEndPeriod= compperiodvalue[-6:]\n",
    "            CompYearEndinDateFormat = datetime.strptime(CompFinancialyear + '-' +  FiscalPeriod[-3:] + '-' + '01', '%Y-%b-%d').date()\n",
    "            CompPeriodEndInDateFormat =  datetime.strptime(ComparitiveSelectionEndPeriod[:4] + '-' +  ComparitiveSelectionEndPeriod[4:]+ '-' + '01', '%Y-%m-%d').date()\n",
    "            gapincompperiod = (int(YearEndinDateFormat.strftime(\"%Y\")) - int(CompYearEndinDateFormat.strftime(\"%Y\")) )-1\n",
    "            print(\"CompYearEndinDateFormat\",CompYearEndinDateFormat)\n",
    "            print(\"CompPeriodEndInDateFormat\",CompPeriodEndInDateFormat)\n",
    "            print('CompFinancialyear',CompFinancialyear)\n",
    "            print(\"ComparitiveSelectionEndPeriod\",ComparitiveSelectionEndPeriod)\n",
    "            print(\"gapincompperiod\",gapincompperiod)\n",
    "            #---------to remove normal period data when  one comparative is selected\n",
    "            if countofcomp==1 and PeriodValueInYHQM==1:\n",
    "                print(\"---------to remove normal period data when  one comparative is selected----\")\n",
    "                print(\"TRUNCATE TABLE df_usergivenmulticomp\")\n",
    "                df_usergivenmulticomp = df_usergivenmulticomp.limit(0)\n",
    "\n",
    "                PreviousPeriod = ComparitiveSelectionEndPeriod  #CP1\n",
    "                PrevPreviousPeriod = getYearMonthFromDate(CompYearEndinDateFormat + relativedelta(months = int(NumericalValueofPeriodType) ))\n",
    "                PreviousYear\t=\tgetYearMonthFromDate(CompYearEndinDateFormat  + relativedelta(months = 0))\n",
    "                PrevPreviousYear\t=\tgetYearMonthFromDate(CompYearEndinDateFormat  + relativedelta(months = -12))       \n",
    "                PrevYearCurrPeriod = ComparitiveSelectionEndPeriod\n",
    "                PrevYearCurrPeriod_YTD = ComparitiveSelectionEndPeriod\n",
    "                PrevYearPrevPeriod = getYearMonthFromDate(CompYearEndinDateFormat  + relativedelta(months = -24))\n",
    "                PPrevPreviousYear\t=\tgetYearMonthFromDate(CompYearEndinDateFormat  + relativedelta(months = -24))\n",
    "                P4PreviousYear\t=\tgetYearMonthFromDate(CompYearEndinDateFormat  + relativedelta(months = -36))\n",
    "        # legacydates\n",
    "        schema_data_tmp_dates = StructType([StructField(\"periodtype\",StringType(),True), StructField(\"currentvalue\",StringType(),True), StructField(\"legacyperiod\",StringType(),True) ])\n",
    "        df_tmpdates = spark.createDataFrame(data = [],schema=schema_data_tmp_dates)\n",
    "        schema_daterangetoperiod = StructType([ StructField(\"periodtype\",StringType(),True), StructField(\"currentvalue\",StringType(),True), StructField(\"legacyperiod\",StringType(),True) ])\n",
    "        df_dates = spark.createDataFrame(data = [],schema=schema_daterangetoperiod)\n",
    "\n",
    "        def getLeagacyPeriod(periodtype,currentvalue,PeriodValuePrefix):\n",
    "            legacyperiod=\"\"\n",
    "            if periodtype[-4:]=='Year' :\n",
    "                legacyperiod= str(currentvalue)+'-12'\n",
    "            else:\n",
    "                #legacyperiod=\"elsepart\"\n",
    "                #NumericalvalueofPeriod=0\n",
    "                if PeriodValuePrefix =='Y':\n",
    "                    legacyperiod=currentvalue + '-12'\n",
    "                elif PeriodValuePrefix =='9':\n",
    "                    legacyperiod=currentvalue + '-09'\n",
    "                elif PeriodValuePrefix =='H':\n",
    "                    legacyperiod=currentvalue + '-06'\n",
    "                elif PeriodValuePrefix =='Q':\n",
    "                    legacyperiod=currentvalue + '-03'\n",
    "                elif PeriodValuePrefix =='M':\n",
    "                    legacyperiod=currentvalue + '-01'\n",
    "            return legacyperiod\n",
    "        getLeagacyPeriod_udf = F.udf(getLeagacyPeriod, StringType()) \n",
    "\n",
    "        #update legacyperiod on the basis of compperiod value\n",
    "        def getLeagacyPeriod2(legacyperiod,compperiod,currentvalue):\n",
    "            if compperiod is not None:\n",
    "                PeriodValuePrefix =  compperiod.split(\"-\")[0][:1]\n",
    "                if PeriodValuePrefix =='Y':\n",
    "                    legacyperiod=currentvalue + '-12'\n",
    "                elif PeriodValuePrefix =='9':\n",
    "                    legacyperiod=currentvalue + '-09'\n",
    "                elif PeriodValuePrefix =='H':\n",
    "                    legacyperiod=currentvalue + '-06'\n",
    "                elif PeriodValuePrefix =='Q':\n",
    "                    legacyperiod=currentvalue + '-03'\n",
    "                elif PeriodValuePrefix =='M':\n",
    "                    legacyperiod=currentvalue + '-01'\n",
    "                return legacyperiod\n",
    "            else:\n",
    "                return legacyperiod\n",
    "        getLeagacyPeriod2_udf = F.udf(getLeagacyPeriod2, StringType()) \n",
    "\n",
    "        #update legacyperiod value where it is Null\n",
    "        def getLeagacyPeriod3(legacyperiod,NumericalPeriodTypeValue,currentvalue):\n",
    "            if legacyperiod is None:\n",
    "                if len(NumericalPeriodTypeValue)==1:\n",
    "                    legacyperiod = currentvalue + '-' + '0' + NumericalPeriodTypeValue\n",
    "                else:\n",
    "                    legacyperiod = currentvalue + '-' + NumericalPeriodTypeValue\n",
    "                \n",
    "            return legacyperiod\n",
    "        getLeagacyPeriod3_udf = F.udf(getLeagacyPeriod3, StringType()) \n",
    "\n",
    "        #update legacyperiod value where PeriodValuePrefix =1 i.e. Y9HQM format\n",
    "        def getLeagacyPeriod4(currentvalue,PeriodValuePrefix):\n",
    "            legacyperiod=\"\"\n",
    "            if PeriodValuePrefix =='Y':\n",
    "                legacyperiod=currentvalue + '-12'\n",
    "            elif PeriodValuePrefix =='9':\n",
    "                legacyperiod=currentvalue + '-09'\n",
    "            elif PeriodValuePrefix =='H':\n",
    "                legacyperiod=currentvalue + '-06'\n",
    "            elif PeriodValuePrefix =='Q':\n",
    "                legacyperiod=currentvalue + '-03'\n",
    "            elif PeriodValuePrefix =='M':\n",
    "                legacyperiod=currentvalue + '-01'\n",
    "            return legacyperiod\n",
    "        getLeagacyPeriod4_udf = F.udf(getLeagacyPeriod4, StringType())\n",
    "\n",
    "        #update legacyperiod on the basis of compperiod value-Y9HQM format\n",
    "        def getLeagacyPeriod5(compperiod,Value):\n",
    "            legacyperiod=\"\"\n",
    "            Value=Value[:1]\n",
    "            if Value =='Y':\n",
    "                legacyperiod=compperiod + '-12'\n",
    "            elif Value =='9':\n",
    "                legacyperiod=compperiod + '-09'\n",
    "            elif Value =='H':\n",
    "                legacyperiod=compperiod + '-06'\n",
    "            elif Value =='Q':\n",
    "                legacyperiod=compperiod + '-03'\n",
    "            elif Value =='M':\n",
    "                legacyperiod=compperiod + '-01'\n",
    "            return legacyperiod\n",
    "        getLeagacyPeriod5_udf = F.udf(getLeagacyPeriod5, StringType())\n",
    "\n",
    "        \n",
    "        def getExtractLastValue(compperiod):\n",
    "            return compperiod.split(\"-\")[2]\n",
    "        getExtractLastValue_udf = F.udf(getExtractLastValue,StringType())\n",
    "        df_complist = df_complist.withColumn(\"Right6_compperiod\",getExtractLastValue_udf(F.col(\"compperiod\")))\n",
    "        \n",
    "\n",
    "        #df_dates \n",
    "        if ( (countofcomp ==0 or countofcomp ==1) and PeriodValueInYHQM ==1):  \n",
    "            print(\"inside if\")\n",
    "            data_tmp_dates2 = [(\"CurrentPeriod\",CurrentPeriod,\"\")\\\n",
    "            ,(\"CurrentPeriod_YTD\",CurrentPeriod_YTD,\"\")\\\n",
    "            ,(\"PreviousPeriod\",PreviousPeriod,\"\")\\\n",
    "            ,(\"PreviousYear\",PreviousYear,\"\")\\\n",
    "            ,(\"PrevYearCurrPeriod\",PrevYearCurrPeriod,\"\")\\\n",
    "            ,(\"PrevYearCurrPeriod_YTD\",PrevYearCurrPeriod_YTD,\"\")]\n",
    "            df_tmpdates2 = spark.createDataFrame(data = data_tmp_dates2,schema=schema_data_tmp_dates)\n",
    "            df_tmpdates = df_tmpdates.union(df_tmpdates2)\n",
    "            \n",
    "            list_compperiodname_ColData = [x[\"compperiodname\"] for x in df_usergivenmulticomp.select(\"compperiodname\").collect()]\n",
    "            print(list_compperiodname_ColData)\n",
    "            df_tmpdates = df_tmpdates.where(~df_tmpdates.periodtype.isin(list_compperiodname_ColData))  \n",
    "            \n",
    "            df_dates2 = df_tmpdates.union(df_usergivenmulticomp.select(\"compperiodname\",\"compperiod\").withColumn(\"EmptyCol\",F.lit(\"\")))\n",
    "            df_dates = df_dates.union(df_dates2)            \n",
    "\n",
    "            #update legacy col\n",
    "            df_dates = df_dates.withColumn(\"legacyperiod\",getLeagacyPeriod_udf(F.col(\"periodtype\"),F.col(\"currentvalue\"),F.lit(PeriodValuePrefix)))            \n",
    "\n",
    "            #update legacy col\n",
    "            df_dates =  df_dates.join(df_complist,how=\"left\",on= df_dates.currentvalue == df_complist.Right6_compperiod).withColumn(\"legacyperiod\",getLeagacyPeriod2_udf(F.col(\"legacyperiod\"),F.col(\"compperiod\"),F.col(\"currentvalue\"))).select(\"periodtype\",\"currentvalue\",\"legacyperiod\")\n",
    "        \n",
    "            if NumericalValueofPeriodType >= 0:\n",
    "                NumericalPeriodTypeValue = NumericalValueofPeriodType\n",
    "            else:\n",
    "                NumericalPeriodTypeValue = -NumericalValueofPeriodType\n",
    "            print(\"outside NumericalValueofPeriodType\")\n",
    "            df_dates = df_dates.withColumn(\"legacyperiod\",getLeagacyPeriod3_udf(F.col(\"legacyperiod\"),F.lit(NumericalPeriodTypeValue),F.col(\"currentvalue\")))\n",
    "            \n",
    "        else:\n",
    "            print(\"inside else part\")\n",
    "            data_tmp_dates2 = [(\"CurrentPeriod\",CurrentPeriod,\"\")]\n",
    "            df_tmpdates2 = spark.createDataFrame(data = data_tmp_dates2,schema=schema_data_tmp_dates)\n",
    "            df_tmpdates = df_tmpdates.union(df_tmpdates2)\n",
    "            \n",
    "            df_temp1 = df_tmpdates.withColumn(\"legacyperiod\",getLeagacyPeriod4_udf(F.col(\"currentvalue\"),F.lit(PeriodValuePrefix)))\n",
    "            df_temp2 = df_usergivenmulticomp.withColumn(\"legacyperiod\",getLeagacyPeriod5_udf(F.col(\"compperiod\"),F.col(\"Value\"))).select(\"compperiodname\",\"compperiod\",\"legacyperiod\")\n",
    "            \n",
    "            df_dates = df_temp1.union(df_temp2)\n",
    "       \n",
    "        print(\"1\")\n",
    "        # df_daterangetoperiodtype\n",
    "        df_daterangetoperiodtype = df_daterangetoperiod.join(df_compperioddetails,how=\"left\",on= df_daterangetoperiod.cpid==df_compperioddetails.comptype)\n",
    "        df_daterangetoperiodtype = df_daterangetoperiodtype.withColumn(\"alias\",F.when(F.col(\"alias\").isNull(),F.lit(\"CurrentPeriod\")).otherwise(F.col(\"alias\")))\\\n",
    "                                                            .withColumnRenamed(\"alias\",\"daterangeperiodtype\").withColumnRenamed(\"id\",\"daterangetoperiodtype_id\").drop(\"comptype\")\n",
    "        print(\"------df_daterangetoperiodtype.display()-------\")\n",
    "        df_daterangetoperiodtype.display()\n",
    "        print(\"2\")\n",
    "        #Get df_legacysddates from df_dates\n",
    "        df_legacysddates_new = df_legacysddates.select(\"periodtype\", \"currentvalue\", \"legacyperiod\",\"pcurrentvalue\", \"categoryid\", \"castingperiod\").\\\n",
    "                                            union(df_dates.withColumn(\"pcurrentvalue\",F.col(\"currentvalue\")).withColumn(\"categoryid\",F.lit(0)).withColumn(\"castingperiod\",F.lit(\"12\")))   \\\n",
    "            .withColumn(\"categoryid\",F.lit(0)).withColumn(\"castingperiod\",F.lit(\"12\")).withColumn(\"id\",F.lit(None)).withColumn(\"Value\",F.lit(None)).withColumn(\"financialyear\",F.lit(None)).withColumn(\"openingperiodvalue\",F.lit(None)).withColumn(\"numericvalueofperiod\",F.lit(None)).withColumn(\"ftpperiod\",F.lit(None)).withColumn(\"periodformat\",F.lit(None)).withColumn(\"p\",F.lit(None)).withColumn(\"ppp\",F.lit(None)).withColumn(\"py\",F.lit(None)).withColumn(\"pypp\",F.lit(None)).withColumn(\"ppy\",F.lit(None)).withColumn(\"p3y\",F.lit(None)).withColumn(\"p4y\",F.lit(None)).select(list_col_legacysddates[1:])\n",
    "        print(\" --- df_legacysddates_new   222 ---\")     \n",
    "        df_legacysddates_new.display()\n",
    "        # applying schema on df_legacysddates\n",
    "        print(\" df_legacysddates schema :\")\n",
    "        df_legacysddates.printSchema()\n",
    "        print(\" df_legacysddates_new schema     NEW:\")\n",
    "        df_legacysddates_new.printSchema()\n",
    "        print()\n",
    "        df_legacysddates = df_legacysddates.union(df_legacysddates_new) #issue\n",
    "        print(\"list_col_legacysddates\",list_col_legacysddates)\n",
    "        # adding id column in df_legacysddates\n",
    "        df_legacysddates = df_legacysddates.withColumn(\"monotonically_increasing_id\",F.monotonically_increasing_id()+1)\n",
    "        window = Window.orderBy(col('monotonically_increasing_id'))\n",
    "        df_legacysddates = df_legacysddates.withColumn('id', F.row_number().over(window)).select(list_col_legacysddates)   \n",
    "        print(\" --- df_legacysddates ---\")     \n",
    "        df_legacysddates.display()\n",
    "        print(\"3\")\n",
    "        def getperiodformat(customdaterange,legacyperiod):\n",
    "            periodformat=\"\"\n",
    "            if customdaterange !=1 or customdaterange is None:\n",
    "                if legacyperiod[7:]=='12':\n",
    "                    periodformat='Yearly'\n",
    "                elif legacyperiod[7:]=='06':\n",
    "                    periodformat='Halfyearly'\n",
    "                elif legacyperiod[7:]=='03':\n",
    "                    periodformat='Quarterly'\n",
    "                elif legacyperiod[7:]=='01':\n",
    "                    periodformat='Monthly'\n",
    "                elif legacyperiod[7:]=='09':\n",
    "                    periodformat='9 Months'\n",
    "                else:\n",
    "                    periodformat='Monthly'\n",
    "            elif customdaterange==1:\n",
    "                periodformat='Monthly'\n",
    "            return periodformat\n",
    "\n",
    "        getperiodformat_udf = F.udf(getperiodformat, StringType())\n",
    "\n",
    "        #getnumericvalueofperiod\n",
    "        def getnumericvalueofperiod(Value,periodtype,openperiodvalue,monthdiff):\n",
    "            numericvalueofperiod=None\n",
    "            if PeriodValueInYHQM == 1:\n",
    "                if Value is None:\n",
    "                    return numericvalueofperiod\n",
    "                else:\n",
    "                    if Value[:1] == \"Y\" :\n",
    "                        numericvalueofperiod= -12\n",
    "                    elif Value[:1] == \"9\" :\n",
    "                        numericvalueofperiod= -9\n",
    "                    elif Value[:1] == \"H\" :\n",
    "                        numericvalueofperiod= -6\n",
    "                    elif Value[:1] == \"Q\" :\n",
    "                        numericvalueofperiod= -3\n",
    "                    elif Value[:1] == \"M\" :\n",
    "                        numericvalueofperiod= -1\n",
    "            elif PeriodValueInYHQM == 0:\n",
    "                print(\"inside elif PeriodValueInYHQM==0\")\n",
    "                if ( (periodtype.find('Year') != -1) and (openperiodvalue is None)):\n",
    "                    print(\"inside if\")\n",
    "                    numericvalueofperiod = -12\n",
    "                    #return numericvalueofperiod\n",
    "                else:\n",
    "                    if monthdiff is None:\n",
    "                        print(\"monthdiff is None\")\n",
    "                        numericvalueofperiod= None\n",
    "                        return numericvalueofperiod \n",
    "                    print(\"inside else\")\n",
    "                    monthdiff = - monthdiff\n",
    "                    numericvalueofperiod = monthdiff\n",
    "                    #return numericvalueofperiod\n",
    "            return numericvalueofperiod\n",
    "\n",
    "        getnumericvalueofperiod_udf = F.udf(getnumericvalueofperiod, IntegerType()) \n",
    "\n",
    "        def getcastingperiod(Value,openingperiodvalue,currentvalue):\n",
    "            castingperiod=None\n",
    "            if Value is None or openingperiodvalue is None or currentvalue is None:\n",
    "                return None\n",
    "            else:\n",
    "                castingperiod = Value + '-' + openingperiodvalue + '-' + currentvalue\n",
    "\n",
    "            return castingperiod\n",
    "        getgetcastingperiod_udf = F.udf(getcastingperiod, StringType())    \n",
    "\n",
    "        def getFTPPeriod(Value):\n",
    "            FTPPeriod=None\n",
    "            if PeriodValueInYHQM == 1:\n",
    "                if Value is None:\n",
    "                    FTPPeriod = 1\n",
    "                    return FTPPeriod\n",
    "                else:\n",
    "                    if Value[:1] == \"Y\" :\n",
    "                        FTPPeriod= 5\n",
    "                    elif Value[:1] == \"9\" :\n",
    "                        FTPPeriod = 4\n",
    "                    elif Value[:1] == \"H\" :\n",
    "                        FTPPeriod = 3\n",
    "                    elif Value[:1] == \"Q\" :\n",
    "                        FTPPeriod = 2\n",
    "                    elif Value[:1] == \"M\" :\n",
    "                        FTPPeriod = 1\n",
    "                    \n",
    "            elif PeriodValueInYHQM == 0:\n",
    "                FTPPeriod = 1\n",
    "            return int(FTPPeriod)\n",
    "\n",
    "        getFTPPeriod_udf = F.udf(getFTPPeriod, IntegerType())\n",
    "\n",
    "        # from datetime import datetime, timedelta\n",
    "        def getpy(currentvalue):\n",
    "            if currentvalue is None:\n",
    "                    return None\n",
    "            else:\n",
    "                    dayValue=\"01\"\n",
    "                    monthsValue = currentvalue[4:]\n",
    "                    yearValue = currentvalue[:4]\n",
    "                    date_currentvalue = datetime(int(yearValue), int(monthsValue), int(dayValue))\n",
    "                    new_datevalue = date_currentvalue - relativedelta(months = 12) # year-1\n",
    "                    req_yr = new_datevalue.strftime(\"%Y\")\n",
    "                    req_month= new_datevalue.strftime(\"%m\")\n",
    "                    return req_yr+req_month\n",
    "\n",
    "        getpy_udf = F.udf(getpy, StringType())\n",
    "\n",
    "        def getppy(currentvalue):\n",
    "            if currentvalue is None:\n",
    "                return None\n",
    "            else:\n",
    "                dayValue=\"01\"\n",
    "                monthsValue = currentvalue[4:]\n",
    "                yearValue = currentvalue[:4]\n",
    "                date_currentvalue = datetime(int(yearValue), int(monthsValue), int(dayValue))\n",
    "                new_datevalue = date_currentvalue - relativedelta(months = 24) # year-2\n",
    "                req_yr = new_datevalue.strftime(\"%Y\")\n",
    "                req_month= new_datevalue.strftime(\"%m\")\n",
    "                return req_yr+req_month\n",
    "\n",
    "        getppy_udf = F.udf(getppy, StringType())\n",
    "\n",
    "        def getp3y(currentvalue):\n",
    "            if currentvalue is None:\n",
    "                return None\n",
    "            else:\n",
    "                dayValue=\"01\"\n",
    "                monthsValue = currentvalue[4:]\n",
    "                yearValue = currentvalue[:4]\n",
    "                date_currentvalue = datetime(int(yearValue), int(monthsValue), int(dayValue))\n",
    "                new_datevalue = date_currentvalue - relativedelta(months = 36) # year-3\n",
    "                req_yr = new_datevalue.strftime(\"%Y\")\n",
    "                req_month= new_datevalue.strftime(\"%m\")\n",
    "                return req_yr+req_month\n",
    "\n",
    "        getp3y_udf = F.udf(getp3y, StringType()) \n",
    "\n",
    "\n",
    "        def getp4y(currentvalue):\n",
    "            if currentvalue is None:\n",
    "                return None\n",
    "            else:\n",
    "                dayValue=\"01\"\n",
    "                monthsValue = currentvalue[4:]\n",
    "                yearValue = currentvalue[:4]\n",
    "                date_currentvalue = datetime(int(yearValue), int(monthsValue), int(dayValue))\n",
    "                new_datevalue = date_currentvalue - relativedelta(months = 48) # year-4\n",
    "                req_yr = new_datevalue.strftime(\"%Y\")\n",
    "                req_month= new_datevalue.strftime(\"%m\")\n",
    "                return req_yr+req_month\n",
    "\n",
    "        getp4y_udf = F.udf(getp4y, StringType())\n",
    "\n",
    "\n",
    "        def getp(currentvalue,numericvalueofperiod):\n",
    "            if currentvalue is None or numericvalueofperiod is None:\n",
    "                return None\n",
    "            else:\n",
    "                dayValue=\"01\"\n",
    "                monthsValue = currentvalue[4:]\n",
    "                yearValue = currentvalue[:4]\n",
    "                date_currentvalue = datetime(int(yearValue), int(monthsValue), int(dayValue))\n",
    "                new_datevalue = date_currentvalue + relativedelta(months = int(numericvalueofperiod))\n",
    "                req_yr = new_datevalue.strftime(\"%Y\")\n",
    "                req_month= new_datevalue.strftime(\"%m\")\n",
    "                return req_yr+req_month\n",
    "\n",
    "        getp_udf = F.udf(getp, StringType())\n",
    "\n",
    "        def getppp(currentvalue,numericvalueofperiod):\n",
    "            if currentvalue is None or numericvalueofperiod is None:\n",
    "                return None\n",
    "            else:\n",
    "                dayValue=\"01\"\n",
    "                monthsValue = currentvalue[4:]\n",
    "                yearValue = currentvalue[:4]\n",
    "                date_currentvalue = datetime(int(yearValue), int(monthsValue), int(dayValue))\n",
    "                new_datevalue = date_currentvalue + relativedelta(months = 2 * int(numericvalueofperiod))\n",
    "                req_yr = new_datevalue.strftime(\"%Y\")\n",
    "                req_month= new_datevalue.strftime(\"%m\")\n",
    "                return req_yr+req_month\n",
    "\n",
    "        getppp_udf = F.udf(getppp, StringType()) \n",
    "\n",
    "        def getpypp(currentvalue,numericvalueofperiod):\n",
    "            if currentvalue is None or numericvalueofperiod is None:\n",
    "                return None\n",
    "            else:\n",
    "                dayValue=\"01\"\n",
    "                monthsValue = currentvalue[4:]\n",
    "                yearValue = currentvalue[:4]\n",
    "                date_currentvalue = datetime(int(yearValue), int(monthsValue), int(dayValue))\n",
    "                new_datevalue = date_currentvalue + relativedelta(months = -12 + int(numericvalueofperiod))\n",
    "                req_yr = new_datevalue.strftime(\"%Y\")\n",
    "                req_month= new_datevalue.strftime(\"%m\")\n",
    "                return req_yr+req_month\n",
    "\n",
    "        getpypp_udf = F.udf(getpypp, StringType())\n",
    "\n",
    "        #update  periodformat\n",
    "        df_joined_ld_dp= df_legacysddates.alias(\"ld\").join(df_daterangetoperiodtype.alias(\"dp\"),how=\"left\",\n",
    "        on= [( F.col(\"ld.currentvalue\") == F.col(\"dp.tbperiodvalue\")) & (F.col(\"ld.periodtype\") == F.col(\"dp.daterangeperiodtype\"))])\n",
    "        df_legacysddates = df_joined_ld_dp.withColumn(\"periodformat\", getperiodformat_udf(F.col(\"customdaterange\"),F.col(\"legacyperiod\"))).select(list_col_legacysddates)\n",
    "\n",
    "        # update Value , financialyear and openingperiodvalue by joining periodicaldates_df , df_legacysddates and df_daterangetoperiodtype \n",
    "        #Renaming column Name to avoid ambiguity\n",
    "        periodicaldates_df = periodicaldates_df.withColumnRenamed(\"Value\",\"periodicaldates_Value\").withColumnRenamed(\"Opening\",\"periodicaldates_Opening\").withColumnRenamed(\"Year\",\"periodicaldates_Year\")\n",
    "\n",
    "        df_joined_pd_ld_drtpt = periodicaldates_df.alias(\"pds\").join(df_legacysddates.alias(\"lds\"), col(\"pds.Closing\") == col(\"lds.currentvalue\"), \"inner\") \\\n",
    "            .join(df_daterangetoperiodtype.alias(\"dpt\"), (col(\"lds.currentvalue\") == col(\"dpt.tbperiodvalue\")) & (col(\"lds.periodtype\") == col(\"dpt.daterangeperiodtype\")),how= \"left_outer\") .filter((periodicaldates_df[\"financialcycleid\"] == FiscalPeriodID) & (periodicaldates_df[\"Type\"] == df_legacysddates[\"periodformat\"]))\n",
    "        \n",
    "\n",
    "        #Renaming column Name to avoid ambiguity\n",
    "        for columnsName in df_joined_pd_ld_drtpt.columns:\n",
    "            df_joined_pd_ld_drtpt= df_joined_pd_ld_drtpt.withColumnRenamed(columnsName,\"joined_\"+columnsName)\n",
    "        \n",
    "\n",
    "        legacysddates_df2_temp = df_legacysddates.alias(\"ld\").join(df_joined_pd_ld_drtpt.alias(\"pdlddrtpt\"),how=\"left\",on= (col(\"ld.periodtype\")==col(\"pdlddrtpt.joined_periodtype\") ) )\\\n",
    "                            .withColumn(\"Value\", col(\"joined_periodicaldates_Value\")) \\\n",
    "                            .withColumn(\"financialyear\", col(\"joined_pd_Year\")) \\\n",
    "                            .withColumn(\"openingperiodvalue\", when(col(\"joined_openperiodvalue\").isNotNull(), col(\"joined_openperiodvalue\")).otherwise(col(\"joined_periodicaldates_Opening\")))\n",
    "        \n",
    "        df_legacysddates = legacysddates_df2_temp.select(list_col_legacysddates)\n",
    "        \n",
    "\n",
    "        #update numericvalueofperiod \n",
    "        legacysddates_df_temp_ld_pt = df_legacysddates.alias(\"ld2\").join(df_daterangetoperiodtype.alias(\"dpt2\"), on=[ (col(\"ld2.currentvalue\") == col(\"dpt2.tbperiodvalue\")) & (col(\"ld2.periodtype\") == col(\"dpt2.daterangeperiodtype\"))], how=\"left\")\\\n",
    "                                    .withColumn(\"numericvalueofperiod\", getnumericvalueofperiod_udf(F.col(\"Value\"),F.col(\"periodtype\"),F.col(\"openperiodvalue\"),F.col(\"monthdiff\")))\n",
    "        df_legacysddates= legacysddates_df_temp_ld_pt.select(list_col_legacysddates)\n",
    "\n",
    "        # update numericvalueofperiod column , where numericvalueofperiod is null replace numericvalueofperiod by -monthDiff value\n",
    "        update_expr = -col(\"monthdiff\")\n",
    "        legacysddates_df4 = df_legacysddates.join(df_daterangetoperiod.filter(col(\"id\") == 1), \"id\", \"left_outer\").withColumn(\"numericvalueofperiod\", when(col(\"numericvalueofperiod\").isNull(), update_expr).otherwise(col(\"numericvalueofperiod\"))) \n",
    "        df_legacysddates = legacysddates_df4.select(list_col_legacysddates)\n",
    "\n",
    "        # Update castingperiod, FTPPeriod , p , py, ppy, p3y, p4y, ppp , pypp\n",
    "        df_legacysddates = df_legacysddates.withColumn(\"castingperiod\",getgetcastingperiod_udf(F.col(\"Value\"),F.col(\"openingperiodvalue\"),F.col(\"currentvalue\")))\n",
    "        df_legacysddates = df_legacysddates.withColumn(\"ftpperiod\",getFTPPeriod_udf(F.col(\"Value\")))\n",
    "        legacysddates_src_df = df_legacysddates.withColumn(\"p\",getp_udf(F.col(\"currentvalue\"),F.col(\"numericvalueofperiod\")))\\\n",
    "                                            .withColumn(\"py\",getpy_udf(F.col(\"currentvalue\")))\\\n",
    "                                            .withColumn(\"ppy\",getppy_udf(F.col(\"currentvalue\")))\\\n",
    "                                            .withColumn(\"p3y\",getp3y_udf(F.col(\"currentvalue\")))\\\n",
    "                                            .withColumn(\"p4y\",getp4y_udf(F.col(\"currentvalue\")))\\\n",
    "                                            .withColumn(\"ppp\",getppp_udf(F.col(\"currentvalue\"),F.col(\"numericvalueofperiod\")))\\\n",
    "                                            .withColumn(\"pypp\",getpypp_udf(F.col(\"currentvalue\"),F.col(\"numericvalueofperiod\")))\n",
    "        legacysddates_src_df.coalesce(1).write.format(\"parquet\").mode(\"overwrite\").save(adls_path+folder_path+\"legacysddates\")\n",
    "        return legacysddates_src_df,countofcomp\n",
    "    except Exception as e:\n",
    "        return \"error\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aabcb3e7-36c6-4b1f-94ed-996b63f59193",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "legacysddates_src_df,countofcomp = get_legacydates(periodicaldates_src_df,df_legacysddates,df_daterangetoperiod,schema_daterangetoperiod,df_usergivenmulticomp,df_daterangeconverttoperiod,df_daterangecustomperiod,df_compperioddetails,AccountingStandardID,PeriodValue)\n",
    "\n",
    "legacysddates_src_df = spark.read.parquet(adls_path+folder_path+\"legacysddates\",schema = legacysddates_schema)\n",
    "period_value_ld = legacysddates_src_df.where(col(\"periodtype\")=='CurrentPeriod').select(\"legacyperiod\").first()[0][:-3]\n",
    "print(\"period_value_ld\",period_value_ld)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "59f0d9b3-49c1-43ea-9a5c-d763873386dc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"inside get_legacydates done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "581939dc-79df-49a7-afd6-3298935dabc1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Read data from trialbalance, castingadjustments, adjustments and legacyadjustments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b5363b8b-652c-40a4-b872-bcb478bdb012",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"Read data from trialbalance, castingadjustments, adjustments and legacyadjustments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0d20a830-cd0b-4bbf-b726-61631abfe80f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "reportrunlog_df = reportrunlog_src_uc_df.filter(col(\"ParameterID_fk\") == parameterID)\n",
    "if reportrunlog_df.count()==0:\n",
    "    print(\"if no entry found in reportlog means first run\")\n",
    "    # trialbalance_src_uc_df.display()\n",
    "\n",
    "    trialbalance_src_uc_df = trialbalance_src_uc_df.where(col(\"Period\").substr(1, 6)==period_value_ld)\n",
    "    trialbalance_src_df = trialbalance_src_uc_df.filter((trialbalance_src_uc_df['EntityID_fk'] == EntityID) & (trialbalance_src_uc_df['AccountingStdID_fk'] == AccountingStandardID) & ((trialbalance_src_uc_df['period'].substr(1, 6)).isin([str(row.closing) for row in periodicaldates_src_df.filter((col(\"financialcycleid\") == FiscalPeriodID) & (col(\"year\") == FinancialYear)).select(\"closing\").collect()])) & (col(\"IsActive\") == True))\n",
    "\n",
    "    # print(\"++++++++++ trialbalance_src_df ++++++\")\n",
    "    # trialbalance_src_df.display()\n",
    "\n",
    "    trialbalance_dim_df = trialbalance_src_df.filter((col(\"Createddate\") <= LastExecutionDate)).select(\"TrailbalanceID\",\"Period\",\"EntityID_fk\",\"AccountingStdID_fk\",\"Userglcode\",\"Usergldescription\",\"Member\",\"CurrencyID_fk\",\"openingbalance\",\"debitamount\",\"creditamount\",\"closingbalance\",\"Createddate\",*alldimcols)\n",
    "    print(\"++++++++++ trialbalance_dim_df ++++++\")\n",
    "    trialbalance_dim_df.display()\n",
    "\n",
    "    castingadjustment_dim_df = castingadjustment_src_uc_df.filter((col(\"Fk_EntityID\") == EntityID) & (col(\"Createddate\") <= LastExecutionDate)).select(\"CastingAdjustmentsID\",\"Fk_AccountingStandardID\",\"Fk_ValidationID\",\"PeriodValue\",\"FinancialYear\",\"Fk_EntityID\",\"EntityName\", \"JournalNumber\", \"JournalDate\", \"UserGLCode\", \"UserGLDescription\", \"StandardGLCode\",\"StandardGLDescription\",\"DebitCredit\",\"Fk_CurrencyID\", \"Amount\", \"JournalType\", \"Narration\", \"Fk_BusinessUnitID\", \"CategoryID\", \"Roundoff\", \"AmountsIn\", \"ReportType\", \"CreatedDate\", \"CreatedBy\", \"ModifiedDate\", \"ModifiedBy\", \"IsUserGLCode\", \"RoundTypeLevel\",*alldimcols)\n",
    "\n",
    "\n",
    "    standaloneadjustments_dim_df = standaloneadjustments_src_uc_df.filter((col(\"EntityID_fk\")== EntityID) & (col(\"Createddate\") <= LastExecutionDate) & (col(\"IsActive\") == True)).select(\"AdjustmentID\", \"CategoryID_fk\", \"AdjustmentDisclosureType\", \"AccountingStdID_fk\", \"Period\", \"Year\", \"EntityID_fk\", \"UserGLcode\", \"Member\", \"DebitCredit\", \"CurrencyID_fk\", \"Amount\", \"IsImported\",\"Journalnumber\",\"Journaldate\",\"Journaltype\",\"narration\",\"IsUserGLCodeAdjustment\",\"Disclosureremarks\",\"AccountType\",\"AccountSubType\",\"Createdby\",\"Createddate\",\"Modifiedby\",\"Modifieddate\",\"IsActive\",*alldimcols)\n",
    "\n",
    "    legacyadjustments_dim_df = legacyadjustments_src_uc_df.filter((col(\"EntityID_fk\")== EntityID) & (col(\"Createddate\") <= LastExecutionDate) & (col(\"IsActive\") == True)).select(\"LegacyAdjustmentID\", \"Period\", \"EntityID_fk\", \"AccountingStdID_fk\",\"Journalnumber\",\"Journaldate\",\"Userglcode\",\"Member\",\"debitcredit\",\"CurrencyID_fk\",\"Amount\",\"Journaltype\",\"Narration\",\"Createdby\",\"Createddate\",\"Modifiedby\",\"Modifieddate\",\"BUID_fk\",\"IsImported\",\"CategoryID\",\"IsMappedGLCode\",\"EndPeriod\",\"IsActive\",\"NotApplicable\",*alldimcols)\n",
    "\n",
    "    insertReportRunLog(reportrunlog_src_uc_df,reportrunlog,parameterID,CategoryID)\n",
    "\n",
    "else:\n",
    "    print(\"else entry found in reportlog means not a first run\")\n",
    "    # trialbalance_src_uc_df.display()\n",
    "\n",
    "    trialbalance_src_uc_df = trialbalance_src_uc_df.where(col(\"Period\").substr(1, 6)==period_value_ld)\n",
    "    trialbalance_src_df = trialbalance_src_uc_df.filter((trialbalance_src_uc_df['EntityID_fk'] == EntityID) & (trialbalance_src_uc_df['AccountingStdID_fk'] == AccountingStandardID) & ((trialbalance_src_uc_df['period'].substr(1, 6)).isin([str(row.closing) for row in periodicaldates_src_df.filter((col(\"financialcycleid\") == FiscalPeriodID) & (col(\"year\") == FinancialYear)).select(\"closing\").collect()])) & (col(\"IsActive\") == True))\n",
    "\n",
    "    # print(\"++++++++++ trialbalance_src_df ++++++\")\n",
    "    # trialbalance_src_df.display()\n",
    "\n",
    "    trialbalance_dim_df = trialbalance_src_df.filter((col(\"Createddate\") >= LastExecutionDate) | (col(\"Modifieddate\") >= LastExecutionDate)).select(\"TrailbalanceID\",\"Period\",\"EntityID_fk\",\"AccountingStdID_fk\",\"Userglcode\",\"Usergldescription\",\"Member\",\"CurrencyID_fk\",\"openingbalance\",\"debitamount\",\"creditamount\",\"closingbalance\",\"Createddate\",*alldimcols)\n",
    "    print(\"++++++++++ trialbalance_dim_df ++++++\")\n",
    "    trialbalance_dim_df.display()\n",
    "\n",
    "    castingadjustment_dim_df = castingadjustment_src_uc_df.filter((col(\"Fk_EntityID\") == EntityID) & (col(\"Createddate\") >= LastExecutionDate) |(col(\"Modifieddate\") >= LastExecutionDate)).select(\"CastingAdjustmentsID\",\"Fk_AccountingStandardID\",\"Fk_ValidationID\",\"PeriodValue\",\"FinancialYear\",\"Fk_EntityID\",\"EntityName\", \"JournalNumber\", \"JournalDate\", \"UserGLCode\", \"UserGLDescription\", \"StandardGLCode\",\"StandardGLDescription\",\"DebitCredit\",\"Fk_CurrencyID\", \"Amount\", \"JournalType\", \"Narration\", \"Fk_BusinessUnitID\", \"CategoryID\", \"Roundoff\", \"AmountsIn\", \"ReportType\", \"CreatedDate\", \"CreatedBy\", \"ModifiedDate\", \"ModifiedBy\", \"IsUserGLCode\", \"RoundTypeLevel\",*alldimcols)\n",
    "\n",
    "    standaloneadjustments_dim_df = standaloneadjustments_src_uc_df.filter((col(\"EntityID_fk\")== EntityID) & (col(\"Createddate\") >= LastExecutionDate) | (col(\"Modifieddate\") >= LastExecutionDate) & (col(\"IsActive\") == True)).select(\"AdjustmentID\", \"CategoryID_fk\", \"AdjustmentDisclosureType\", \"AccountingStdID_fk\", \"Period\", \"Year\", \"EntityID_fk\", \"UserGLcode\", \"Member\", \"DebitCredit\", \"CurrencyID_fk\", \"Amount\", \"IsImported\",\"Journalnumber\",\"Journaldate\",\"Journaltype\",\"narration\",\"IsUserGLCodeAdjustment\",\"Disclosureremarks\",\"AccountType\",\"AccountSubType\",\"Createdby\",\"Createddate\",\"Modifiedby\",\"Modifieddate\",\"IsActive\",*alldimcols)\n",
    "\n",
    "    legacyadjustments_dim_df = legacyadjustments_src_uc_df.filter((col(\"EntityID_fk\")== EntityID) & (col(\"Createddate\") >= LastExecutionDate) | (col(\"Modifieddate\") >= LastExecutionDate) & (col(\"IsActive\") == True)).select(\"LegacyAdjustmentID\", \"Period\", \"EntityID_fk\", \"AccountingStdID_fk\",\"Journalnumber\",\"Journaldate\",\"Userglcode\",\"Member\",\"debitcredit\",\"CurrencyID_fk\",\"Amount\",\"Journaltype\",\"Narration\",\"Createdby\",\"Createddate\",\"Modifiedby\",\"Modifieddate\",\"BUID_fk\",\"IsImported\",\"CategoryID\",\"IsMappedGLCode\",\"EndPeriod\",\"IsActive\",\"NotApplicable\",*alldimcols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "32a80c29-b7cb-4953-b972-2acbd9d42bff",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_legacy_adjustment_for_mapping(legacyadjustments_dim_df,legacyadjustmentsdrilldown_union_df,catalog_schema_name):\n",
    "    try:\n",
    "        templegcat_df = spark.createDataFrame(\n",
    "        [(category,) for category in CategoryID.split(',')],[\"categoryid\"])\n",
    "        tmplegacywithoutendperiod_df = legacyadjustments_dim_df \\\n",
    "        .filter((col(\"BUID_fk\") == 0) &\n",
    "                (col(\"CategoryID\").isin([row.CategoryID for row in templegcat_df.select(\"CategoryID\").collect()])) &\n",
    "                (col(\"EntityID_fk\") == EntityID) &\n",
    "                (col(\"AccountingStdID_fk\") == AccountingStandardID) &\n",
    "                (coalesce(col(\"EndPeriod\"),lit(\"999999-99\")) > col(\"Period\"))).drop(\"LegacyAdjustmentID\")\n",
    "        \n",
    "        templegacy_df1 = tmplegacywithoutendperiod_df.withColumn(\"split_column\",F.split(F.coalesce(F.col(\"NotApplicable\"), F.lit('')), ','))\n",
    "\n",
    "        # Explode the array created by split to mimic CROSS APPLY\n",
    "        templegacy_df = templegacy_df1.select(\"*\", F.explode(\"split_column\").alias(\"value\"))\n",
    "        tmplegacywithoutna_df = templegacy_df.withColumn(\n",
    "        \"IsmappedGLUpdatedCode\",\n",
    "        when((col(\"Period\") == col(\"value\")), 0)\n",
    "        .when((col(\"NotApplicable\").isNotNull()) & (col(\"NotApplicable\") != '')\n",
    "            & (col(\"value\") == '')\n",
    "            , 0)\n",
    "        .otherwise(1)\n",
    "        )\n",
    "        \n",
    "        templegacy_df = tmplegacywithoutna_df.filter(\n",
    "        (F.col(\"IsmappedGLUpdatedCode\") == 1) &\n",
    "        (~F.col(\"Period\").isin([row.value for row in tmplegacywithoutna_df.select(\"value\").distinct().collect()]))\n",
    "        ).select(\"AccountingStdID_fk\", \"Period\", \"EntityID_fk\",\n",
    "            \"JournalNumber\", \"JournalDate\", \"UserGLCode\",\"Member\",\n",
    "            \"DebitCredit\", \"CurrencyID_Fk\", \"Amount\", \"JournalType\",\n",
    "            \"Narration\", \"CreatedDate\", \"CreatedBy\", \"ModifiedDate\", \"ModifiedBy\",\n",
    "            \"BUID_fk\", \"IsImported\", \"CategoryID\", \"IsMappedGLCode\",\n",
    "            \"EndPeriod\", \"NotApplicable\",*alldimcols).distinct()\n",
    "        \n",
    "        for col_name in legacyadjustmentsdrilldown_union_df.columns:\n",
    "                    if col_name not in templegacy_df.columns:\n",
    "                        templegacy_df = templegacy_df.withColumn(col_name, lit(None).cast(legacyadjustmentsdrilldown_union_df.schema[col_name].dataType))\n",
    "\n",
    "        legacyadjustmentsdrilldown_df = templegacy_df.select(\"AccountingStdID_fk\",\"Period\",\"EntityID_Fk\",\"JournalNumber\",\"JournalDate\",\"UserGLCode\",\"Member\",\"DebitCredit\",\"CurrencyID_Fk\",\"Amount\",\"JournalType\",\"Narration\",\"CreatedDate\",\"CreatedBy\",\"ModifiedDate\",\"ModifiedBy\",\"BUID_fk\",\"IsImported\",\"CategoryID\",\"IsMappedGLCode\",\"EndPeriod\",\"NotApplicable\",\"DimType1ID_fk\",\"DimType2ID_fk\",\"DimType3ID_fk\",\"DimType4ID_fk\",\"DimType5ID_fk\",\"DimType6ID_fk\",\"DimType7ID_fk\",\"DimType8ID_fk\",\"DimType9ID_fk\",\"DimType10ID_fk\",\"DimType11ID_fk\",\"DimType12ID_fk\",\"DimType13ID_fk\",\"DimType14ID_fk\",\"DimType15ID_fk\",\"DimType16ID_fk\",\"DimType17ID_fk\",\"DimType18ID_fk\",\"DimType19ID_fk\",\"DimType20ID_fk\",\"DimType21ID_fk\",\"DimType22ID_fk\",\"DimType23ID_fk\",\"DimType24ID_fk\",\"DimType25ID_fk\")\n",
    "        \n",
    "        legacyadjustmentsdrilldown_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(catalog_schema_name+\".\"+\"legacyadjustmentsdrilldown\")\n",
    "    except Exception as e:\n",
    "        return \"error\"\n",
    "\n",
    "def get_SCOA_mapping(scoa_src_uc_df,standaloneadjustments_dim_df,trialbalance_dim_df,adls_path,folder_path,catalog_schema_name):\n",
    "    try:\n",
    "        legacyadjustmentsdrilldown_df = spark.read.table(catalog_schema_name+\".\"+\"legacyadjustmentsdrilldown\")\n",
    "        AccountingType = AccountingStandardID\n",
    "        allperiodftpscoa_df = scoa_src_uc_df.filter((scoa_src_uc_df[\"mcoayear\"] == FinancialYear) & (scoa_src_uc_df[\"accountingtype\"] == AccountingType))\n",
    "\n",
    "        allperiodftpscoa_df =  allperiodftpscoa_df.select(\"SCOAID\",\"Member\",\"Path\",\"Column27\",\"AccountSubType\",\"UltimateParent\")\n",
    "        allperiodftpscoa_df.coalesce(1).write.format(\"parquet\").mode(\"overwrite\").save(adls_path+folder_path+\"allperiodftpscoa\")\n",
    "        legrecords_df =  legacyadjustmentsdrilldown_df\n",
    "        standaloneadj_df = standaloneadjustments_dim_df.filter((col(\"EntityID_fk\") == EntityID) & (col(\"AccountingStdID_fk\") == AccountingStandardID) & (col(\"CategoryID_fk\") == CategoryID ))\n",
    "\n",
    "        # ---------TB data-----\n",
    "\n",
    "        trialbalance_df = trialbalance_dim_df.select(\n",
    "            col(\"UserGLCode\"),\n",
    "            col(\"UserGLDescription\"),\n",
    "            col(\"EntityID_fk\")\n",
    "        ).filter(\n",
    "            (col(\"EntityID_fk\") == EntityID) &\n",
    "            (col(\"period\").substr(1, 6).isin( [row[\"closing\"] for row in periodicaldates_src_df\n",
    "                .filter((col(\"financialcycleid\") == FiscalPeriodID) & (col(\"year\") == FinancialYear))\n",
    "                .select(\"closing\")\n",
    "                .collect()]\n",
    "            ))\n",
    "        )\n",
    "\n",
    "        #combined tb,adj and leagcy adj\n",
    "        tb_df= trialbalance_df.select('UserGLCode', 'UserGLDescription', 'EntityID_fk') \\\n",
    "            .union(standaloneadj_df.select('UserGLCode',lit(None).alias('UserGLDescription'), 'EntityID_fk')) \\\n",
    "            .union(legrecords_df.select('UserGLCode', lit(None).alias('UserGLDescription'), 'EntityID_fk')) \\\n",
    "            .groupBy(\"UserGLCode\",\"EntityID_fk\") \\\n",
    "            .agg(first(col(\"UserGLDescription\"), True).alias(\"UserGLDescription\"))\n",
    "\n",
    "        # first time tb upload\n",
    "        year_period= periodicaldates_src_df \\\n",
    "            .filter((col(\"FinancialCycleID\") == FiscalPeriodID) & (col(\"Year\") == FinancialYear) & (col(\"Type\") == 'Yearly')) \\\n",
    "            .select(\"closing\") \\\n",
    "            .collect()\n",
    "\n",
    "        year_period_value = year_period[0][\"closing\"] if year_period else \"\"\n",
    "\n",
    "        isautomapped_df = categorydropdown_src_df.filter(categorydropdown_src_df[\"CategoryID\"] == CategoryID) \\\n",
    "        .select(\"IsAutomapped\").collect()\n",
    "\n",
    "        if isautomapped_df:\n",
    "            isautomapped_df = isautomapped_df[0][0]\n",
    "        else:\n",
    "            print(\"No rows found for the given CategoryID.\")\n",
    "        \n",
    "\n",
    "        if isautomapped_df == 1:\n",
    "            print(\"IsAutomapped is 1\")\n",
    "            SCOAMappingTable1_df = tb_df.join(\n",
    "            allperiodftpscoa_df,\n",
    "            tb_df[\"UserGLCode\"] == allperiodftpscoa_df[\"Member\"],\n",
    "            \"inner\").select(\n",
    "            col(\"EntityID_fk\").alias(\"EntityId\"),\n",
    "            concat(lit(year_period_value), lit('-12')).alias(\"Period\"),\n",
    "            lit(AccountingStandardID).alias(\"accountingtype\"),\n",
    "            col(\"Member\").alias(\"EyGlcode\"),\n",
    "            col(\"UserGLCode\"),\n",
    "            lit(CategoryID).alias(\"CategoryID\"),\n",
    "            col(\"UltimateParent\"),\n",
    "            lit(\"admin\").alias(\"createdby\"),\n",
    "            lit(\"admin\").alias(\"modifiedby\"),\n",
    "            lit(current_timestamp()).alias(\"createdDate\"),\n",
    "            lit(current_timestamp()).alias(\"modifiedDate\"),\n",
    "            lit(None).alias(\"PresentationLayer\"),\n",
    "            col(\"UserGLDescription\")\n",
    "            ).join(\n",
    "            masterscoamapping_src_uc_df.filter((col(\"EntityId\") == EntityID) & (col(\"CategoryID\") == CategoryID) & (col(\"mapped\") == 1)),\n",
    "            tb_df[\"UserGLCode\"] == masterscoamapping_src_uc_df[\"UserGLCode\"],\n",
    "            \"left_anti\"\n",
    "            )\n",
    "\n",
    "            SCOAMappingTable_df = SCOAMappingTable1_df.join(\n",
    "            masterscoamapping_src_uc_df.filter((col(\"CategoryID\") == CategoryID) & (col(\"unmapped\") == 1)), \\\n",
    "            SCOAMappingTable1_df[\"UserGLCode\"] == masterscoamapping_src_uc_df[\"UserGLCode\"], \\\n",
    "            \"left_anti\") \\\n",
    "            .select(\"EntityId\", \"Period\", \"accountingtype\", \"EYGLCode\", \"UserGLCode\", \"CategoryID\", \"UltimateParent\", lit(\"admin\").alias(\"CreatedBy\"), lit(\"admin\").alias(\"ModifiedBy\"), lit(current_timestamp()).alias(\"CreatedDate\"), lit(current_timestamp()).alias(\"ModifiedDate\"), \"PresentationLayer\", \"UserGLDescription\") \\\n",
    "            .union(\n",
    "                masterscoamapping_src_uc_df\n",
    "                .filter((col(\"EntityId\") == EntityID) & (col(\"CategoryID\") == CategoryID) & (col(\"mapped\") == 1))\n",
    "                .select(\"EntityID\",\"ReportingFrequency\",\"accountingtype\",\"EYGLCode\",\"UserGLCode\",\"CategoryID\",\"UltimateParent\",\"CreatedBy\",\"modifiedby\",\"CreatedDate\",\"ModifiedDate\",\"PresentationLayer\",\"UserGLDescription\")\n",
    "            )\n",
    "    \n",
    "            SCOAMappingTable_df.coalesce(1).write.format(\"parquet\").mode(\"overwrite\").save(adls_path+folder_path+\"SCOAMappingTable\")\n",
    "            \n",
    "        else:\n",
    "            print(\"IsAutomapped is 0\")\n",
    "            SCOAMappingTable1_df = masterscoamapping_src_uc_df \\\n",
    "                .filter((col(\"EntityId\") == EntityID) & (col(\"CategoryID\") == CategoryID) & (col(\"mapped\") == 1)) \\\n",
    "                .select(\"EntityId\",col(\"ReportingFrequency\").alias(\"Period\"),\"accountingtype\",\"EYGLCode\",\"UserGLCode\",\"CategoryID\",\"UltimateParent\",\"CreatedBy\",\"modifiedby\",\"CreatedDate\",\"ModifiedDate\",\"PresentationLayer\",\"UserGLDescription\")\n",
    "\n",
    "            SCOAMappingTable_df =  SCOAMappingTable1_df \\\n",
    "                .filter((col(\"EntityId\") == EntityID) & (col(\"CategoryID\") == CategoryID) & (col(\"mapped\") == 1)) \\\n",
    "                .select(\"EntityId\",\"Period\",\"accountingtype\",\"EYGLCode\",\"UserGLCode\",\"CategoryID\",\"UltimateParent\",\"CreatedBy\",\"modifiedby\",\"CreatedDate\",\"ModifiedDate\",\"PresentationLayer\",\"UserGLDescription\")\n",
    "\n",
    "            SCOAMappingTable_df.coalesce(1).write.format(\"parquet\").mode(\"overwrite\").save(adls_path+folder_path+\"SCOAMappingTable\")\n",
    "    \n",
    "        SCOAMappingTable_df.display()\n",
    "    except Exception as e:\n",
    "        return \"error\"\n",
    "\n",
    "def get_legacy_adjustment_by_category_drilldown(legacyadjustments_dim_df,legacyadjustmentsdrilldown_union_df,AccountingStandardID,EntityID,LCurrperiod,LCategoryID):\n",
    "    try:\n",
    "        tmplegacywithoutendperiod_df = legacyadjustments_dim_df \\\n",
    "        .filter((col(\"BUID_fk\") == 0) &\n",
    "                (col(\"CategoryID\") == LCategoryID) &\n",
    "                (col(\"EntityID_fk\") == EntityID) &\n",
    "                (col(\"AccountingStdID_fk\") == AccountingStandardID) & (col(\"Period\") <= LCurrperiod) &\n",
    "                (coalesce(col(\"EndPeriod\"),lit(\"999999-99\")) > LCurrperiod)).drop(\"LegacyAdjustmentID\")\n",
    "\n",
    "        templegacy_df1 = tmplegacywithoutendperiod_df.withColumn(\n",
    "            \"split_column\",\n",
    "            F.split(F.coalesce(F.col(\"NotApplicable\"), F.lit('')), ',')\n",
    "        )\n",
    "\n",
    "\n",
    "        # Explode the array created by split to mimic CROSS APPLY\n",
    "        templegacy_df = templegacy_df1.select(\"*\", F.explode(\"split_column\").alias(\"value\"))\n",
    "\n",
    "\n",
    "        tmplegacywithoutna_df = templegacy_df.withColumn(\n",
    "        \"IsmappedGLUpdatedCode\",\n",
    "        when((col(\"Period\") == col(\"value\")), 0)\n",
    "        .when((col(\"NotApplicable\").isNotNull()) & (col(\"NotApplicable\") != '')\n",
    "            & (col(\"value\") == '')\n",
    "            , 0)\n",
    "        .otherwise(1)\n",
    "        )\n",
    "\n",
    "        # reset  ismappedglcode to 0 for na periods\n",
    "        templegacy_df = tmplegacywithoutna_df.filter(\n",
    "        (F.col(\"IsmappedGLUpdatedCode\") == 1) &\n",
    "        (~F.col(\"Period\").isin([row.value for row in tmplegacywithoutna_df.select(\"value\").distinct().collect()]))\n",
    "        ).select(\n",
    "            \"AccountingStdID_fk\", \"Period\", \"EntityID_fk\",\n",
    "            \"JournalNumber\", \"JournalDate\", \"UserGLCode\",\"Member\",\n",
    "            \"DebitCredit\", \"CurrencyID_Fk\", \"Amount\", \"JournalType\",\n",
    "            \"Narration\", \"CreatedDate\", \"CreatedBy\", \"ModifiedDate\", \"ModifiedBy\",\n",
    "            \"BUID_fk\", \"IsImported\", \"CategoryID\", \"IsMappedGLCode\",\n",
    "            \"EndPeriod\", \"NotApplicable\",*alldimcols\n",
    "        ).distinct()\n",
    "\n",
    "        for col_name in legacyadjustmentsdrilldown_union_df.columns:\n",
    "                    if col_name not in templegacy_df.columns:\n",
    "                        templegacy_df = templegacy_df.withColumn(col_name, lit(None).cast(legacyadjustmentsdrilldown_union_df.schema[col_name].dataType))\n",
    "\n",
    "\n",
    "        legacyadjustmentsdrilldown_df = templegacy_df.select(\"AccountingStdID_fk\", \"Period\", \"EntityID_fk\",\n",
    "            \"JournalNumber\", \"JournalDate\", \"UserGLCode\",\"Member\",\n",
    "            \"DebitCredit\", \"CurrencyID_Fk\", \"Amount\", \"JournalType\",\n",
    "            \"Narration\", \"CreatedDate\", \"CreatedBy\", \"ModifiedDate\", \"ModifiedBy\",\n",
    "            \"BUID_fk\", \"IsImported\", \"CategoryID\", \"IsMappedGLCode\",\n",
    "            \"EndPeriod\", \"NotApplicable\",\"DimType1ID_fk\",\"DimType2ID_fk\",\"DimType3ID_fk\",\"DimType4ID_fk\",\"DimType5ID_fk\",\"DimType6ID_fk\",\"DimType7ID_fk\",\"DimType8ID_fk\",\"DimType9ID_fk\",\"DimType10ID_fk\",\"DimType11ID_fk\",\"DimType12ID_fk\",\"DimType13ID_fk\",\"DimType14ID_fk\",\"DimType15ID_fk\",\"DimType16ID_fk\",\"DimType17ID_fk\",\"DimType18ID_fk\",\"DimType19ID_fk\",\"DimType20ID_fk\",\"DimType21ID_fk\",\"DimType22ID_fk\",\"DimType23ID_fk\",\"DimType24ID_fk\",\"DimType25ID_fk\")\n",
    "\n",
    "        return legacyadjustmentsdrilldown_df\n",
    "\n",
    "    except Exception as e:\n",
    "        return \"error\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d4fd9a15-da8f-4ea0-8042-70be8ac6b305",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "get_legacy_adjustment_for_mapping(legacyadjustments_dim_df,legacyadjustmentsdrilldown_union_df,catalog_schema_name)\n",
    "get_SCOA_mapping(scoa_src_uc_df,standaloneadjustments_dim_df,trialbalance_dim_df,adls_path,folder_path,catalog_schema_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5fac2203-bfca-4ee2-a8f4-b53542ac9f94",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if extflag != 1 or (extflag == 1 and extendedmonthdiff <= 12) or (extflag == 1 and extendedmonthdiff > 12 and int(extendedcycleyear) != int(FinancialYear)): \n",
    "    print(\"standard period\")\n",
    "    periodicaldateswithlargeperiod_df = legacysddates_src_df.alias(\"ld\").join(periodicaldates_src_df.alias(\"mp\"),\n",
    "    (col(\"ld.ftpperiod\") >= col(\"mp.Order\")),\n",
    "    \"inner\"\n",
    "    ).filter((col(\"financialcycleid\") == FiscalPeriodID) & (col(\"mp.closing\") <= col(\"ld.currentvalue\")) & (col(\"mp.closing\") >= col(\"ld.openingperiodvalue\"))).select(\n",
    "    col(\"ld.PeriodType\"),\n",
    "    col(\"mp.closing\"),\n",
    "    col(\"ld.LegacyPeriod\"),\n",
    "    col(\"ld.numericvalueofperiod\")\n",
    "    ).distinct()\n",
    "\n",
    "    periodicaldateswithequalperiod_df = legacysddates_src_df.alias(\"ld\").join(periodicaldates_src_df.alias(\"mp\"),\n",
    "    col(\"mp.closing\") <= col(\"ld.currentvalue\"),\n",
    "    \"inner\"\n",
    "    ).filter((col(\"financialcycleid\") == FiscalPeriodID) & (col(\"mp.closing\") >= col(\"ld.openingperiodvalue\")) & (col(\"ld.ftpperiod\") == col(\"mp.Order\"))).select(\n",
    "    col(\"ld.PeriodType\"),\n",
    "    col(\"mp.closing\"),\n",
    "    col(\"ld.LegacyPeriod\"),\n",
    "    col(\"ld.numericvalueofperiod\")\n",
    "    ).distinct()\n",
    "\n",
    "    if PeriodValueInYHQM == 0: \n",
    "        print(\"inside PeriodValueInYHQM == 0 \")\n",
    "        legacysddates_df = legacysddates_src_df.filter(~col(\"periodtype\").like('%YTD'))\n",
    "        legacysddates_period_df = legacysddates_df.withColumn(\"PeriodType\", concat(col(\"PeriodType\"), lit(\"_YTD\")))\n",
    "\n",
    "        tballperiod_df = (\n",
    "        trialbalance_dim_df.alias(\"tb\")\n",
    "        .join(periodicaldateswithlargeperiod_df.alias(\"ld\"), (col(\"tb.Period\").substr(1, 6) == col(\"ld.closing\")) &\n",
    "        (col(\"tb.period\").substr(-2, 2) <= abs(col(\"numericvalueofperiod\"))),\"inner\")\n",
    "        .where(col(\"EntityID_fk\") == EntityID)\n",
    "        .select(\"tb.Period\", col(\"tb.Period\").substr(1, 6).alias(\"stdperiod\"),\"ld.*\",*alldimcols)\n",
    "        .orderBy(\"PeriodType\")\n",
    "        )\n",
    "        tballperiod_df.coalesce(1).write.format(\"parquet\").mode(\"overwrite\").save(adls_path+folder_path+\"tballperiod\")\n",
    "        tballperiod_df = spark.read.parquet(adls_path+folder_path+\"tballperiod\", schema = tballperiod_schema)\n",
    "\n",
    "        \n",
    "        tblegacymatchperiod_df = (trialbalance_dim_df.alias(\"tb\")\n",
    "        .join(periodicaldateswithequalperiod_df.alias(\"ld\"), (col(\"tb.Period\") == concat(col(\"closing\"),lit(\"-\"),col(\"legacyperiod\").substr(-2, 2))),\"inner\")\n",
    "        .where(col(\"tb.EntityID_fk\") == EntityID)\n",
    "        .select(\"Period\", col(\"Period\").substr(1, 6).alias(\"stdperiod\"),\"ld.*\",*alldimcols)\n",
    "        .orderBy(\"PeriodType\")\n",
    "        )\n",
    "\n",
    "        \n",
    "        tblegacymatchperiod_df.coalesce(1).write.format(\"parquet\").mode(\"overwrite\").save(adls_path+folder_path+\"tblegacymatchperiod\")\n",
    "        tblegacymatchperiod_df = spark.read.parquet(adls_path+folder_path+\"tblegacymatchperiod\", schema = tblegacymatchperiod_schema)\n",
    "        \n",
    "        priorityseq1_df = (\n",
    "        tballperiod_df.alias(\"tb\").join(\n",
    "            (\n",
    "                legacysddates_period_df.alias(\"ld\")\n",
    "                .join(\n",
    "                    tblegacymatchperiod_df.alias(\"mp\"),\n",
    "                    (col(\"ld.LegacyPeriod\") == col(\"mp.LegacyPeriod\")) & (col(\"ld.periodtype\") == col(\"mp.periodtype\")),\n",
    "                    \"left\"\n",
    "                )\n",
    "                .where(col(\"mp.LegacyPeriod\").isNull())\n",
    "                .select(\"ld.LegacyPeriod\", \"ld.periodtype\", \"ld.ftpperiod\")\n",
    "                .distinct()\n",
    "            ),\n",
    "            (col(\"tb.Period\").substr(1, 6) <= col(\"ld.LegacyPeriod\").substr(1, 6)) &\n",
    "            (col(\"tb.periodtype\") == col(\"ld.periodtype\")) &\n",
    "            (col(\"tb.Period\").substr(-2, 2) != col(\"ld.LegacyPeriod\").substr(-2, 2)),\n",
    "            \"inner\"\n",
    "        )\n",
    "        .select(\"tb.*\", \"ld.ftpperiod\")\n",
    "        )\n",
    "\n",
    "        \n",
    "        window_spec = Window.partitionBy(\"stdperiod\", \"periodtype\").orderBy(col(\"Period\").desc())\n",
    "\n",
    "        priorityseq_df = priorityseq1_df \\\n",
    "        .withColumn(\"priorityseq\", row_number().over(window_spec)) \\\n",
    "        .withColumn(\"order\", when(col(\"Period\").substr(-2, 2) >= '12', 4)\n",
    "        .when(col(\"Period\").substr(-2, 2) == '06', 3)\n",
    "        .when(col(\"Period\").substr(-2, 2) == '03', 2)\n",
    "        # .when(col(\"Period\").substr(-2, 2) == '01', 1)\n",
    "        .otherwise(1)) \\\n",
    "        .select(\n",
    "            col(\"Period\"),\n",
    "            col(\"stdperiod\"),\n",
    "            col(\"PeriodType\"),\n",
    "            col(\"closing\"),\n",
    "            col(\"legacyperiod\"),\n",
    "            col(\"numericvalueofperiod\"),\n",
    "            col(\"priorityseq\"),\n",
    "            col(\"order\"),\n",
    "            col(\"ftpperiod\")\n",
    "        ) \\\n",
    "        .orderBy(\"periodtype\")\n",
    "            \n",
    "        priorityseq_df = priorityseq_df.filter(col(\"priorityseq\") == 1)\n",
    "\n",
    "        priorityseq_df.coalesce(1).write.format(\"parquet\").mode(\"overwrite\").save(adls_path+folder_path+\"priorityseq\")\n",
    "        priorityseq_df = spark.read.parquet(adls_path+folder_path+\"priorityseq\", schema = priorityseq_schema)\n",
    "\n",
    "        legacysddates_final_df = legacysddates_period_df.alias(\"ld\") \\\n",
    "        .join(priorityseq_df.alias(\"ps\"),(col(\"ld.PeriodType\") == col(\"ps.PeriodType\")) & (col(\"ld.currentvalue\") == col(\"ps.stdperiod\")),\"left_outer\") \\\n",
    "        .select(\"ld.*\",col(\"ps.PeriodType\").alias(\"ps_PeriodType\"),col(\"ps.stdperiod\").alias(\"ps_stdperiod\"),col(\"ps.Period\")) \\\n",
    "        .withColumn(\"LegacyPeriod\",when((col(\"ld.periodtype\") == col(\"ps_PeriodType\")) & (col(\"ld.currentvalue\") == col(\"ps_stdperiod\")),col(\"ps.Period\"))\n",
    "                .otherwise(col(\"ld.LegacyPeriod\"))) \\\n",
    "    .drop(\"ps_PeriodType\", \"ps_stdperiod\", col(\"ps.Period\"))\n",
    "\n",
    "        legacysddates_final_df.coalesce(1).write.format(\"parquet\").mode(\"overwrite\").save(adls_path+folder_path+\"legacysddates\")  \n",
    "\n",
    "        ytddates_df = (\n",
    "                tblegacymatchperiod_df\n",
    "                .filter((col(\"PeriodType\").like(\"%YTD\")) | (col(\"PeriodType\").like(\"%Year\")))\n",
    "                .select(\"PeriodType\", col(\"legacyperiod\").substr(1, 6).alias(\"CurrentValue\"), col(\"Period\").alias(\"FTPValue\"))\n",
    "                .union(\n",
    "                    priorityseq_df\n",
    "                    .filter((col(\"PeriodType\").like(\"%YTD\")) | (col(\"PeriodType\").like(\"%Year\")))\n",
    "                    .select(\"PeriodType\", col(\"legacyperiod\").substr(1, 6).alias(\"CurrentValue\"), col(\"Period\").alias(\"FTPValue\"))\n",
    "                )\n",
    "            )\n",
    "\n",
    "        ytddates_df.coalesce(1).write.format(\"parquet\").mode(\"overwrite\").save(adls_path+folder_path+\"ytddates\") \n",
    "    elif PeriodValueInYHQM == 1:   \n",
    "        print(\"inside PeriodValueInYHQM == 1\")\n",
    "        tballperiod_df = (\n",
    "        trialbalance_dim_df.alias(\"tb\")\n",
    "        .join(\n",
    "            periodicaldateswithlargeperiod_df.alias(\"ld\"),\n",
    "            (\n",
    "                (col(\"tb.Period\").substr(1, 6) == col(\"ld.closing\")) &\n",
    "                (col(\"tb.Period\").substr(-2, 2) <= abs(col(\"ld.numericvalueofperiod\")))\n",
    "            ),\n",
    "            \"inner\"\n",
    "        )\n",
    "        .where(col(\"EntityID_fk\") == EntityID)\n",
    "        .select(\"Period\", col(\"Period\").substr(1, 6).alias(\"stdperiod\"),\"ld.*\",*alldimcols)\n",
    "        .orderBy(\"PeriodType\")\n",
    "        ).distinct()\n",
    "        tballperiod_df.coalesce(1).write.format(\"parquet\").mode(\"overwrite\").save(adls_path+folder_path+\"tballperiod\")\n",
    "\n",
    "        tballperiod_df = spark.read.parquet(adls_path+folder_path+\"tballperiod\", schema = tballperiod_schema)\n",
    "\n",
    "        tblegacymatchperiod_df = (trialbalance_dim_df.alias(\"tb\").join(\n",
    "        periodicaldateswithequalperiod_df.alias(\"ld\"),\n",
    "        (\n",
    "            col(\"tb.Period\") == concat(col(\"closing\"),lit(\"-\"),col(\"legacyperiod\").substr(-2, 2))\n",
    "        ),\"inner\")\n",
    "        .where(col(\"EntityID_fk\") == EntityID)\n",
    "        .select(\"Period\", col(\"Period\").substr(1, 6).alias(\"stdperiod\"),\"ld.*\",*alldimcols)\n",
    "        .orderBy(\"PeriodType\")\n",
    "        ).distinct()\n",
    "\n",
    "        tblegacymatchperiod_df = tblegacymatchperiod_df.filter(~(\n",
    "            (F.expr(\"substr(Period, 1, 6)\") == F.expr(\"substr(LegacyPeriod, 1, 6)\")) &\n",
    "            (F.expr(\"cast(substr(Period, -2) as int)\") != F.abs(col(\"NumericValueofPeriod\")))\n",
    "        ))\n",
    "\n",
    "        tblegacymatchperiod_df = tblegacymatchperiod_df.filter(~(\n",
    "            (F.expr(\"substr(Period, 1, 6)\") == F.expr(\"substr(LegacyPeriod, 1, 6)\")) &\n",
    "            (F.expr(\"cast(substr(Period, -2) as int)\") < F.abs(col(\"NumericValueofPeriod\")))\n",
    "        ))\n",
    "\n",
    "        tblegacymatchperiod_df1 = tblegacymatchperiod_df\n",
    "\n",
    "        tblegacymatchperiod_df1 = tblegacymatchperiod_df1.filter(~(\n",
    "            (F.expr(\"substr(Period, 1, 6)\") != F.expr(\"substr(LegacyPeriod, 1, 6)\")) &\n",
    "            (F.expr(\"cast(substr(Period, -2) as int)\") == F.abs(col(\"NumericValueofPeriod\")))\n",
    "        ))\n",
    "            \n",
    "        legacysddates_tmp_df = legacysddates_src_df.alias(\"ls\").join(tblegacymatchperiod_df1.alias(\"mp\"),((col(\"ls.LegacyPeriod\") == col(\"mp.LegacyPeriod\")) & (col(\"ls.periodtype\") == col(\"mp.periodtype\"))),\"left\") \\\n",
    "        .filter(col(\"mp.LegacyPeriod\").isNull()) \\\n",
    "                    .select(\"ls.LegacyPeriod\",\"ls.periodtype\",\"ls.ftpperiod\").withColumn(\"fromperiod\",concat(F.year(F.add_months(F.to_date(F.concat(F.substring(col('LegacyPeriod'), 1, 4),F.lit('-'),F.substring(col('ls.LegacyPeriod'), 5, 2),F.lit('-01')),'yyyy-MM-dd'),-F.expr(\"CAST(RIGHT(LegacyPeriod, 2) AS INT)\"))), \n",
    "            F.substring(F.date_format(F.add_months(F.to_date(F.concat(F.substring(col('LegacyPeriod'), 1, 4),F.lit('-'),F.substring(col('LegacyPeriod'), 5, 2),\n",
    "                            F.lit('-01')),'yyyy-MM-dd'),-F.expr(\"CAST(RIGHT(LegacyPeriod, 2) AS INT)\")),'yyyy-MM-dd' ),6,2)))\n",
    "\n",
    "\n",
    "        priorityseq_df1 = tballperiod_df.alias(\"tb\").join(legacysddates_tmp_df.alias(\"ld\"), ((col(\"tb.Period\").substr(1, 6) <= col(\"ld.LegacyPeriod\").substr(1, 6)) &\n",
    "                        (col(\"tb.Period\").substr(1, 6) > col(\"fromperiod\")) & (col(\"tb.PeriodType\") == col(\"ld.periodtype\"))),\"inner\").filter((col(\"tb.Period\").substr(-2, 2) != col(\"ld.LegacyPeriod\").substr(-2, 2))).select(\"tb.*\",\"ld.ftpperiod\")\n",
    "\n",
    "        window_spec = Window().partitionBy(\"stdperiod\", \"periodtype\").orderBy(col(\"Period\").desc())\n",
    "\n",
    "        priorityseq_df = priorityseq_df1 \\\n",
    "                .withColumn(\"priorityseq\", row_number().over(window_spec)) \\\n",
    "                .withColumn(\"order\", when(col(\"Period\").substr(-2, 2) >= '12', 4)\n",
    "                .when(col(\"Period\").substr(-2, 2) >= '12', 5)\n",
    "                .when(col(\"Period\").substr(-2, 2) >= '09', 4)\n",
    "                .when(col(\"Period\").substr(-2, 2) >= '06', 3)\n",
    "                .when(col(\"Period\").substr(-2, 2) >= '03', 2)\n",
    "                .when(col(\"Period\").substr(-2, 2) >= '01', 1)\n",
    "                .otherwise(1)) \\\n",
    "                .select(\n",
    "                    col(\"Period\"),\n",
    "                    col(\"stdperiod\"),\n",
    "                    col(\"PeriodType\"),\n",
    "                    col(\"closing\"),\n",
    "                    col(\"legacyperiod\"),\n",
    "                    col(\"numericvalueofperiod\"),\n",
    "                    col(\"priorityseq\"),\n",
    "                    col(\"order\"),\n",
    "                    col(\"ftpperiod\")\n",
    "                ) \\\n",
    "                .orderBy(\"periodtype\")\n",
    "\n",
    "        # #################PRIORITY : fetch data for Q3 & Q4 (lower period)in case data for H1 Present but H2 not present #####################\n",
    "\n",
    "        prioritylevel2temp_1_df = tblegacymatchperiod_df.alias(\"tbm\").join(legacysddates_src_df.alias(\"ld\"),(col(\"ld.periodtype\") == col(\"tbm.periodtype\")) & \n",
    "                      (col(\"ld.legacyperiod\") == col(\"tbm.legacyperiod\")), \"inner\") \\\n",
    "            .groupBy(\"tbm.periodtype\", \"ld.FinancialYear\", \"ld.LegacyPeriod\", \"ftpperiod\") \\\n",
    "            .agg(\n",
    "            max(col(\"tbm.Period\")).alias(\"Period\"))\n",
    "        prioritylevel2temp_2_df = prioritylevel2temp_1_df.withColumn(\"ftpperiod\",\n",
    "            when(col(\"ftpperiod\") == 1, col(\"ftpperiod\"))\n",
    "            .otherwise(col(\"ftpperiod\") - 1)) \\\n",
    "            .withColumn(\"fromperiod\",\n",
    "            concat(F.year(F.add_months(F.to_date(F.concat(F.substring(col('ld.LegacyPeriod'), 1, 4),F.lit('-'),F.substring(col('ld.LegacyPeriod'), 5, 2),F.lit('-01')),'yyyy-MM-dd'),-F.expr(\"CAST(RIGHT(ld.LegacyPeriod, 2) AS INT)\"))), \n",
    "            F.substring(F.date_format(F.add_months(F.to_date(F.concat(F.substring(col('ld.LegacyPeriod'), 1, 4),F.lit('-'),F.substring(col('ld.LegacyPeriod'), 5, 2),\n",
    "            F.lit('-01')),'yyyy-MM-dd'),-F.expr(\"CAST(RIGHT(ld.LegacyPeriod, 2) AS INT)\")),'yyyy-MM-dd' ),6,2)).alias(\"fromperiod\"))\n",
    "\n",
    "        prioritylevel2temp1_df = periodicaldates_src_df.alias(\"pd\").join(prioritylevel2temp_2_df.alias(\"ld\"),(col(\"pd.closing\") > col(\"ld.fromperiod\")) &\n",
    "        (col(\"pd.closing\") <= col(\"ld.LegacyPeriod\").substr(1, 6)) & \n",
    "        (col(\"pd.Order\") == col(\"ld.ftpperiod\")) &\n",
    "        (col(\"pd.Year\") == col(\"ld.financialyear\")) &\n",
    "        (col(\"FinancialCycleID\") == FiscalPeriodID) &\n",
    "        (col(\"ld.legacyperiod\") != col(\"ld.Period\"))\n",
    "        ,\"inner\")\n",
    "        prioritylevel2temp_df = prioritylevel2temp1_df.withColumn(\n",
    "        \"closing\",\n",
    "        col(\"closing\") +\n",
    "        when(col(\"ftpperiod\") == 5, \"-12\")\n",
    "        .when(col(\"ftpperiod\") == 4, \"-09\")\n",
    "        .when(col(\"ftpperiod\") == 3, \"-06\")\n",
    "        .when(col(\"ftpperiod\") == 2, \"-03\")\n",
    "        .when(col(\"ftpperiod\") == 1, \"-01\")\n",
    "        .otherwise(\"\")) \\\n",
    "        .select(\"periodtype\",\"ld.Period\",\"ld.legacyperiod\",\"ftpperiod\",\"closing\")\n",
    "     \n",
    "        prioritylevel2temp_df = trialbalance_dim_df.alias(\"tb\").join(prioritylevel2temp_df.alias(\"pt\"),col(\"tb.Period\") == col(\"pt.closing\"),\"inner\").where( col(\"EntityID_fk\") == EntityID).select(\"tb.Period\",\n",
    "\t\t\t\t\t\t\tcol(\"tb.Period\").substr(1,6).alias(\"stdperiod\"),\n",
    "\t\t\t\t\t\t\t\"periodtype\",\n",
    "\t\t\t\t\t\t\tcol(\"tb.Period\").substr(1,6).alias(\"closing\"),\n",
    "\t\t\t\t\t\t\t\"legacyperiod\",\n",
    "\t\t\t\t\t\t\t\"ftpperiod\",*alldimcols)\n",
    "\n",
    "        window_spec = Window.partitionBy(\"stdperiod\", \"periodtype\").orderBy(col(\"Period\").desc())\n",
    "\n",
    "        prioritylevel2_df = prioritylevel2temp_df \\\n",
    "                .withColumn(\"priorityseq\", row_number().over(window_spec)) \\\n",
    "                .withColumn(\"order\", when(col(\"Period\").substr(-2, 2) >= '12', 4)\n",
    "                .when(col(\"Period\").substr(-2, 2) >= '12', 5)\n",
    "                .when(col(\"Period\").substr(-2, 2) >= '09', 4)\n",
    "                .when(col(\"Period\").substr(-2, 2) >= '06', 3)\n",
    "                .when(col(\"Period\").substr(-2, 2) >= '03', 2)\n",
    "                .when(col(\"Period\").substr(-2, 2) >= '01', 1)\n",
    "                .otherwise(1)) \\\n",
    "                .select(\"*\") \\\n",
    "                .orderBy(\"periodtype\")\n",
    "\n",
    "        priorityseq_upt_df = priorityseq_df.filter(col(\"priorityseq\") == 1)\n",
    "        prioritylevel2_df = prioritylevel2_df.select(\"Period\",\"stdperiod\",\"PeriodType\",\"closing\",\"legacyperiod\",col(\"ftpperiod\").alias(\"numericvalueofperiod\"),\"priorityseq\",\"Order\",*alldimcols).filter(col(\"priorityseq\") == 1)\n",
    "\n",
    "        prioritylevel2_df.coalesce(1).write.format(\"parquet\").mode(\"overwrite\").save(adls_path+folder_path+\"prioritylevel2\")\n",
    "        prioritylevel2_df = spark.read.parquet(adls_path+folder_path+\"prioritylevel2\", schema = prioritylevel2_schema)\n",
    "\n",
    "        # ######################### For Next level priority check ##############################\n",
    "\n",
    "        order_df = (\n",
    "        priorityseq_upt_df\n",
    "        .groupBy(\"PeriodType\")\n",
    "        .agg({\"Order\": \"max\"})\n",
    "        .withColumnRenamed(\"max(Order)\", \"order\")\n",
    "        .exceptAll(\n",
    "        priorityseq_upt_df.alias(\"ps\")\n",
    "        .join(\n",
    "            legacysddates_src_df.alias(\"ld\"),\n",
    "            (col(\"ps.PeriodType\") == col(\"ld.PeriodType\")) &\n",
    "            (col(\"ps.LegacyPeriod\") == col(\"ld.LegacyPeriod\")) &\n",
    "            (col(\"ps.Order\") == col(\"ld.ftpperiod\") - 1),\n",
    "            \"inner\"\n",
    "        )\n",
    "        .select(\"ps.PeriodType\", \"ps.Order\")\n",
    "        )\n",
    "        )\n",
    "\n",
    "        if not order_df.isEmpty():\n",
    "            print(\"if order not empty\")\n",
    "            print(\"if count is greater than 0\")\n",
    "            ctePS_df = priorityseq_upt_df.groupBy(\"PeriodType\",\"LegacyPeriod\").agg(max(col(\"Order\")).alias(\"Order\")).select(\"PeriodType\",\"LegacyPeriod\",\"Order\")\n",
    "\n",
    "            legacysddates_df = legacysddates_src_df.alias(\"ld\").join(ctePS_df.alias(\"ps\"),\n",
    "            (col(\"ps.periodtype\") == col(\"ld.PeriodType\")) &\n",
    "            (col(\"ps.legacyperiod\") == col(\"ld.legacyperiod\")),\"left_outer\").select(\"ld.*\",col(\"ps.Order\").alias(\"ps_Order\"),col(\"ps.periodtype\").alias(\"ps_periodtype\"),col(\"ps.legacyperiod\").alias(\"ps_legacyperiod\")) \\\n",
    "            .withColumn(\"ftpperiod\",when((col(\"periodtype\") == col(\"ps_periodtype\")) & (col(\"legacyperiod\") == col(\"ps_legacyperiod\")),col(\"ps_Order\") + 1).otherwise(col(\"ftpperiod\"))).drop(\"ps_Order\",\"ps_periodtype\",\"ps_legacyperiod\")\n",
    "            legacysddates_df.coalesce(1).write.format(\"parquet\").mode(\"overwrite\").save(adls_path+folder_path+\"legacysddates\") \n",
    "\n",
    "        legacysddates_df = spark.read.parquet(adls_path+folder_path+\"legacysddates\", schema = legacysddates_schema)\n",
    "        tblegacymatchperiod_df = (\n",
    "        tblegacymatchperiod_df.alias(\"tbm\")\n",
    "        .join(\n",
    "            prioritylevel2_df.alias(\"pl\"),\n",
    "            (col(\"tbm.PeriodType\") == col(\"pl.PeriodType\")) &\n",
    "            (col(\"tbm.LegacyPeriod\") == col(\"pl.LegacyPeriod\")),\n",
    "            \"left_anti\"\n",
    "        )\n",
    "        )\n",
    "\n",
    "        priorityseq_upt1_df = (\n",
    "        priorityseq_upt_df.alias(\"ps\")\n",
    "        .join(\n",
    "            legacysddates_df.alias(\"ld\"),\n",
    "            (col(\"ps.PeriodType\") == col(\"ld.PeriodType\")) &\n",
    "            (col(\"ps.LegacyPeriod\") == col(\"ld.LegacyPeriod\")) &\n",
    "            (col(\"ps.Order\") != col(\"ld.ftpperiod\") - 1),\n",
    "            \"left_anti\"\n",
    "        )\n",
    "        )   \n",
    "        \n",
    "        priorityseq_upt1_df.coalesce(1).write.format(\"parquet\").mode(\"overwrite\").save(adls_path+folder_path+\"priorityseq\")\n",
    "        priorityseq_upt1_df = spark.read.parquet(adls_path+folder_path+\"priorityseq\", schema = priorityseq_schema)\n",
    "        \n",
    "        bspriority_df = (\n",
    "        legacysddates_df.alias(\"ldd\")\n",
    "        .join(priorityseq_upt1_df.alias(\"pseq\"), (col(\"ldd.PeriodType\") == col(\"pseq.PeriodType\")) & (col(\"ldd.LegacyPeriod\") == col(\"pseq.LegacyPeriod\")), \"left\")\n",
    "        .join(prioritylevel2_df.alias(\"pl2\"), (col(\"ldd.PeriodType\") == col(\"pl2.PeriodType\")) & (col(\"ldd.LegacyPeriod\") == col(\"pl2.LegacyPeriod\")), \"left\")\n",
    "        .join(tblegacymatchperiod_df.alias(\"lm\"), (col(\"ldd.PeriodType\") == col(\"lm.PeriodType\")) & (col(\"ldd.LegacyPeriod\") == col(\"lm.LegacyPeriod\")), \"left\")\n",
    "        .groupBy(\"ldd.PeriodType\", \"ldd.LegacyPeriod\", \"ldd.CurrentValue\")\n",
    "        .agg(\n",
    "            max(coalesce(\"pseq.Period\", \"pl2.Period\", \"lm.Period\")).alias(\"Period\"),\n",
    "            max(coalesce(\"pseq.StdPeriod\", \"pl2.StdPeriod\", \"lm.StdPeriod\")).alias(\"StdPeriod\")\n",
    "        )\n",
    "        .filter(col(\"Period\").isNotNull())\n",
    "        .groupBy(\"PeriodType\", \"LegacyPeriod\", \"CurrentValue\")\n",
    "        .agg(\n",
    "            max(\"Period\").alias(\"Period\"),\n",
    "            max(\"StdPeriod\").alias(\"StdPeriod\")\n",
    "        )\n",
    "        )\n",
    "\n",
    "        legacysddates_final_df = legacysddates_df.alias(\"ld\") \\\n",
    "        .join(\n",
    "            bspriority_df.alias(\"ps\"),\n",
    "            (col(\"ld.PeriodType\") == col(\"ps.PeriodType\")) &\n",
    "            (col(\"ld.LegacyPeriod\") == col(\"ps.LegacyPeriod\")),\n",
    "            \"left_outer\").select(\"ld.*\",col(\"ps.Period\").alias(\"ps_Period\"),col(\"ps.stdperiod\").alias(\"ps_stdperiod\")).withColumn(\"legacyperiod\",F.coalesce(\"ps_Period\",\"ld.legacyperiod\")) \\\n",
    "        .withColumn(\"currentvalue\",F.coalesce(\"ps_stdperiod\",\"ld.currentvalue\")).drop(\"ps_Period\",\"ps_stdperiod\")\n",
    "        \n",
    "        legacysddates_final_df.coalesce(1).write.format(\"parquet\").mode(\"overwrite\").save(adls_path+folder_path+\"legacysddates\") \n",
    "        legacysddates_final_df = spark.read.parquet(adls_path+folder_path+\"legacysddates\", schema = legacysddates_schema)\n",
    "        \n",
    "        tblegacymatchperiod_df2 = tblegacymatchperiod_df.filter((col(\"Period\").substr(1,6) != col(\"LegacyPeriod\").substr(1,6)) & (col(\"Period\").substr(-2, 2).cast(\"int\") == abs(col(\"numericvalueofperiod\")))).alias(\"a\").join(tblegacymatchperiod_df.alias(\"b\"),col(\"a.LegacyPeriod\") == col(\"b.Period\"),\"left_anti\").select(\"a.*\")\n",
    "\n",
    "        tblegacymatchperiod_updated1_df = tblegacymatchperiod_df.alias(\"tb\").join(tblegacymatchperiod_df2.alias(\"del\"),col(\"tb.LegacyPeriod\")==col(\"del.LegacyPeriod\"),\"inner\").select(\"tb.*\")\n",
    "\n",
    "        tblegacymatchperiod_df = tblegacymatchperiod_df.exceptAll(tblegacymatchperiod_updated1_df)\n",
    "        \n",
    "        tblegacymatchperiod_df.coalesce(1).write.format(\"parquet\").mode(\"overwrite\").save(adls_path+folder_path+\"tblegacymatchperiod\")\n",
    "        tblegacymatchperiod_df = spark.read.parquet(adls_path+folder_path+\"tblegacymatchperiod\", schema = tblegacymatchperiod_schema)\n",
    "        \n",
    "\n",
    "        if countofcomp == 0 or countofcomp ==1:\n",
    "            print(\"inside PeriodValueInYHQM == 1 and countofcomp == 0 or countofcomp ==1\")\n",
    "            ytddates_df = tblegacymatchperiod_df.filter((col(\"Periodtype\").like('%YTD')) | (col(\"Periodtype\").like('%Year'))).select(\"PeriodTYpe\",col(\"legacyperiod\").substr(1,6).alias(\"CurrentValue\"),col(\"Period\").alias(\"FTPValue\")) \\\n",
    "            .union(priorityseq_upt1_df.filter((col(\"Periodtype\").like('%YTD')) | (col(\"Periodtype\").like('%Year'))).select(\"PeriodTYpe\",col(\"legacyperiod\").substr(1,6).alias(\"CurrentValue\"),col(\"Period\").alias(\"FTPValue\"))) \\\n",
    "            .union(prioritylevel2_df.filter((col(\"Periodtype\").like('%YTD')) | (col(\"Periodtype\").like('%Year'))).select(\"PeriodTYpe\",col(\"legacyperiod\").substr(1,6).alias(\"CurrentValue\"),col(\"Period\").alias(\"FTPValue\"))).distinct()\n",
    "\n",
    "            ytddates_df.coalesce(1).write.format(\"parquet\").mode(\"overwrite\").save(adls_path+folder_path+\"ytddates\") \n",
    "           \n",
    "        if countofcomp > 1:\n",
    "            print(\"inside PeriodValueInYHQM == 1 and countofcomp > 1\")\n",
    "            \n",
    "            ytddates_df = (\n",
    "            legacysddates_final_df\n",
    "            .filter(~(col(\"periodtype\").isin(\"currentPeriod\", \"PreviousPeriod\"))).select(\"periodtype\",\"pcurrentvalue\",\"legacyperiod\")\n",
    "            .exceptAll(\n",
    "            legacysddates_final_df\n",
    "            .filter(col(\"legacyperiod\").substr(1, 6) != col(\"pcurrentvalue\")).alias(\"a\")\n",
    "            .join(\n",
    "                tblegacymatchperiod_df.alias(\"b\"),\n",
    "                (col(\"a.legacyperiod\") == col(\"b.Period\")) &\n",
    "                (col(\"a.periodtype\") == col(\"b.PeriodType\")),\n",
    "                \"left_anti\"\n",
    "            )\n",
    "            .select(\"a.periodType\", col(\"a.pcurrentvalue\"), col(\"a.legacyperiod\"))\n",
    "            )\n",
    "            .union(\n",
    "                priorityseq_upt1_df\n",
    "                .select(\"PeriodType\", col(\"legacyperiod\").substr(1, 6).alias(\"pcurrentvalue\"), col(\"Period\").alias(\"legacyperiod\"))\n",
    "            )\n",
    "            .union(\n",
    "                prioritylevel2_df\n",
    "                .select(\"PeriodType\", col(\"legacyperiod\").substr(1, 6).alias(\"pcurrentvalue\"), col(\"Period\").alias(\"legacyperiod\"))\n",
    "            ).distinct()\n",
    "            )\n",
    "                        \n",
    "            ytddates_df = ytddates_df.withColumnRenamed(\"pcurrentvalue\",\"CurrentValue\").withColumnRenamed(\"legacyperiod\",\"FTPValue\")\n",
    "            ytddates_df.coalesce(1).write.format(\"parquet\").mode(\"overwrite\").save(adls_path+folder_path+\"ytddates\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "662d198c-ea49-4208-b108-70d674c84321",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "legacysddates_df = spark.read.parquet(adls_path+folder_path+\"legacysddates\", schema = legacysddates_schema)\n",
    "prioritylevel2_df = spark.read.parquet(adls_path+folder_path+\"prioritylevel2\", schema = prioritylevel2_schema)\n",
    "ytddates_df = spark.read.parquet(adls_path+folder_path+\"ytddates\", schema = ytddates_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c35ba869-afc3-4256-b4e6-5c40597ac099",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "############################ extended financial cycle changes #############################\n",
    "\n",
    "if extflag == 1 and extendedmonthdiff <= 12:\n",
    "    print(\"extflag == 1 and extendedmonthdiff <= 12\")\n",
    "    print(\"extended financial cycle changes\")\n",
    "    print(\"less than 12 month cycle\")\n",
    "    legacysddates_updated_df1 = legacysddates_df.alias(\"ld\").join(periodicaldatesforextensionperiod_src_df.alias(\"pd\"),\n",
    "            (col(\"pd.closing\") == col(\"ld.pcurrentvalue\")) &\n",
    "            (col(\"pd.Value\") == col(\"ld.Value\")),\"left_outer\").filter((col(\"ld.financialyear\") == extendedcycleyear) &\n",
    "        (col(\"pd.entityid\") == EntityID)) \\\n",
    "            .select(\"ld.*\")\n",
    "    \n",
    "    legacysddates_updated_df2 = legacysddates_df.exceptAll(legacysddates_updated_df1)\n",
    "    \n",
    "    legacysddates_updated_df3 = legacysddates_updated_df1.alias(\"ld\").join(\n",
    "        periodicaldatesforextensionperiod_src_df.filter(col(\"entityid\") == EntityID).alias(\"pd\"),\n",
    "        (col(\"pd.closing\") == col(\"ld.pcurrentvalue\")) & (col(\"pd.Value\") == col(\"ld.Value\"))\n",
    "    ).select(\n",
    "        (\"ld.*\")\n",
    "    ).withColumn(\n",
    "        \"p\", lit(None)\n",
    "    ).withColumn(\n",
    "        \"ppp\", lit(None)\n",
    "    ).withColumn(\n",
    "        \"py\", lit(None)\n",
    "    ).withColumn(\n",
    "        \"pypp\", lit(None)\n",
    "    ).withColumn(\n",
    "        \"ppy\", lit(None)\n",
    "    ).withColumn(\n",
    "        \"p3y\", lit(None)\n",
    "    ).withColumn(\n",
    "        \"p4y\", lit(None)\n",
    "    )\n",
    "\n",
    "    legacysddates_updated_df = legacysddates_updated_df2.union(legacysddates_updated_df3)\n",
    "    \n",
    "    min_currentvalue = legacysddates_updated_df.filter(col(\"p\").isNull()).selectExpr(\"MIN(currentvalue)\").collect()[0][0]\n",
    "\n",
    "    legacysddates_curr_df = legacysddates_updated_df.filter(col(\"currentvalue\") < min_currentvalue)\n",
    "    legacysddates_updt_df = legacysddates_updated_df.exceptAll(legacysddates_curr_df)\n",
    "    legacysddates_updt_df.coalesce(1).write.format(\"parquet\").mode(\"overwrite\").save(adls_path+folder_path+\"legacysddates\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0c777541-80c4-46c0-b47f-9828811b216a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_extended_period_legacydates(legacysddates_df,periodicaldatesforextensionperiod_src_df,extvalue,FiscalPeriodID,FinancialYear,EntityID,extopen,extclose,extendedcycleyear):\n",
    "    print(\"inside get_extended_period_legacydates function\")\n",
    "    legacysddates_ext1_df =legacysddates_df.alias(\"ld\").join(\n",
    "        periodicaldatesforextensionperiod_src_df.alias(\"pd\"),(col(\"pd.closing\") == col(\"ld.pcurrentvalue\")) & (col(\"pd.Value\") == col(\"ld.Value\")),how=\"inner\").filter((col(\"entityid\") == EntityID) & (col(\"financialcycleid\") == FiscalPeriodID) & (col(\"financialyear\") == extendedcycleyear)).select(\"ld.*\")\n",
    "    \n",
    "    legacysddates_ext2_df = legacysddates_df.exceptAll(legacysddates_ext1_df)\n",
    "    \n",
    "    legacysddates_ext3_df = legacysddates_ext1_df.alias(\"ld\").join(\n",
    "        periodicaldatesforextensionperiod_src_df.filter((col(\"EntityID\") == EntityID)).alias(\"pd\"),\n",
    "        (col(\"pd.closing\") == col(\"ld.pcurrentvalue\")) &\n",
    "        (col(\"pd.Value\") == col(\"ld.Value\")),how=\"inner\").select(\"ld.*\",col(\"pd.Value\").alias(\"pd_Value\"),col(\"pd.Year\").alias(\"pd_Year\"),col(\"pd.opening\").alias(\"pd_opening\")) \\\n",
    "            .withColumn(\"Value\",col(\"pd_Value\"))\\\n",
    "                .withColumn(\"financialyear\",col(\"pd_Year\")) \\\n",
    "                    .withColumn(\"openingperiodvalue\",col(\"pd_opening\")) \\\n",
    "                        .withColumn(\"CurrentValue\",col(\"ld.pcurrentvalue\")).drop(\"pd_Value\",\"pd_Year\",\"pd_opening\")\n",
    "   \n",
    "    legacysddates_ext_df =  legacysddates_ext2_df.union(legacysddates_ext3_df)\n",
    "\n",
    "    legacysddates_updated_df1 = legacysddates_ext_df.alias(\"ld\").join(periodicaldatesforextensionperiod_src_df.alias(\"pd\"),\n",
    "            (col(\"pd.closing\") == col(\"ld.pcurrentvalue\")) &\n",
    "            (col(\"pd.Value\") == col(\"ld.Value\")),\"left_outer\").filter((col(\"ld.financialyear\") == extendedcycleyear) &\n",
    "        (col(\"pd.entityid\") == EntityID)) \\\n",
    "            .select(\"ld.*\")\n",
    "\n",
    "    legacysddates_updated_df2 = legacysddates_ext_df.exceptAll(legacysddates_updated_df1)\n",
    "\n",
    "    legacysddates_updated_df3 = legacysddates_updated_df1.alias(\"ld\").join(\n",
    "        periodicaldatesforextensionperiod_src_df.filter(col(\"entityid\") == EntityID).alias(\"pd\"),\n",
    "        (col(\"pd.closing\") == col(\"ld.pcurrentvalue\")) & (col(\"pd.Value\") == col(\"ld.Value\"))\n",
    "    ).select(\n",
    "        (\"ld.*\"),\n",
    "        col(\"pd.Value\").alias(\"pd_Value\"),\n",
    "        col(\"pd.closing\").alias(\"pd_closing\"),\n",
    "        col(\"pd.opening\").alias(\"pd_opening\")\n",
    "    ).withColumn(\n",
    "        \"openingperiodvalue\",\n",
    "        when(\n",
    "            (col(\"pd_closing\") == col(\"ld.pcurrentvalue\")) & (col(\"pd_Value\") == col(\"ld.Value\")),\n",
    "            lit(extopen)\n",
    "        ).otherwise(col(\"ld.openingperiodvalue\"))\n",
    "    ).withColumn(\n",
    "        \"CastingPeriod\",\n",
    "        when(\n",
    "            (col(\"pd_closing\") == col(\"ld.pcurrentvalue\")) & (col(\"pd_Value\") == col(\"ld.Value\")),\n",
    "            concat(col(\"Value\"), lit('-'), lit(extopen), lit('-'), col(\"currentvalue\"))\n",
    "        ).otherwise(col(\"ld.CastingPeriod\"))\n",
    "    ).withColumn(\n",
    "        \"p\", lit('')\n",
    "    ).withColumn(\n",
    "        \"ppp\", lit('')\n",
    "    ).withColumn(\n",
    "        \"py\", lit('')\n",
    "    ).withColumn(\n",
    "        \"pypp\", lit('')\n",
    "    ).withColumn(\n",
    "        \"ppy\", lit('')\n",
    "    ).withColumn(\n",
    "        \"p3y\", lit('')\n",
    "    ).withColumn(\n",
    "        \"p4y\", lit('')\n",
    "    ).withColumn(\n",
    "        \"legacyperiod\",\n",
    "        when(\n",
    "            col(\"ld.Value\").isin(\"H1\", \"Q1\", \"Year\", \"9M\"),\n",
    "            concat(\n",
    "                col(\"pcurrentvalue\"), \n",
    "                lit(\"-\"),\n",
    "                when(\n",
    "                    length(months_between(to_date(concat(lit(\"01-\"),col(\"pd_closing\").substr(-2, 2),lit(\"-\"),col(\"pd_closing\").substr(1, 4)),\"dd-MM-yyyy\"\n",
    "            ),to_date(concat(lit(\"01-\"),col(\"pd_opening\").substr(-2, 2),lit(\"-\"),col(\"pd_opening\").substr(1, 4)),\"dd-MM-yyyy\"))+1\n",
    "                ).cast(\"int\") == 1,\n",
    "                concat(lit('0'), months_between(\n",
    "                    to_date(concat(lit(\"01-\"), col(\"pd_closing\").substr(-2, 2), lit(\"-\"), col(\"pd_closing\").substr(1, 4)), \n",
    "                    \"dd-MM-yyyy\"), \n",
    "                    to_date(concat(lit(\"01-\"), col(\"pd_opening\").substr(-2, 2), lit(\"-\"), col(\"pd_opening\").substr(1, 4)), \n",
    "                    \"dd-MM-yyyy\")).cast(\"int\") + 1\n",
    "                )).otherwise(months_between(\n",
    "                    to_date(concat(lit(\"01-\"), col(\"pd_closing\").substr(-2, 2), lit(\"-\"), col(\"pd_closing\").substr(1, 4)), \n",
    "                    \"dd-MM-yyyy\"), \n",
    "                    to_date(concat(lit(\"01-\"), col(\"pd_opening\").substr(-2, 2), lit(\"-\"), col(\"pd_opening\").substr(1, 4)), \n",
    "                    \"dd-MM-yyyy\")).cast(\"int\") + 1\n",
    "                )\n",
    "            )\n",
    "        ).when(\n",
    "            col(\"Value\").isin(\"H2\"),\n",
    "            concat(col(\"currentvalue\"), lit(\"-\"), lit(\"06\"))\n",
    "        ).when(\n",
    "            col(\"Value\").isin('Q2', 'Q3', 'Q4'),\n",
    "            concat(col(\"currentvalue\"), lit(\"-\"), lit(\"03\"))\n",
    "        ).otherwise(None)\n",
    "    ).drop(\"pd_value\", \"pd_closing\", \"pd_opening\")\n",
    "\n",
    "    legacysddates_updated_df = legacysddates_updated_df2.union(legacysddates_updated_df3)\n",
    "\n",
    "    min_currentvalue = legacysddates_updated_df.filter(col(\"p\") == lit('')).selectExpr(\"MIN(currentvalue)\").collect()[0][0]\n",
    "\n",
    "    # Filter the DataFrame based on the condition\n",
    "    legacysddates_curr_df = legacysddates_updated_df.filter(col(\"currentvalue\") < min_currentvalue)\n",
    "\n",
    "    legacysddates_updt1_df = legacysddates_updated_df.exceptAll(legacysddates_curr_df)\n",
    "\n",
    "    legacysddates_num_df = legacysddates_updt1_df.withColumn(\"numericvalueofperiod\",-expr(\"CAST(SUBSTR(LegacyPeriod, -2) AS INT)\"))\n",
    "\n",
    "    legacysddates_num_df.coalesce(1).write.format(\"parquet\").mode(\"overwrite\").save(adls_path+folder_path+\"legacysddates_num\") \n",
    "    print(\"legacyddates write successful\")\n",
    "    return legacysddates_num_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c7d1af21-f892-4b7d-a0fb-38c83e392640",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if extflag == 1 and extendedmonthdiff > 12:\n",
    "    print(\"extflag == 1 and extendedmonthdiff > 12\")\n",
    "    print(\"more than 12 month cycle\")\n",
    "    extvalue = spark.createDataFrame([(PeriodValue,)], [\"PeriodValue\"]) \\\n",
    "    .withColumn(\"ExtValue\", expr(\"substring(PeriodValue, 1, locate('-', PeriodValue)-1)\")) \\\n",
    "    .select(\"ExtValue\") \\\n",
    "    .collect()[0][0]\n",
    "    print(extvalue)\n",
    "\n",
    "    selected_data = periodicaldatesforextensionperiod_src_df.where(\n",
    "        (col(\"entityid\") == EntityID) &\n",
    "        (col(\"Value\") == extvalue) &\n",
    "        (col(\"financialcycleid\") == FiscalPeriodID)\n",
    "        ).select(\"opening\",\"closing\",\"Year\").collect()\n",
    "    \n",
    "    extopen = selected_data[0][\"opening\"]\n",
    "    extclose = selected_data[0][\"closing\"]\n",
    "    extendedcycleyear = selected_data[0][\"Year\"]\n",
    "\n",
    "    print(extopen)\n",
    "    print(extclose)\n",
    "    print(extendedcycleyear)\n",
    "   \n",
    "    # --------Handle YTD calc for ext financial cycle ----------\n",
    "    legacysddates_df = spark.read.parquet(adls_path+folder_path+\"legacysddates\", schema=legacysddates_schema)\n",
    "    if extendedcycleyear == int(FinancialYear):\n",
    "\n",
    "        print(\"extended year\")\n",
    "        \n",
    "        get_extended_period_legacydates(legacysddates_df,periodicaldatesforextensionperiod_src_df,extvalue,FiscalPeriodID,FinancialYear,EntityID,extopen,extclose,extendedcycleyear)\n",
    "        legacysddates_num_df = spark.read.parquet(adls_path+folder_path+\"legacysddates_num\", schema=legacysddates_schema)\n",
    "\n",
    "        legacymatchperiodforlargerftp_df = legacysddates_num_df.alias(\"ld\").join(periodicaldatesforextensionperiod_src_df.alias(\"mp\"),(col(\"mp.closing\") <= col(\"ld.currentvalue\")) & (col(\"mp.opening\") >= col(\"ld.openingperiodvalue\")) & (col(\"ld.ftpperiod\") >= col(\"mp.Order\"))).where((col(\"financialcycleid\") == FiscalPeriodID) & (col(\"Entityid\") == EntityID)).select(\n",
    "        \"PeriodType\",\n",
    "        \"closing\",\n",
    "        \"LegacyPeriod\",\n",
    "        \"numericvalueofperiod\"\n",
    "        ).distinct()\n",
    "\n",
    "        legacymatchperiodforequalftp_df = legacysddates_num_df.alias(\"ld\").join(periodicaldatesforextensionperiod_src_df.alias(\"mp\"),(col(\"mp.closing\") <= col(\"ld.currentvalue\")) & (col(\"mp.opening\") >= col(\"ld.openingperiodvalue\")) & (col(\"ld.ftpperiod\") == col(\"mp.Order\"))).where((col(\"financialcycleid\") == FiscalPeriodID) & (col(\"Entityid\") == EntityID)).select(\n",
    "        \"PeriodType\",\n",
    "        \"closing\",\n",
    "        \"LegacyPeriod\",\n",
    "        \"numericvalueofperiod\"\n",
    "        ).distinct()\n",
    "\n",
    "        tballperiod_df = (\n",
    "            trialbalance_dim_df.alias(\"tb\")\n",
    "            .join(legacymatchperiodforlargerftp_df.alias(\"ld\"), (F.substring(\"tb.Period\", 1, 6) == col(\"ld.closing\")))\n",
    "            .where(col(\"EntityID_fk\") == EntityID)\n",
    "            .select(\n",
    "                col(\"Period\"),\n",
    "                F.substring(\"Period\", 1, 6).alias(\"stdperiod\"),\n",
    "                col(\"ld.*\"),*alldimcols\n",
    "            ).distinct()\n",
    "            .orderBy(\"PeriodType\")\n",
    "        )\n",
    "\n",
    "        tballperiod_df.coalesce(1).write.format(\"parquet\").mode(\"overwrite\").save(adls_path+folder_path+\"tballperiod\")\n",
    "        tblegacymatchperiod_df = (\n",
    "            trialbalance_dim_df.alias(\"tb\")\n",
    "            .join(legacymatchperiodforequalftp_df.alias(\"ld\"), (F.substring(\"tb.Period\", 1, 6) == col(\"ld.closing\")))\n",
    "            .where(col(\"EntityID_fk\") == EntityID)\n",
    "            .select(\n",
    "                col(\"Period\"),\n",
    "                F.substring(\"Period\", 1, 6).alias(\"stdperiod\"),\n",
    "                col(\"ld.*\"),*alldimcols\n",
    "            ).distinct()\n",
    "            .orderBy(\"PeriodType\")\n",
    "        )\n",
    "\n",
    "        tblegacymatchperiod_df.coalesce(1).write.format(\"parquet\").mode(\"overwrite\").save(adls_path+folder_path+\"tblegacymatchperiod\")\n",
    "        tblegacymatchperiod_df = spark.read.parquet(adls_path+folder_path+\"tblegacymatchperiod\", schema = tblegacymatchperiod_schema)\n",
    "        \n",
    "         # # ------table contain the number of months in ectended period----\n",
    "\n",
    "        numperiod_df = legacysddates_num_df.alias(\"ld\") \\\n",
    "        .join(tblegacymatchperiod_df.alias(\"md\"), [\"periodtype\", \"legacyperiod\"]) \\\n",
    "        .withColumn(\n",
    "        \"val_numericvalueofperiod\",\n",
    "        when(expr(\"left(ld.Value, 1)\") == 'Y', extendedmonthdiff)\n",
    "        .when(expr(\"left(ld.Value, 1)\") == '9', extendedmonthdiff - 3)\n",
    "        .when(expr(\"left(ld.Value, 1)\") == 'H', extendedmonthdiff - 6)\n",
    "        .when(expr(\"left(ld.Value, 1)\") == 'Q', extendedmonthdiff - 9)\n",
    "        .otherwise(1)\n",
    "        ) \\\n",
    "        .groupBy(\"periodtype\", \"LegacyPeriod\") \\\n",
    "        .agg(max(\"val_numericvalueofperiod\").alias(\"val_numericvalueofperiod\")) \\\n",
    "        .select(\"periodtype\",\"LegacyPeriod\",col(\"val_numericvalueofperiod\").alias(\"numericvalueofperiod\"))\n",
    "        \n",
    "\n",
    "        tblegacymatchperiod_del1_df = tblegacymatchperiod_df.filter(~(\n",
    "        (col(\"Period\").substr(1, 6) == col(\"LegacyPeriod\").substr(1, 6)) &\n",
    "        (col(\"Period\").substr(-2, 2).cast(\"int\") != abs(col(\"numericvalueofperiod\")))\n",
    "        ))\n",
    "\n",
    "        tblegacymatchperiod_del2_df = tblegacymatchperiod_del1_df.filter(~(\n",
    "            (col(\"Period\").substr(1, 6) != col(\"LegacyPeriod\").substr(1, 6)) &\n",
    "            (col(\"Period\").substr(-2, 2).cast(\"int\") < abs(col(\"NumericValueofPeriod\")))\n",
    "        ))\n",
    "\n",
    "        tblegacymatchperiod_del3_df = tblegacymatchperiod_del2_df.alias(\"tb1\").join(numperiod_df.alias(\"nm\"), [\"PeriodType\", \"LegacyPeriod\"]) \\\n",
    "        .filter((col(\"tb1.Period\").substr(1, 6) != col(\"tb1.LegacyPeriod\").substr(1, 6)) & (col(\"tb1.Period\").substr(-2, 2).cast(\"int\") > abs(col(\"nm.numericvalueofperiod\")))\n",
    "        ).select(\"tb1.*\")\n",
    "\n",
    "        tblegacymatchperiod_updated_df = tblegacymatchperiod_del2_df.exceptAll(tblegacymatchperiod_del3_df)\n",
    "        \n",
    "        legacysddates_tmp_df = legacysddates_num_df.alias(\"ls\").join(tblegacymatchperiod_updated_df.alias(\"mp\"),((col(\"ls.LegacyPeriod\") == col(\"mp.LegacyPeriod\")) & (col(\"ls.periodtype\") == col(\"mp.periodtype\"))),\"left\") \\\n",
    "                            .filter(col(\"mp.LegacyPeriod\").isNull()) \\\n",
    "                                        .select(\"ls.LegacyPeriod\",\"ls.periodtype\",\"ls.ftpperiod\").withColumn(\"fromperiod\",concat(F.year(F.add_months(F.to_date(F.concat(F.substring(col('LegacyPeriod'), 1, 4),F.lit('-'),F.substring(col('LegacyPeriod'), 5, 2),F.lit('-01')),'yyyy-MM-dd'),-F.expr(\"CAST(RIGHT(LegacyPeriod, 2) AS INT)\"))),  \n",
    "                                F.substring(F.date_format(F.add_months(F.to_date(F.concat(F.substring(col('LegacyPeriod'), 1, 4),F.lit('-'),F.substring(col('LegacyPeriod'), 5, 2),\n",
    "                                                F.lit('-01')),'yyyy-MM-dd'),-F.expr(\"CAST(RIGHT(LegacyPeriod, 2) AS INT)\")),'yyyy-MM-dd' ),6,2)))\n",
    "        \n",
    "        tballperiod_df = spark.read.parquet(adls_path+folder_path+\"tballperiod\", schema = tballperiod_schema )\n",
    "        priorityseq_df1 = tballperiod_df.alias(\"tb\").join(legacysddates_tmp_df.alias(\"ld\"), ((col(\"tb.Period\").substr(1, 6) <= col(\"ld.LegacyPeriod\").substr(1, 6)) &\n",
    "                                    (col(\"tb.Period\").substr(1, 6) > col(\"fromperiod\")) & (col(\"tb.PeriodType\") == col(\"ld.periodtype\"))),\"inner\").filter((col(\"tb.Period\").substr(-2, 2) != col(\"ld.LegacyPeriod\").substr(-2, 2)) & ((substring(col(\"Period\"), -2, 2).cast(\"int\")) <= abs(col(\"numericvalueofperiod\")))).select(\"tb.*\",\"ld.ftpperiod\")\n",
    "        \n",
    "        window_spec = Window().partitionBy(\"stdperiod\", \"periodtype\").orderBy(col(\"Period\").desc())\n",
    "\n",
    "        priorityseq_df = priorityseq_df1 \\\n",
    "                            .withColumn(\"priorityseq\", row_number().over(window_spec)) \\\n",
    "                            .withColumn(\"order\", lit(0)) \\\n",
    "                            .select(\n",
    "                                col(\"Period\"),\n",
    "                                col(\"stdperiod\"),\n",
    "                                col(\"PeriodType\"),\n",
    "                                col(\"closing\"),\n",
    "                                col(\"legacyperiod\"),\n",
    "                                col(\"numericvalueofperiod\"),\n",
    "                                col(\"priorityseq\"),\n",
    "                                col(\"order\"),\n",
    "                                col(\"ftpperiod\")\n",
    "                            ) \\\n",
    "                            .orderBy(\"periodtype\")                  \n",
    "        \n",
    "        #---populate order for ext cycle periods---\n",
    "        mp_df = periodicaldatesforextensionperiod_src_df \\\n",
    "                                .withColumn(\"FromPeriod\",substring(to_date(concat(lit(\"01-\"),substring(col(\"period\"),instr(col(\"period\"), \" \") + 1,when(instr(col(\"period\"), \"-\") == 0,\n",
    "                                length(col(\"period\")) - instr(col(\"period\"), \" \"))\n",
    "                            .otherwise(instr(col(\"period\"), \"-\") - instr(col(\"period\"), \" \") - 1)\n",
    "                        ),lit(\"-\"),substring(col(\"closing\"), 1, 4)), \"dd-MMM-yyyy\"),6,2))\n",
    "        ps_df = priorityseq_df.withColumn(\"fromperiod\",\n",
    "                    concat(F.year(F.add_months(F.to_date(F.concat(F.substring(col('Period'), 1, 4),F.lit('-'),F.substring(col('Period'), 5, 2),F.lit('-01')),'yyyy-MM-dd'),-F.expr(\"CAST(RIGHT(Period, 2) AS INT)\"))), \n",
    "                    F.substring(F.date_format(F.add_months(F.to_date(F.concat(F.substring(col('Period'), 1, 4),F.lit('-'),F.substring(col('Period'), 5, 2),\n",
    "                    F.lit('-01')),'yyyy-MM-dd'),-F.expr(\"CAST(RIGHT(Period, 2) AS INT)\")),'yyyy-MM-dd' ),6,2)))\n",
    "\n",
    "        cteorder_df = mp_df.alias(\"mp\").join(ps_df.alias(\"ps\"),(col(\"ps.closing\") == col(\"mp.closing\")) & (col(\"ps.ftpperiod\") > col(\"mp.Order\")) & (col(\"ps.fromperiod\") == concat(col(\"ps.FromPeriod\").substr(1,4),col(\"mp.FromPeriod\")))).filter((col(\"financialcycleid\") == FiscalPeriodID) & (col(\"entityid\") == EntityID)) \\\n",
    "                        .groupBy(\"ps.Period\",\"ps.PeriodType\",\"mp.closing\",\"ps.legacyperiod\",\"priorityseq\") \\\n",
    "                        .agg(F.max(col(\"mp.Order\")).alias(\"Order\"))\n",
    "        # Perform the update by joining the original DataFrame with the CTE results\n",
    "        priority_cte_joined_df = priorityseq_df.alias(\"ps\").join(cteorder_df.alias(\"cte\"), on=\"Period\", how=\"left_outer\").select(\"ps.*\",col(\"cte.period\").alias(\"cte_period\"),col(\"cte.order\").alias(\"cte_order\"))\n",
    "\n",
    "        priorityseq_updated_df = priority_cte_joined_df.withColumn(\n",
    "            \"order\",\n",
    "            when(col(\"ps.Period\") == col(\"cte_Period\"), col(\"cte_order\")).otherwise(col(\"ps.order\"))\n",
    "        ).drop(\"cte_Period\",\"cte_order\")\n",
    "\n",
    "        \n",
    "        ld1_df = legacysddates_num_df.alias(\"ld\").join(tblegacymatchperiod_updated_df.alias(\"tbm\"),(col(\"ld.periodtype\") == col(\"tbm.periodtype\")) & \n",
    "                            (col(\"ld.legacyperiod\") == col(\"tbm.legacyperiod\")), \"inner\") \\\n",
    "                            .groupBy(\"tbm.periodtype\", \"ld.FinancialYear\", \"ld.LegacyPeriod\", \"ftpperiod\") \\\n",
    "                            .agg(\n",
    "                            max(col(\"tbm.Period\")).alias(\"Period\")) \\\n",
    "                            .withColumn(\"ftpperiod\",when(col(\"ftpperiod\") == 1,col(\"ftpperiod\")).otherwise(col(\"ftpperiod\")-1)) \\\n",
    "                            .withColumn(\"fromperiod\",\n",
    "                    concat(F.year(F.add_months(F.to_date(F.concat(F.substring(col('ld.LegacyPeriod'), 1, 4),F.lit('-'),F.substring(col('ld.LegacyPeriod'), 5, 2),F.lit('-01')),'yyyy-MM-dd'),-F.expr(\"CAST(RIGHT(ld.LegacyPeriod, 2) AS INT)\"))), \n",
    "                    F.substring(F.date_format(F.add_months(F.to_date(F.concat(F.substring(col('ld.LegacyPeriod'), 1, 4),F.lit('-'),F.substring(col('ld.LegacyPeriod'), 5, 2),\n",
    "                    F.lit('-01')),'yyyy-MM-dd'),-F.expr(\"CAST(RIGHT(ld.LegacyPeriod, 2) AS INT)\")),'yyyy-MM-dd' ),6,2))) \\\n",
    "                    .select(\"tbm.periodtype\",\"Period\",\"ld.FinancialYear\",\"ld.LegacyPeriod\", \"ftpperiod\",\"fromperiod\")\n",
    "        \n",
    "        ld_df = ld1_df.alias(\"ld1\").join(periodicaldatesforextensionperiod_src_df.alias(\"pd\"),(col(\"pd.closing\") > col(\"ld1.fromperiod\")) & (col(\"pd.closing\") <= col(\"ld1.LegacyPeriod\").substr(1, 6))\n",
    "                        & (col(\"pd.Order\") == col(\"ld1.ftpperiod\")) & (col(\"pd.Year\") == col(\"FinancialYear\"))).filter((col(\"financialcycleid\") == FiscalPeriodID) & (col(\"Entityid\") == EntityID)) \\\n",
    "                        .withColumn(\"FromOpeningPeriod\",concat(substring(col(\"FromPeriod\"), 1, 4),substring(to_date(concat(lit(\"01-\"),substring(\n",
    "                                col(\"pd.Period\"),\n",
    "                                instr(col(\"pd.Period\"), \" \") + 1,  # Starting index\n",
    "                                when(instr(col(\"pd.Period\"), \"-\") == 0, \n",
    "                                    length(col(\"pd.Period\")) - instr(col(\"pd.Period\"), \" \")\n",
    "                                    )\n",
    "                                .otherwise(instr(col(\"pd.Period\"), \"-\") - instr(col(\"pd.Period\"), \" \") - 1)\n",
    "                            ),lit(\"-\"),substring(col(\"pd.closing\"), 1, 4)),\"dd-MMM-yyyy\"),6,2))) \\\n",
    "                        .select(\"ld1.PeriodType\",\"ld1.Period\",\"ld1.FinancialYear\",\"ld1.LegacyPeriod\",\"ld1.ftpperiod\"\n",
    "                            ,\"pd.Value\",\"pd.closing\",\"FromOpeningPeriod\")\n",
    "                        \n",
    "        \n",
    "        prioritylevel2temp = ld_df.withColumn(\"monthdiff\",\n",
    "        when(col(\"Value\").isin(\"H1\", \"Q1\", \"Year\", \"9M\"),\n",
    "            (\n",
    "        months_between(\n",
    "            to_date(\n",
    "            concat(\n",
    "                lit(\"01-\"), \n",
    "                col(\"closing\").substr(-2, 2), \n",
    "                lit(\"-\"), \n",
    "                col(\"closing\").substr(1, 4)\n",
    "            ), \n",
    "            \"dd-MM-yyyy\"\n",
    "        ), \n",
    "        to_date(\n",
    "            concat(\n",
    "                lit(\"01-\"), \n",
    "                col(\"FromOpeningPeriod\").substr(-2, 2), \n",
    "                lit(\"-\"), \n",
    "                col(\"FromOpeningPeriod\").substr(1, 4)\n",
    "            ), \n",
    "            \"dd-MM-yyyy\"\n",
    "        )\n",
    "        )+1\n",
    "        ).cast(\"int\"))\n",
    "        .when(~(col(\"Value\").isin(\"H1\", \"Q1\", \"Year\", \"9M\")),\n",
    "            when(col(\"Value\").substr(1, 1) == \"9\", \"9\")\n",
    "            .when(col(\"Value\").substr(1, 1) == \"H\", \"6\")\n",
    "            .when(col(\"Value\").substr(1, 1) == \"Q\", \"3\")\n",
    "            .when(col(\"Value\").substr(1, 1) == \"M\", \"1\")\n",
    "            .otherwise(None))\n",
    "        ).select(\"periodtype\",\"Period\",\"legacyperiod\",\"ftpperiod\",\"closing\",\"monthdiff\").distinct()\n",
    "           \n",
    "        prioritylevel2temp_df = trialbalance_dim_df.alias(\"tb\").join(prioritylevel2temp.alias(\"pt\"),(col(\"tb.Period\").substr(1,6) == col(\"pt.closing\")) & ((abs(col(\"tb.Period\").substr(-2, 2))) == col(\"pt.monthdiff\")),\"inner\").where( col(\"EntityID_fk\") == EntityID).select(\"tb.Period\",\n",
    "                                col(\"tb.Period\").substr(1,6).alias(\"stdperiod\"),\n",
    "                                \"periodtype\",\n",
    "                                col(\"tb.Period\").substr(1,6).alias(\"closing\"),\n",
    "                                \"legacyperiod\",\n",
    "                                col(\"ftpperiod\").alias(\"numericvalueofperiod\"),*alldimcols).distinct()\n",
    "                \n",
    "        \n",
    "        window_spec = Window().partitionBy(\"stdperiod\", \"periodtype\").orderBy(col(\"Period\").desc())\n",
    "\n",
    "        prioritylevel2_inst_df = prioritylevel2temp_df \\\n",
    "                            .withColumn(\"priorityseq\", row_number().over(window_spec)) \\\n",
    "                            .withColumn(\"order\", lit(0)) \\\n",
    "                            .select(\n",
    "                                col(\"Period\"),\n",
    "                                col(\"stdperiod\"),\n",
    "                                col(\"PeriodType\"),\n",
    "                                col(\"closing\"),\n",
    "                                col(\"legacyperiod\"),\n",
    "                                col(\"numericvalueofperiod\"),\n",
    "                                col(\"priorityseq\"),\n",
    "                                col(\"order\")\n",
    "                            ) \\\n",
    "                            .orderBy(\"periodtype\")\n",
    "                \n",
    "        priorityseq_final_df = priorityseq_updated_df.filter(col(\"priorityseq\") == 1)\n",
    "\n",
    "        prioritylevel2_final_df = prioritylevel2_inst_df.filter(col(\"priorityseq\") == 1)\n",
    "\n",
    "\n",
    "        ######################## For Next level priority check ##############################\n",
    "\n",
    "        if_exists_df = (\n",
    "        priorityseq_final_df\n",
    "        .groupBy(\"PeriodType\")\n",
    "        .agg({\"Order\": \"max\"})\n",
    "        .withColumnRenamed(\"max(Order)\", \"order\")\n",
    "        .exceptAll(\n",
    "        priorityseq_final_df.alias(\"ps\")\n",
    "        .join(\n",
    "        legacysddates_num_df.alias(\"ld\"),\n",
    "        (col(\"ps.PeriodType\") == col(\"ld.PeriodType\")) &\n",
    "        (col(\"ps.LegacyPeriod\") == col(\"ld.LegacyPeriod\")) &\n",
    "        (col(\"ps.Order\") == col(\"ld.ftpperiod\") - 1),\n",
    "        \"inner\"\n",
    "        )\n",
    "        .select(\"ps.PeriodType\", \"ps.Order\")\n",
    "        ))\n",
    "        if not if_exists_df.isEmpty():\n",
    "            print(\"if exists not empty\")\n",
    "            # If the DataFrame is not empty, perform further operations\n",
    "            ctePS_df = (\n",
    "                priorityseq_final_df.groupBy(\"PeriodType\",\"LegacyPeriod\").agg(max(col(\"Order\")).alias(\"Order\")).select(\"PeriodType\",\"LegacyPeriod\",\"Order\")\n",
    "            )\n",
    "            legacysddates_num1_df = legacysddates_num_df.alias(\"ld\").join(ctePS_df.alias(\"ps\"),\n",
    "                    (col(\"ps.periodtype\") == col(\"ld.PeriodType\")) &\n",
    "                    (col(\"ps.legacyperiod\") == col(\"ld.legacyperiod\")) & (col(\"ps.Order\") != col(\"ld.ftpperiod\")) ,\"left_outer\").select(\"ld.*\",col(\"ps.Order\").alias(\"ps_Order\"),col(\"ps.legacyperiod\").alias(\"ps_legacyperiod\"),col(\"ps.periodtype\").alias(\"ps_periodtype\")) \\\n",
    "                    .withColumn(\"ftpperiod\",when((col(\"ps_periodtype\") == col(\"PeriodType\")) &\n",
    "                    (col(\"ps_legacyperiod\") == col(\"legacyperiod\")) & (col(\"ps_Order\") != col(\"ftpperiod\")), col(\"ps_Order\") + 1).otherwise(col(\"ftpperiod\"))).drop(\"ps_Order\",\"ps_legacyperiod\",\"ps_periodtype\")\n",
    "            legacysddates_num1_df.coalesce(1).write.format(\"parquet\").mode(\"overwrite\").save(adls_path+folder_path+\"legacysddates_num\")\n",
    "            \n",
    "        else:\n",
    "            print(\"else exists is empty\")\n",
    "            legacysddates_num1_df = legacysddates_num_df\n",
    "            legacysddates_num1_df.coalesce(1).write.format(\"parquet\").mode(\"overwrite\").save(adls_path+folder_path+\"legacysddates_num\")\n",
    "\n",
    "        \n",
    "        legacysddates_num1_df = spark.read.parquet(adls_path+folder_path+\"legacysddates_num\",schema=legacysddates_schema)\n",
    "        deleteplev2_df = tblegacymatchperiod_updated_df.filter(col(\"Period\") == col(\"LegacyPeriod\")).select(\"LegacyPeriod\",\"PeriodType\")\n",
    "\n",
    "        prioritylevel2_final_del_df = prioritylevel2_final_df.alias(\"pl\").join(\n",
    "            deleteplev2_df.alias(\"tbm\"),\n",
    "            (col(\"tbm.PeriodType\") == col(\"pl.PeriodType\")) &\n",
    "            (col(\"tbm.LegacyPeriod\") == col(\"pl.LegacyPeriod\")),\n",
    "            \"left_anti\"\n",
    "        )\n",
    "        \n",
    "        prioritylevel2_final_del_df.coalesce(1).write.format(\"parquet\").mode(\"overwrite\").save(adls_path+folder_path+\"prioritylevel2_ext\")\n",
    "       \n",
    "        prioritylevel2_final_del_df = spark.read.parquet(adls_path+folder_path+\"prioritylevel2_ext\",schema=prioritylevel2_ext_schema)\n",
    "\n",
    "        tblegacymatchperiod_del_df = (\n",
    "        tblegacymatchperiod_updated_df.alias(\"tbm\")\n",
    "        .join(\n",
    "        prioritylevel2_final_del_df.alias(\"pl\"),\n",
    "        (col(\"tbm.PeriodType\") == col(\"pl.PeriodType\")) &\n",
    "        (col(\"tbm.LegacyPeriod\") == col(\"pl.LegacyPeriod\")),\n",
    "        \"left_anti\"\n",
    "        )\n",
    "        )\n",
    "\n",
    "        priorityseq_final_del_df = (\n",
    "        priorityseq_final_df.alias(\"ps\")\n",
    "        .join(\n",
    "        legacysddates_num1_df.alias(\"ld\"),\n",
    "        (col(\"ps.PeriodType\") == col(\"ld.PeriodType\")) &\n",
    "        (col(\"ps.LegacyPeriod\") == col(\"ld.LegacyPeriod\")) &\n",
    "        (col(\"ps.Order\") != col(\"ld.ftpperiod\") - 1),\n",
    "        \"left_anti\"\n",
    "        )\n",
    "        )\n",
    "        priorityseq_final_del_df.coalesce(1).write.format(\"parquet\").mode(\"overwrite\").save(adls_path+folder_path+\"priorityseq\")\n",
    "        \n",
    "        priorityseq_final_del_df = spark.read.parquet(adls_path+folder_path+\"priorityseq\",schema=priorityseq_schema)\n",
    "\n",
    "        bspriority_tmp_df = (\n",
    "                        legacysddates_num1_df.alias(\"ld\").join(priorityseq_final_del_df.alias(\"ps\"), (col(\"ld.PeriodType\") == col(\"ps.PeriodType\")) & (col(\"ld.LegacyPeriod\") == col(\"ps.LegacyPeriod\"))).select(\"ld.PeriodType\",\"ld.legacyperiod\",\"ld.currentvalue\",\"ps.Period\",\"ps.stdperiod\") \\\n",
    "                        .union(\n",
    "                            legacysddates_num1_df.alias(\"ld\").join(prioritylevel2_final_del_df.alias(\"ps\"), (col(\"ld.PeriodType\") == col(\"ps.PeriodType\")) & (col(\"ld.LegacyPeriod\") == col(\"ps.LegacyPeriod\"))).select(\"ld.PeriodType\",\"ld.legacyperiod\",\"ld.currentvalue\",\"ps.Period\",\"ps.stdperiod\")))\n",
    "\n",
    "        tblegacymatchperiod_del_grouped_df =  tblegacymatchperiod_del_df.withColumn(\"currentvalue\",col(\"legacyperiod\").substr(1,6)).select(\"PeriodType\",\"legacyperiod\",\"currentvalue\",\"Period\",\"stdperiod\")\n",
    "\n",
    "\n",
    "        bspriority_df = bspriority_tmp_df.union(tblegacymatchperiod_del_grouped_df) \\\n",
    "                        .groupBy(\"PeriodType\", \"legacyperiod\", \"currentvalue\") \\\n",
    "                        .agg(F.max(\"Period\").alias(\"Period\"),\n",
    "                            F.max(\"stdperiod\").alias(\"stdperiod\")).select(\"PeriodType\",\"legacyperiod\",\"currentvalue\",\"Period\",\"stdperiod\")\n",
    "                        \n",
    "        bspriority_df.coalesce(1).write.format(\"parquet\").mode(\"overwrite\").save(adls_path+folder_path+\"bspriority\")\n",
    "        bspriority_df = spark.read.parquet(adls_path+folder_path+\"bspriority\")\n",
    "        legacysddates_updt_df = legacysddates_num1_df.alias(\"ld\") \\\n",
    "        .join(\n",
    "        bspriority_df.alias(\"ps\"),\n",
    "        (col(\"ld.PeriodType\") == col(\"ps.PeriodType\")) &\n",
    "        (col(\"ld.LegacyPeriod\") == col(\"ps.LegacyPeriod\")),\n",
    "        \"left_outer\").select(\"ld.*\",col(\"ps.Period\").alias(\"ps_Period\"),col(\"ps.stdperiod\").alias(\"ps_stdperiod\"),col(\"ps.PeriodType\").alias(\"ps_PeriodType\"),col(\"ps.LegacyPeriod\").alias(\"ps_LegacyPeriod\")).withColumn(\"legacyperiod\",when((col(\"PeriodType\") == col(\"ps_PeriodType\")) &\n",
    "        (col(\"LegacyPeriod\") == col(\"ps_LegacyPeriod\")),col(\"ps_Period\")).otherwise(col(\"legacyperiod\"))) \\\n",
    "        .withColumn(\"currentvalue\",when((col(\"PeriodType\") == col(\"ps_PeriodType\")) &\n",
    "        (col(\"LegacyPeriod\") == col(\"ps_LegacyPeriod\")),col(\"ps_stdperiod\")).otherwise(col(\"currentvalue\"))).drop(\"ps_Period\",\"ps_stdperiod\",\"ps_PeriodType\",\"ps_LegacyPeriod\")\n",
    "        \n",
    "        legacysddates_updt_df.coalesce(1).write.format(\"parquet\").mode(\"overwrite\").save(adls_path+folder_path+\"legacysddates\")\n",
    "        legacysddates_updt_df = spark.read.parquet(adls_path+folder_path+\"legacysddates\",schema=legacysddates_schema)\n",
    "\n",
    "        #### ---delete from legacyperiod -----\n",
    "\n",
    "        tblegacymatchperiod_del_df1 = tblegacymatchperiod_del_df.filter(col(\"Period\").substr(1, 6) != col(\"LegacyPeriod\").substr(1, 6)).alias(\"a\").join(tblegacymatchperiod_del_df.alias(\"b\"),col(\"a.LegacyPeriod\") == col(\"b.Period\"),\"left_anti\")\n",
    "\n",
    "        tblegacymatchperiod_final_del_df = tblegacymatchperiod_del_df.alias(\"tb\").join(tblegacymatchperiod_del_df1.alias(\"del\"),col(\"tb.LegacyPeriod\") ==col(\"del.LegacyPeriod\"),\"left_anti\")\n",
    "        tblegacymatchperiod_final_del_df.coalesce(1).write.format(\"parquet\").mode(\"overwrite\").save(adls_path+folder_path+\"tblegacymatchperiod\")\n",
    "        tblegacymatchperiod_final_del_df = spark.read.parquet(adls_path+folder_path+\"tblegacymatchperiod\",schema=tblegacymatchperiod_schema)\n",
    "     \n",
    "        print(\"count of comp\",countofcomp)\n",
    "        if countofcomp == 0 or countofcomp ==1:\n",
    "            print(\"count equal to 0 or 1\")\n",
    "            ytddates_df = tblegacymatchperiod_final_del_df.filter(col(\"Periodtype\").like('%YTD') | col(\"Periodtype\").like('%Year')).select(\"PeriodType\",col(\"legacyperiod\").substr(1,6).alias(\"CurrentValue\"),col(\"Period\").alias(\"FTPValue\")) \\\n",
    "            .union(priorityseq_final_del_df.filter(col(\"Periodtype\").like('%YTD') | col(\"Periodtype\").like('%Year')).select(\"PeriodTYpe\",col(\"legacyperiod\").substr(1,6).alias(\"CurrentValue\"),col(\"Period\").alias(\"FTPValue\"))) \\\n",
    "            .union(prioritylevel2_final_del_df.filter(col(\"Periodtype\").like('%YTD') | col(\"Periodtype\").like('%Year')).select(\"PeriodTYpe\",col(\"legacyperiod\").substr(1,6).alias(\"CurrentValue\"),col(\"Period\").alias(\"FTPValue\"))).distinct()\n",
    "            \n",
    "            ytddates_df.coalesce(1).write.format(\"parquet\").mode(\"overwrite\").save(adls_path+folder_path+\"ytddates\")\n",
    "\n",
    "        if countofcomp > 1:\n",
    "            print(\"count greater than 1\")\n",
    "            ytddates_df1 = (\n",
    "            legacysddates_updt_df.select(\"periodtype\",\"pcurrentvalue\",\"legacyperiod\")\n",
    "            .filter(~col(\"periodtype\").isin(\"currentPeriod\", \"PreviousPeriod\"))\n",
    "            .exceptAll(\n",
    "            legacysddates_updt_df.filter((col(\"legacyperiod\").substr(1, 6) != col(\"pcurrentvalue\"))).alias(\"a\")\n",
    "            .join(\n",
    "                tblegacymatchperiod_final_del_df.alias(\"b\"),    \n",
    "                (col(\"a.legacyperiod\") == col(\"b.Period\")) &\n",
    "                (col(\"a.periodtype\") == col(\"b.PeriodType\")),\n",
    "                \"left_anti\"\n",
    "            )\n",
    "            .select(\"a.periodtype\", \"a.pcurrentvalue\", \"a.legacyperiod\")\n",
    "            )\n",
    "            .union(\n",
    "                priorityseq_final_del_df\n",
    "                .select(\"PeriodType\", col(\"legacyperiod\").substr(1, 6).alias(\"legacyperiod\"), \"Period\")\n",
    "            )\n",
    "            .union(\n",
    "                prioritylevel2_final_del_df\n",
    "                .select(\"PeriodType\", col(\"legacyperiod\").substr(1, 6).alias(\"legacyperiod\"), \"Period\")\n",
    "            )\n",
    "            .union(\n",
    "            tblegacymatchperiod_final_del_df\n",
    "            .select(\"PeriodType\", col(\"LegacyPeriod\").substr(1, 6).alias(\"pcurrentvalue\"), \"Period\")\n",
    "            .exceptAll(\n",
    "                legacysddates_updt_df\n",
    "                .select(\"PeriodType\", \"pcurrentvalue\", \"legacyperiod\"))\n",
    "                .select(\"PeriodType\", col(\"pcurrentvalue\"), \"Period\")\n",
    "            \n",
    "        ).distinct()\n",
    "            )\n",
    "            print(\"ytddates table result final\")\n",
    "            ytddates_df = ytddates_df1.withColumnRenamed(\"pcurrentvalue\",\"CurrentValue\") \\\n",
    "            .withColumnRenamed(\"legacyperiod\",\"FTPValue\").distinct()\n",
    "            \n",
    "            ytddates_df.coalesce(1).write.format(\"parquet\").mode(\"overwrite\").save(adls_path+folder_path+\"ytddates\")\n",
    "    else:\n",
    "        print(\"not extended year\")\n",
    "        legacysddates_num_df = get_extended_period_legacydates(legacysddates_df,periodicaldatesforextensionperiod_src_df,extvalue,FiscalPeriodID,FinancialYear,EntityID,extopen,extclose,extendedcycleyear)    \n",
    "        \n",
    "        legacysddates_num_df = spark.read.parquet(adls_path+folder_path+\"legacysddates_num\",schema=legacysddates_schema)\n",
    "        \n",
    "        priority_rows_to_delete = priorityseq_df.join(legacysddates_num_df, (priorityseq_df[\"periodtype\"] == legacysddates_num_df[\"periodtype\"]) & \\\n",
    "                 (legacysddates_num_df[\"financialyear\"] == extendedcycleyear), how=\"inner\")\n",
    "        \n",
    "        priorityseq_del_df = priorityseq_df.join(priority_rows_to_delete, on=priorityseq_df.columns, how=\"left_anti\")\n",
    "        \n",
    "        prioritylevel2_rows_to_delete = prioritylevel2_df.join(legacysddates_num_df, (prioritylevel2_df[\"periodtype\"] == legacysddates_num_df[\"periodtype\"]) & \\\n",
    "                        (legacysddates_num_df[\"financialyear\"] == extendedcycleyear), how=\"inner\").select(prioritylevel2_df[\"*\"])\n",
    "        \n",
    "        prioritylevel2_del_df = prioritylevel2_df.join(prioritylevel2_rows_to_delete, on=prioritylevel2_df.columns, how=\"left_anti\")\n",
    "        \n",
    "        \n",
    "        ld_tmp_df = legacysddates_num_df.alias(\"ld\").join(periodicaldatesforextensionperiod_src_df.alias(\"mp\"),(col(\"mp.closing\") <= col(\"ld.CurrentValue\")) & (col(\"mp.opening\") >= col(\"ld.openingperiodvalue\")) & (col(\"ld.ftpperiod\") >= col(\"mp.Order\"))).where((col(\"financialcycleid\") == FiscalPeriodID) & (col(\"Entityid\") == EntityID)) \\\n",
    "                .select(\"PeriodType\",\"closing\",\"LegacyPeriod\",\"numericvalueofperiod\").distinct()\n",
    "\n",
    "        tballperiod_df = trialbalance_dim_df.alias(\"tb\").join(ld_tmp_df.alias(\"ld\"),(col(\"tb.Period\").substr(1, 6) == col(\"ld.closing\"))).where(col(\"EntityID_fk\") == EntityID) \\\n",
    "            .select(\"Period\",col(\"tb.Period\").substr(1, 6).alias(\"stdperiod\"),\"ld.*\",*alldimcols).distinct() \\\n",
    "            .orderBy(\"PeriodType\")\n",
    "\n",
    "        ld1_tmp_df = legacysddates_num_df.alias(\"ld\").join(periodicaldatesforextensionperiod_src_df.alias(\"mp\"),(col(\"mp.closing\") <= col(\"ld.CurrentValue\")) & (col(\"mp.opening\") >= col(\"ld.openingperiodvalue\")) & (col(\"ld.ftpperiod\") == col(\"mp.Order\"))).where((col(\"financialcycleid\") == FiscalPeriodID) & (col(\"Entityid\") == EntityID)) \\\n",
    "        .select(\"PeriodType\",\"closing\",\"LegacyPeriod\",\"numericvalueofperiod\").distinct()\n",
    "\n",
    "        tblegacymatchperiod_df = trialbalance_dim_df.alias(\"tb\").join(ld1_tmp_df.alias(\"ld\"),(col(\"Period\").substr(1, 6) == col(\"ld.closing\"))).where(col(\"EntityID_fk\") == EntityID) \\\n",
    "            .select(\"Period\",col(\"tb.Period\").substr(1, 6).alias(\"stdperiod\"),\"ld.*\",*alldimcols).distinct() \\\n",
    "            .orderBy(\"PeriodType\")\n",
    "        \n",
    "        tblegacymatchperiod_df.coalesce(1).write.format(\"parquet\").mode(\"overwrite\").save(adls_path+folder_path+\"tblegacymatchperiod\")\n",
    "        tblegacymatchperiod_df = spark.read.parquet(adls_path+folder_path+\"tblegacymatchperiod\",schema=tblegacymatchperiod_schema)\n",
    "        \n",
    "        numperiod_df = legacysddates_num_df.alias(\"ld\") \\\n",
    "            .join(tblegacymatchperiod_df.alias(\"md\"), [\"periodtype\", \"legacyperiod\"]) \\\n",
    "            .withColumn(\n",
    "            \"val_numericvalueofperiod\",\n",
    "            when(expr(\"left(ld.Value, 1)\") == 'Y', extendedmonthdiff)\n",
    "            .when(expr(\"left(ld.Value, 1)\") == '9', extendedmonthdiff - 3)\n",
    "            .when(expr(\"left(ld.Value, 1)\") == 'H', extendedmonthdiff - 6)\n",
    "            .when(expr(\"left(ld.Value, 1)\") == 'Q', extendedmonthdiff - 9)\n",
    "            .otherwise(1)\n",
    "            ) \\\n",
    "            .groupBy(\"periodtype\", \"LegacyPeriod\") \\\n",
    "            .agg(max(\"val_numericvalueofperiod\").alias(\"val_numericvalueofperiod\")) \\\n",
    "            .select(\"periodtype\",\"LegacyPeriod\",col(\"val_numericvalueofperiod\").alias(\"numericvalueofperiod\"))\n",
    "        \n",
    "        \n",
    "        tblegacymatchperiod_del1_df = tblegacymatchperiod_df.filter(~(\n",
    "                    (col(\"Period\").substr(1, 6) == col(\"LegacyPeriod\").substr(1, 6)) &\n",
    "                    (col(\"Period\").substr(-2, 2).cast(\"int\") != abs(col(\"numericvalueofperiod\")))\n",
    "                    ))\n",
    "        \n",
    "        tblegacymatchperiod_del2_df = tblegacymatchperiod_del1_df.filter(~(\n",
    "                (col(\"Period\").substr(1, 6) != col(\"LegacyPeriod\").substr(1, 6)) &\n",
    "                (col(\"Period\").substr(-2, 2).cast(\"int\") < abs(col(\"NumericValueofPeriod\")))\n",
    "            ))\n",
    "        \n",
    "\n",
    "        tblegacymatchperiod_del3_df = tblegacymatchperiod_del2_df.alias(\"tb1\").join(numperiod_df.alias(\"nm\"), [\"PeriodType\", \"LegacyPeriod\"]) \\\n",
    "            .filter((col(\"tb1.Period\").substr(1, 6) != col(\"tb1.LegacyPeriod\").substr(1, 6)) & (col(\"tb1.Period\").substr(-2, 2).cast(\"int\") > abs(col(\"nm.numericvalueofperiod\")))\n",
    "            ).select(\"tb1.*\")\n",
    "        \n",
    "        tblegacymatchperiod_updated_df = tblegacymatchperiod_del2_df.exceptAll(tblegacymatchperiod_del3_df)\n",
    "        \n",
    "        legacysddates_tmp_df = legacysddates_num_df.alias(\"ls\").join(tblegacymatchperiod_updated_df.alias(\"mp\"),((col(\"ls.LegacyPeriod\") == col(\"mp.LegacyPeriod\")) & (col(\"ls.periodtype\") == col(\"mp.periodtype\"))),\"left\") \\\n",
    "                            .filter(col(\"mp.LegacyPeriod\").isNull()) \\\n",
    "                                        .select(\"ls.LegacyPeriod\",\"ls.periodtype\",\"ls.ftpperiod\").withColumn(\"fromperiod\",concat(F.year(F.add_months(F.to_date(F.concat(F.substring(col('LegacyPeriod'), 1, 4),F.lit('-'),F.substring(col('LegacyPeriod'), 5, 2),F.lit('-01')),'yyyy-MM-dd'),-F.expr(\"CAST(RIGHT(LegacyPeriod, 2) AS INT)\"))),  \n",
    "                                F.substring(F.date_format(F.add_months(F.to_date(F.concat(F.substring(col('LegacyPeriod'), 1, 4),F.lit('-'),F.substring(col('LegacyPeriod'), 5, 2),\n",
    "                                                F.lit('-01')),'yyyy-MM-dd'),-F.expr(\"CAST(RIGHT(LegacyPeriod, 2) AS INT)\")),'yyyy-MM-dd' ),6,2)))\n",
    "        \n",
    "\n",
    "        priorityseq_df1 = tballperiod_df.alias(\"tb\").join(legacysddates_tmp_df.alias(\"ld\"), ((col(\"tb.Period\").substr(1, 6) <= col(\"ld.LegacyPeriod\").substr(1, 6)) &\n",
    "                                    (col(\"tb.Period\").substr(1, 6) > col(\"fromperiod\")) & (col(\"tb.PeriodType\") == col(\"ld.periodtype\"))),\"inner\").filter((col(\"tb.Period\").substr(-2, 2) != col(\"ld.LegacyPeriod\").substr(-2, 2)) & ((substring(col(\"Period\"), -2, 2).cast(\"int\")) <= abs(col(\"numericvalueofperiod\")))).select(\"tb.*\",\"ld.ftpperiod\")\n",
    "        \n",
    "        window_spec = Window().partitionBy(\"stdperiod\", \"periodtype\").orderBy(col(\"Period\").desc())\n",
    "        \n",
    "        priorityseq_df2 = priorityseq_df1 \\\n",
    "                            .withColumn(\"priorityseq\", row_number().over(window_spec)) \\\n",
    "                            .withColumn(\"order\", lit(0)) \\\n",
    "                            .select(\n",
    "                                col(\"Period\"),\n",
    "                                col(\"stdperiod\"),\n",
    "                                col(\"PeriodType\"),\n",
    "                                col(\"closing\"),\n",
    "                                col(\"legacyperiod\"),\n",
    "                                col(\"numericvalueofperiod\"),\n",
    "                                col(\"priorityseq\"),\n",
    "                                col(\"order\"),\n",
    "                                col(\"ftpperiod\")\n",
    "                            ) \\\n",
    "                            .orderBy(\"periodtype\")\n",
    "        priorityseq_inst_df = priorityseq_del_df.union(priorityseq_df2)                    \n",
    "        \n",
    "        mp_df_expr = concat(lit(\"01-\"),substring(col(\"period\"),instr(col(\"period\"), \" \") + 1,when(instr(col(\"period\"), \"-\") == 0,\n",
    "                                length(col(\"period\")) - instr(col(\"period\"), \" \"))\n",
    "                            .otherwise(instr(col(\"period\"), \"-\") - instr(col(\"period\"), \" \") - 1)\n",
    "                        ),lit(\"-\"),substring(col(\"closing\"), 1, 4))\n",
    "\n",
    "        ps_df_expr = F.add_months(F.to_date(F.concat(F.substring(col('Period'), 1, 4),F.lit('-'),F.substring(col('Period'), 5, 2),F.lit('-01')),'yyyy-MM-dd'),-F.expr(\"CAST(RIGHT(Period, 2) AS INT)\"))\n",
    "\n",
    "        mp_df = periodicaldatesforextensionperiod_src_df.withColumn(\"FromPeriod\",substring(to_date(mp_df_expr, \"dd-MMM-yyyy\"),6,2))\n",
    "                                \n",
    "\n",
    "        ps_df = priorityseq_inst_df.withColumn(\"fromperiod\",concat(F.year(ps_df_expr),F.substring(F.date_format(ps_df_expr,'yyyy-MM-dd' ),6,2)))\n",
    "        \n",
    "        cteorder_df = mp_df.alias(\"mp\").join(ps_df.alias(\"ps\"),(col(\"ps.closing\") == col(\"mp.closing\")) & (col(\"ps.ftpperiod\") > col(\"mp.Order\")) & (col(\"ps.fromperiod\") == concat(col(\"ps.FromPeriod\").substr(1,4),col(\"mp.FromPeriod\")))).filter((col(\"financialcycleid\") == FiscalPeriodID) & (col(\"entityid\") == EntityID)) \\\n",
    "                        .groupBy(\"ps.Period\",\"ps.PeriodType\",\"mp.closing\",\"ps.legacyperiod\",\"priorityseq\") \\\n",
    "                        .agg(F.max(col(\"mp.Order\")).alias(\"Order\"))\n",
    "\n",
    "        priority_cte_joined_df = priorityseq_inst_df.alias(\"ps\").join(cteorder_df.alias(\"cte\"), on=\"Period\", how=\"left_outer\").select(\"ps.*\",col(\"cte.period\").alias(\"cte_period\"),col(\"cte.order\").alias(\"cte_order\"))\n",
    "\n",
    "        priorityseq_updated_df = priority_cte_joined_df.withColumn(\n",
    "            \"order\",\n",
    "            when(col(\"ps.Period\") == col(\"cte_Period\"), col(\"cte_order\")).otherwise(col(\"ps.order\"))\n",
    "        ).drop(\"cte_Period\",\"cte_order\")\n",
    "        \n",
    "        ld1_df_expr = F.add_months(F.to_date(F.concat(F.substring(col('ld.LegacyPeriod'), 1, 4),F.lit('-'),F.substring(col('ld.LegacyPeriod'), 5, 2),F.lit('-01')),'yyyy-MM-dd'),-F.expr(\"CAST(RIGHT(ld.LegacyPeriod, 2) AS INT)\"))\n",
    "        \n",
    "        ld1_df = legacysddates_num_df.alias(\"ld\").join(tblegacymatchperiod_updated_df.alias(\"tbm\"),(col(\"ld.periodtype\") == col(\"tbm.periodtype\")) & \n",
    "                    (col(\"ld.legacyperiod\") == col(\"tbm.legacyperiod\")), \"inner\") \\\n",
    "                    .groupBy(\"tbm.periodtype\", \"ld.FinancialYear\", \"ld.LegacyPeriod\", \"ftpperiod\") \\\n",
    "                    .agg(\n",
    "                    max(col(\"tbm.Period\")).alias(\"Period\")) \\\n",
    "                    .withColumn(\"ftpperiod\",when(col(\"ftpperiod\") == 1,col(\"ftpperiod\")).otherwise(col(\"ftpperiod\")-1)) \\\n",
    "                    .withColumn(\"fromperiod\",\n",
    "            concat(F.year(ld1_df_expr), \n",
    "            F.substring(F.date_format(ld1_df_expr,'yyyy-MM-dd' ),6,2))) \\\n",
    "            .select(\"tbm.periodtype\",\"Period\",\"ld.FinancialYear\",\"ld.LegacyPeriod\", \"ftpperiod\",\"fromperiod\")\n",
    "        \n",
    "        ld_df = ld1_df.alias(\"ld1\").join(periodicaldatesforextensionperiod_src_df.alias(\"pd\"),(col(\"pd.closing\") > col(\"ld1.fromperiod\")) & (col(\"pd.closing\") <= col(\"ld1.LegacyPeriod\").substr(1, 6))\n",
    "                        & (col(\"pd.Order\") == col(\"ld1.ftpperiod\")) & (col(\"pd.Year\") == col(\"FinancialYear\"))).filter((col(\"financialcycleid\") == FiscalPeriodID) & (col(\"Entityid\") == EntityID)) \\\n",
    "                        .withColumn(\"FromOpeningPeriod\",concat(substring(col(\"FromPeriod\"), 1, 4),substring(to_date(concat(lit(\"01-\"),substring(\n",
    "                                col(\"pd.Period\"),\n",
    "                                instr(col(\"pd.Period\"), \" \") + 1,  # Starting index\n",
    "                                when(instr(col(\"pd.Period\"), \"-\") == 0, \n",
    "                                    length(col(\"pd.Period\")) - instr(col(\"pd.Period\"), \" \")\n",
    "                                    )\n",
    "                                .otherwise(instr(col(\"pd.Period\"), \"-\") - instr(col(\"pd.Period\"), \" \") - 1)\n",
    "                            ),lit(\"-\"),substring(col(\"pd.closing\"), 1, 4)),\"dd-MMM-yyyy\"),6,2))) \\\n",
    "                        .select(\"ld1.PeriodType\",\"ld1.Period\",\"ld1.FinancialYear\",\"ld1.LegacyPeriod\",\"ld1.ftpperiod\"\n",
    "                            ,\"pd.Value\",\"pd.closing\",\"FromOpeningPeriod\")\n",
    "                        \n",
    "        \n",
    "        prioritylevel2temp = ld_df.withColumn(\"monthdiff\",\n",
    "        when(col(\"Value\").isin(\"H1\", \"Q1\", \"Year\", \"9M\"),\n",
    "            (\n",
    "        months_between(\n",
    "            to_date(\n",
    "            concat(\n",
    "                lit(\"01-\"), \n",
    "                col(\"closing\").substr(-2, 2), \n",
    "                lit(\"-\"), \n",
    "                col(\"closing\").substr(1, 4)\n",
    "            ), \n",
    "            \"dd-MM-yyyy\"\n",
    "        ), \n",
    "        to_date(\n",
    "            concat(\n",
    "                lit(\"01-\"), \n",
    "                col(\"FromOpeningPeriod\").substr(-2, 2), \n",
    "                lit(\"-\"), \n",
    "                col(\"FromOpeningPeriod\").substr(1, 4)\n",
    "            ), \n",
    "            \"dd-MM-yyyy\"\n",
    "        )\n",
    "        )+1\n",
    "        ).cast(\"int\"))\n",
    "        .when(~(col(\"Value\").isin(\"H1\", \"Q1\", \"Year\", \"9M\")),\n",
    "            when(col(\"Value\").substr(1, 1) == \"9\", \"9\")\n",
    "            .when(col(\"Value\").substr(1, 1) == \"H\", \"6\")\n",
    "            .when(col(\"Value\").substr(1, 1) == \"Q\", \"3\")\n",
    "            .when(col(\"Value\").substr(1, 1) == \"M\", \"1\")\n",
    "            .otherwise(None))\n",
    "        ).select(\"periodtype\",\"Period\",\"legacyperiod\",\"ftpperiod\",\"closing\",\"monthdiff\").distinct()\n",
    "       \n",
    "        prioritylevel2temp_df = trialbalance_dim_df.alias(\"tb\").join(prioritylevel2temp.alias(\"pt\"),(col(\"tb.Period\").substr(1,6) == col(\"pt.closing\")) & ((abs(col(\"tb.Period\").substr(-2, 2))) == col(\"pt.monthdiff\")),\"inner\").where( col(\"EntityID_fk\") == EntityID).select(\"tb.Period\",\n",
    "                                col(\"tb.Period\").substr(1,6).alias(\"stdperiod\"),\n",
    "                                \"periodtype\",\n",
    "                                col(\"tb.Period\").substr(1,6).alias(\"closing\"),\n",
    "                                \"legacyperiod\",\n",
    "                                col(\"ftpperiod\").alias(\"numericvalueofperiod\"),*alldimcols).distinct()\n",
    "        \n",
    "\n",
    "        window_spec = Window().partitionBy(\"stdperiod\", \"periodtype\").orderBy(col(\"Period\").desc())\n",
    "\n",
    "        prioritylevel2_del_df2 = prioritylevel2temp_df \\\n",
    "                            .withColumn(\"priorityseq\", row_number().over(window_spec)) \\\n",
    "                            .withColumn(\"order\", lit(0)) \\\n",
    "                            .select(\n",
    "                                col(\"Period\"),\n",
    "                                col(\"stdperiod\"),\n",
    "                                col(\"PeriodType\"),\n",
    "                                col(\"closing\"),\n",
    "                                col(\"legacyperiod\"),\n",
    "                                col(\"numericvalueofperiod\"),\n",
    "                                col(\"priorityseq\"),\n",
    "                                col(\"order\")\n",
    "                            ) \\\n",
    "                            .orderBy(\"periodtype\")\n",
    "        prioritylevel2_del_df = prioritylevel2_del_df.select(col(\"Period\"),\n",
    "                                col(\"stdperiod\"),\n",
    "                                col(\"PeriodType\"),\n",
    "                                col(\"closing\"),\n",
    "                                col(\"legacyperiod\"),\n",
    "                                col(\"numericvalueofperiod\"),\n",
    "                                col(\"priorityseq\"),\n",
    "                                col(\"order\"))\n",
    "        \n",
    "        prioritylevel2_inst_df = prioritylevel2_del_df.union(prioritylevel2_del_df2)                    \n",
    "        \n",
    "\n",
    "        priorityseq_final_df = priorityseq_updated_df.filter(col(\"priorityseq\") == 1)\n",
    "        \n",
    "        prioritylevel2_final_df = prioritylevel2_inst_df.filter(col(\"priorityseq\") == 1)\n",
    "        \n",
    "        # # #     ######################## For Next level priority check ##############################\n",
    "        \n",
    "        if_exists_df = (\n",
    "        priorityseq_final_df\n",
    "        .groupBy(\"PeriodType\")\n",
    "        .agg({\"Order\": \"max\"})\n",
    "        .withColumnRenamed(\"max(Order)\", \"order\")\n",
    "        .exceptAll(\n",
    "        priorityseq_final_df.alias(\"ps\")\n",
    "        .join(\n",
    "        legacysddates_num_df.alias(\"ld\"),\n",
    "        (col(\"ps.PeriodType\") == col(\"ld.PeriodType\")) &\n",
    "        (col(\"ps.LegacyPeriod\") == col(\"ld.LegacyPeriod\")) &\n",
    "        (col(\"ps.Order\") == col(\"ld.ftpperiod\") - 1),\n",
    "        \"inner\"\n",
    "        )\n",
    "        .select(\"ps.PeriodType\", \"ps.Order\")\n",
    "        ))\n",
    "        \n",
    "        if not if_exists_df.isEmpty():\n",
    "            print(\"inside if exists\")\n",
    "            \n",
    "            ctePS_df = (\n",
    "                priorityseq_final_df.groupBy(\"PeriodType\",\"LegacyPeriod\").agg(max(col(\"Order\")).alias(\"Order\")).select(\"PeriodType\",\"LegacyPeriod\",\"Order\")\n",
    "            )\n",
    "            \n",
    "            legacysddates_num1_df = legacysddates_num_df.alias(\"ld\").join(ctePS_df.alias(\"ps\"),\n",
    "                    (col(\"ps.periodtype\") == col(\"ld.PeriodType\")) &\n",
    "                    (col(\"ps.legacyperiod\") == col(\"ld.legacyperiod\")) & (col(\"ps.Order\") != col(\"ld.ftpperiod\")) ,\"left_outer\").select(\"ld.*\",col(\"ps.Order\").alias(\"ps_Order\"),col(\"ps.legacyperiod\").alias(\"ps_legacyperiod\"),col(\"ps.periodtype\").alias(\"ps_periodtype\")) \\\n",
    "                    .withColumn(\"ftpperiod_new\",when((col(\"ps_periodtype\") == col(\"PeriodType\")) &\n",
    "                    (col(\"ps_legacyperiod\") == col(\"legacyperiod\")) & (col(\"ps_Order\") != col(\"ftpperiod\")), col(\"ps_Order\") + 1).otherwise(col(\"ftpperiod\"))).drop(\"ps_Order\",\"ps_legacyperiod\",\"ps_periodtype\",\"ftpperiod\")\n",
    "            legacysddates_num1_df = legacysddates_num1_df.withColumnRenamed(\"ftpperiod_new\",\"ftpperiod\")       \n",
    "            legacysddates_num1_df.coalesce(1).write.format(\"parquet\").mode(\"overwrite\").save(adls_path+folder_path+\"legacysddates_num\")\n",
    "\n",
    "        else:\n",
    "            print(\"else not exists\")\n",
    "            legacysddates_num1_df = legacysddates_num_df\n",
    "            legacysddates_num1_df.coalesce(1).write.format(\"parquet\").mode(\"overwrite\").save(adls_path+folder_path+\"legacysddates_num\")\n",
    "        \n",
    "        legacysddates_num2_df = spark.read.parquet(adls_path+folder_path+\"legacysddates_num\",schema=legacysddates_schema)\n",
    "        deleteplev2_df = tblegacymatchperiod_updated_df.filter(col(\"Period\") == col(\"LegacyPeriod\")).select(\"LegacyPeriod\",\"PeriodType\")\n",
    "\n",
    "        prioritylevel2_final_del_df = prioritylevel2_final_df.alias(\"pl\").join(\n",
    "            deleteplev2_df.alias(\"tbm\"),\n",
    "            (col(\"tbm.PeriodType\") == col(\"pl.PeriodType\")) &\n",
    "            (col(\"tbm.LegacyPeriod\") == col(\"pl.LegacyPeriod\")),\n",
    "            \"left_anti\"\n",
    "        )\n",
    "        \n",
    "        prioritylevel2_final_del_df.coalesce(1).write.format(\"parquet\").mode(\"overwrite\").save(adls_path+folder_path+\"prioritylevel2_ext\")\n",
    "        prioritylevel2_final_del_df = spark.read.parquet(adls_path+folder_path+\"prioritylevel2_ext\", schema = prioritylevel2_ext_schema)\n",
    "\n",
    "        tblegacymatchperiod_del_df = (\n",
    "        tblegacymatchperiod_updated_df.alias(\"tbm\")\n",
    "        .join(\n",
    "        prioritylevel2_final_del_df.alias(\"pl\"),\n",
    "        (col(\"tbm.PeriodType\") == col(\"pl.PeriodType\")) &\n",
    "        (col(\"tbm.LegacyPeriod\") == col(\"pl.LegacyPeriod\")),\n",
    "        \"left_anti\"\n",
    "        )\n",
    "        )\n",
    "\n",
    "        priorityseq_final_del_df = (\n",
    "        priorityseq_final_df.alias(\"ps\")\n",
    "        .join(\n",
    "        legacysddates_num1_df.alias(\"ld\"),\n",
    "        (col(\"ps.PeriodType\") == col(\"ld.PeriodType\")) &\n",
    "        (col(\"ps.LegacyPeriod\") == col(\"ld.LegacyPeriod\")) &\n",
    "        (col(\"ps.order\") != (col(\"ld.ftpperiod\")- 1)),\n",
    "        \"left_anti\"\n",
    "        )\n",
    "        )\n",
    "\n",
    "        priorityseq_final_del_df.coalesce(1).write.format(\"parquet\").mode(\"overwrite\").save(adls_path+folder_path+\"priorityseq\")\n",
    "        priorityseq_final_del_df = spark.read.parquet(adls_path+folder_path+\"priorityseq\", schema = priorityseq_schema)\n",
    "\n",
    "        bspriority_tmp_df = (\n",
    "                        legacysddates_num1_df.alias(\"ld\").join(priorityseq_final_del_df.alias(\"ps\"), (col(\"ld.PeriodType\") == col(\"ps.PeriodType\")) & (col(\"ld.LegacyPeriod\") == col(\"ps.LegacyPeriod\"))).select(\"ld.PeriodType\",\"ld.legacyperiod\",\"ld.currentvalue\",\"ps.Period\",\"ps.stdperiod\") \\\n",
    "                        .union(\n",
    "                            legacysddates_num1_df.alias(\"ld\").join(prioritylevel2_final_del_df.alias(\"ps\"), (col(\"ld.PeriodType\") == col(\"ps.PeriodType\")) & (col(\"ld.LegacyPeriod\") == col(\"ps.LegacyPeriod\"))).select(\"ld.PeriodType\",\"ld.legacyperiod\",\"ld.currentvalue\",\"ps.Period\",\"ps.stdperiod\")))\n",
    "\n",
    "        tblegacymatchperiod_del_grouped_df =  tblegacymatchperiod_del_df.withColumn(\"currentvalue\",col(\"legacyperiod\").substr(1,6)).select(\"PeriodType\",\"legacyperiod\",\"currentvalue\",\"Period\",\"stdperiod\")\n",
    "\n",
    "\n",
    "        bspriority_df = bspriority_tmp_df.union(tblegacymatchperiod_del_grouped_df) \\\n",
    "                        .groupBy(\"PeriodType\", \"legacyperiod\", \"currentvalue\") \\\n",
    "                        .agg(F.max(\"Period\").alias(\"Period\"),\n",
    "                            F.max(\"stdperiod\").alias(\"stdperiod\")).select(\"PeriodType\",\"legacyperiod\",\"currentvalue\",\"Period\",\"stdperiod\")\n",
    "                        \n",
    "        bspriority_df.coalesce(1).write.format(\"parquet\").mode(\"overwrite\").save(adls_path+folder_path+\"bspriority\")\n",
    "        bspriority_df = spark.read.parquet(adls_path+folder_path+\"bspriority\")\n",
    "\n",
    "        legacysddates_updt_df = legacysddates_num1_df.alias(\"ld\") \\\n",
    "        .join(\n",
    "        bspriority_df.alias(\"ps\"),\n",
    "        (col(\"ld.PeriodType\") == col(\"ps.PeriodType\")) &\n",
    "        (col(\"ld.LegacyPeriod\") == col(\"ps.LegacyPeriod\")),\n",
    "        \"left_outer\").select(\"ld.*\",col(\"ps.Period\").alias(\"ps_Period\"),col(\"ps.stdperiod\").alias(\"ps_stdperiod\"),col(\"ps.PeriodType\").alias(\"ps_PeriodType\"),col(\"ps.LegacyPeriod\").alias(\"ps_LegacyPeriod\")).withColumn(\"legacyperiod\",when((col(\"PeriodType\") == col(\"ps_PeriodType\")) &\n",
    "        (col(\"LegacyPeriod\") == col(\"ps_LegacyPeriod\")),col(\"ps_Period\")).otherwise(col(\"legacyperiod\"))) \\\n",
    "        .withColumn(\"currentvalue\",when((col(\"PeriodType\") == col(\"ps_PeriodType\")) &\n",
    "        (col(\"LegacyPeriod\") == col(\"ps_LegacyPeriod\")),col(\"ps_stdperiod\")).otherwise(col(\"currentvalue\"))).drop(\"ps_Period\",\"ps_stdperiod\",\"ps_PeriodType\",\"ps_LegacyPeriod\")\n",
    "\n",
    "\n",
    "        legacysddates_updt_df.coalesce(1).write.format(\"parquet\").mode(\"overwrite\").save(adls_path+folder_path+\"legacysddates\") \n",
    "        legacysddates_updt_df = spark.read.parquet(adls_path+folder_path+\"legacysddates\", schema = legacysddates_schema)\n",
    "\n",
    "        ytddates_del_df = ytddates_df.alias(\"ys\").join(legacysddates_updt_df.alias(\"ld\"),(col(\"ys.periodtype\")==col(\"ld.periodtype\")) & (col(\"financialyear\") == extendedcycleyear),\"left_anti\")\n",
    "\n",
    "        # # #     #### ---delete from legacyperiod -----\n",
    "\n",
    "\n",
    "        tblegacymatchperiod_del_df1 = tblegacymatchperiod_del_df.filter(col(\"Period\").substr(1, 6) != col(\"LegacyPeriod\").substr(1, 6)).alias(\"a\").join(tblegacymatchperiod_del_df.alias(\"b\"),col(\"a.LegacyPeriod\") == col(\"b.Period\"),\"left_anti\")\n",
    "\n",
    "        tblegacymatchperiod_final_del_df = tblegacymatchperiod_del_df.alias(\"tb\").join(tblegacymatchperiod_del_df1.alias(\"del\"),col(\"tb.LegacyPeriod\") ==col(\"del.LegacyPeriod\"),\"left_anti\")\n",
    "        tblegacymatchperiod_final_del_df.coalesce(1).write.format(\"parquet\").mode(\"overwrite\").save(adls_path+folder_path+\"tblegacymatchperiod\")\n",
    "        tblegacymatchperiod_final_del_df = spark.read.parquet(adls_path+folder_path+\"tblegacymatchperiod\",schema=tblegacymatchperiod_schema)\n",
    "\n",
    "        print(\"count of comp\",countofcomp)\n",
    "        if countofcomp == 0 or countofcomp ==1:\n",
    "            print(\"inside countodcomp 0 or 1\")\n",
    "            ytddates_df1 = tblegacymatchperiod_final_del_df.filter(col(\"Periodtype\").like('%YTD') | col(\"Periodtype\").like('%Year')).select(\"PeriodTYpe\",col(\"legacyperiod\").substr(1,6).alias(\"CurrentValue\"),col(\"Period\").alias(\"FTPValue\")) \\\n",
    "            .union(priorityseq_final_del_df.alias(\"ps\").join(legacysddates_updt_df.alias(\"ld\"),col(\"ps.periodtype\") == col(\"ld.periodtype\")).filter((col(\"ld.Periodtype\").like('%YTD') | col(\"ld.Periodtype\").like('%Year')) & (col(\"financialyear\") == extendedcycleyear)).select(\"ld.PeriodTYpe\",col(\"ld.legacyperiod\").substr(1,6).alias(\"CurrentValue\"),col(\"Period\").alias(\"FTPValue\"))) \\\n",
    "            .union(prioritylevel2_final_del_df.alias(\"pl\").join(legacysddates_updt_df.alias(\"ld\"),col(\"pl.periodtype\") == col(\"ld.periodtype\")).filter((col(\"ld.Periodtype\").like('%YTD') | col(\"ld.Periodtype\").like('%Year')) & (col(\"financialyear\") == extendedcycleyear)).select(\"ld.PeriodTYpe\",col(\"pl.legacyperiod\").substr(1,6).alias(\"CurrentValue\"),col(\"Period\").alias(\"FTPValue\")))\n",
    "            ytddates_df = ytddates_del_df.union(ytddates_df1).distinct()\n",
    "            ytddates_df.coalesce(1).write.format(\"parquet\").mode(\"overwrite\").save(adls_path+folder_path+\"ytddates\") \n",
    "            \n",
    "\n",
    "        if countofcomp > 1:\n",
    "            print(\"count greater than 1\")\n",
    "            ytddates_df1 = (\n",
    "            legacysddates_updt_df.select(\"periodtype\",\"pcurrentvalue\",\"legacyperiod\")\n",
    "            .filter(~(col(\"periodtype\").isin(\"currentPeriod\", \"PreviousPeriod\")) & (col(\"financialyear\") == extendedcycleyear))\n",
    "            .exceptAll(\n",
    "            legacysddates_updt_df.filter((col(\"legacyperiod\").substr(1, 6) != col(\"pcurrentvalue\")) & (col(\"financialyear\") == extendedcycleyear)).alias(\"a\")\n",
    "            .join(\n",
    "                tblegacymatchperiod_final_del_df.alias(\"b\"),    \n",
    "                (col(\"a.legacyperiod\") == col(\"b.Period\")) &\n",
    "                (col(\"a.periodtype\") == col(\"b.PeriodType\")),\n",
    "                \"left_anti\"\n",
    "            )\n",
    "            .select(\"a.periodtype\", \"a.pcurrentvalue\", \"a.legacyperiod\")\n",
    "            )\n",
    "            .union(\n",
    "                priorityseq_final_del_df\n",
    "                .select(\"PeriodType\", col(\"legacyperiod\").substr(1, 6).alias(\"legacyperiod\"), \"Period\")\n",
    "            )\n",
    "            .union(\n",
    "                prioritylevel2_final_del_df\n",
    "                .select(\"PeriodType\", col(\"legacyperiod\").substr(1, 6).alias(\"legacyperiod\"), \"Period\")\n",
    "            )\n",
    "            .union(\n",
    "            tblegacymatchperiod_final_del_df\n",
    "            .select(\"PeriodType\", col(\"LegacyPeriod\").substr(1, 6).alias(\"pcurrentvalue\"), \"Period\")\n",
    "            .exceptAll(\n",
    "                legacysddates_updt_df.filter(col(\"financialyear\") ==  extendedcycleyear)\n",
    "                .select(\"PeriodType\", \"pcurrentvalue\", \"legacyperiod\"))\n",
    "                .select(\"PeriodType\", col(\"pcurrentvalue\"), \"Period\")\n",
    "            \n",
    "        )\n",
    "            )\n",
    "        print(\"ytddates table result final\")\n",
    "        ytddates_df1 = ytddates_df1.withColumnRenamed(\"pcurrentvalue\",\"CurrentValue\") \\\n",
    "        .withColumnRenamed(\"legacyperiod\",\"FTPValue\")\n",
    "        ytddates_df = ytddates_del_df.union(ytddates_df1).distinct()\n",
    "        ytddates_df.coalesce(1).write.format(\"parquet\").mode(\"overwrite\").save(adls_path+folder_path+\"ytddates\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e18989a5-dc5a-4f8c-95d2-b078856c9ee1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "legacysddates_df = spark.read.parquet(adls_path+folder_path+\"legacysddates\",schema=legacysddates_schema)\n",
    "legacysddates_df.coalesce(1).write.format(\"parquet\").mode(\"overwrite\").save(adls_path+folder_path+\"legacysddates_final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9bdfafe5-f03d-44b8-9f2e-7a8371993c4f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "legacyadjustmentsdrilldown_src_uc_df = spark.read.table(catalog_schema_name+\".\"+\"legacyadjustmentsdrilldown\")\n",
    "legacysddates_final_df = spark.read.parquet(adls_path+folder_path+\"legacysddates_final\",schema=legacysddates_schema)\n",
    "for row in legacysddates_final_df.collect():\n",
    "    ID = row.id\n",
    "    LEGACYPERIOD = row.legacyperiod\n",
    "    CatYear = row.financialyear\n",
    "    result_category_df = categorydropdown_src_df.filter((col(\"categoryname\") == CategorySDName) & (col(\"accountingstandardid\") == AccountingStandardID) & (col(\"EntityId\") == EntityID) & (col(\"Year\") == CatYear)).select(\"categoryid\")\n",
    "    \n",
    "    if result_category_df.count() > 0:\n",
    "    # Get the first categoryid value from the DataFrame\n",
    "        category_value = result_category_df.first()[0]\n",
    "    else:\n",
    "    # Handle the case where no matching rows were found\n",
    "        category_value = 0\n",
    "    legacysddates_df_tmp = legacysddates_final_df.filter(col(\"id\") == row.id).withColumn(\"categoryid\",lit(category_value))\n",
    "\n",
    "    legacysddates_union_df = legacysddates_union_df.union(legacysddates_df_tmp)\n",
    "\n",
    "    row_values = (\n",
    "        legacysddates_union_df.filter(col(\"id\") == row.id)\n",
    "        .select(\"legacyperiod\", \"categoryid\", \"castingperiod\")\n",
    "        .first()\n",
    "    )\n",
    "    print (row_values)\n",
    "    LCurrperiod, LCategoryID, CastingPeriod = row_values\n",
    "\n",
    "    legacyadjustmentsdrilldown_df = get_legacy_adjustment_by_category_drilldown(legacyadjustments_dim_df,legacyadjustmentsdrilldown_union_df,AccountingStandardID,EntityID,LCurrperiod,LCategoryID)\n",
    "    legacyadjustmentsdrilldown_union_df = legacyadjustmentsdrilldown_union_df.union(legacyadjustmentsdrilldown_df).distinct()\n",
    "\n",
    "    new_castingsdadjustment_df = castingadjustment_dim_df.filter(\n",
    "                (col(\"categoryid\") == int(LCategoryID)) &\n",
    "                (col(\"fk_entityid\") == EntityID) &\n",
    "                (col(\"fk_accountingstandardid\") == AccountingStandardID) &\n",
    "                (col(\"roundoff\") == Roundoff) &\n",
    "                (col(\"amountsin\") == AmountsIn) &\n",
    "                (col(\"periodvalue\") == CastingPeriod) &\n",
    "                (col(\"roundtypelevel\") == RoundTypeLevel)).select(\"CastingAdjustmentsID\",\"Fk_AccountingStandardID\",\"Fk_ValidationID\",\"PeriodValue\",\"FinancialYear\",\"Fk_EntityID\",\"EntityName\",\"JournalNumber\",\"JournalDate\",\"UserGLCode\",\"UserGLDescription\",\"StandardGLCode\",\"StandardGLDescription\",\"DebitCredit\",\"Fk_CurrencyID\",\"Amount\",\"JournalType\",\"Narration\",\"Fk_BusinessUnitID\",\"CategoryID\",\"Roundoff\",\"AmountsIn\",\"ReportType\",\"CreatedDate\",\"CreatedBy\",\"ModifiedDate\",\"ModifiedBy\",\"IsUserGLCode\",\"RoundTypeLevel\",*alldimcols)\n",
    "    castingsdadjustment_df = castingsdadjustment_df.union(new_castingsdadjustment_df).distinct()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8514016f-92bf-4a88-acce-517000df6406",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "legacysddates_union_df.coalesce(1).write.format(\"parquet\").mode(\"overwrite\").save(adls_path+folder_path+\"legacysddates_final\")\n",
    "legacyadjustmentsdrilldown_union_df.write.mode(\"overwrite\").saveAsTable(\"dtdf_assu_faas_uc.frh.legacyadjustmentsdrilldown\")\n",
    "castingsdadjustment_df.coalesce(1).write.format(\"parquet\").mode(\"overwrite\").save(adls_path+folder_path+\"castingsdadjustment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b81fcf12-5cec-4cbd-bb86-7d06fd48135d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "castingsdadjustment_df = spark.read.parquet(adls_path+folder_path+\"castingsdadjustment\",schema=castingsdadjustment_schema)\n",
    "legacysddates_final_df = spark.read.parquet(adls_path+folder_path+\"legacysddates_final\",schema=legacysddates_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1786825d-c15a-4a13-8ce6-3b3bf8eb7496",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# delete duplicate entries for Legacy adj and casting adj\n",
    "legacyadjustmentsdrilldown_df = spark.read.table(catalog_schema_name+\".\"+\"legacyadjustmentsdrilldown\")\n",
    "window_spec2 = Window.partitionBy(\"LegacyAdjustmentID\").orderBy(\"LegacyAdjustmentID\")\n",
    "\n",
    "legacyadjustmentsdrilldown_df = legacyadjustmentsdrilldown_df.withColumn(\"row_number\", row_number().over(window_spec2))\n",
    "legacyadjustmentsdrilldown_df = legacyadjustmentsdrilldown_df.filter(legacyadjustmentsdrilldown_df.row_number == 1).drop(\"row_number\")\n",
    "\n",
    "\n",
    "ladjjfinal_df = legacyadjustmentsdrilldown_df.withColumn(\"convertedperiod\", F.lit('')).withColumn(\"periodtype\", F.lit(''))\n",
    "\n",
    "ladjjfinal_updated1_df =  ladjjfinal_df.alias(\"la\").join(legacysddates_union_df.alias(\"ld\"),\n",
    "    col(\"la.categoryid\") == col(\"ld.categoryid\"),\n",
    "    \"left_outer\"\n",
    ").select(\"la.*\",col(\"ld.periodtype\").alias(\"ld_PeriodType\"),col(\"ld.currentvalue\").alias(\"ld_currentvalue\"),col(\"ld.categoryid\").alias(\"ld_categoryid\")) \\\n",
    "\n",
    "\n",
    "ladjjfinal_updated_df = ladjjfinal_updated1_df.withColumn(\"convertedperiod\",when(col(\"categoryid\")==col(\"ld_categoryid\"),col(\"ld_currentvalue\")).otherwise(col(\"ld_currentvalue\"))) \\\n",
    ".withColumn(\"PeriodType\",when(col(\"categoryid\")==col(\"ld_categoryid\"),col(\"ld_PeriodType\")).otherwise(col(\"la.periodtype\")))\\\n",
    ".drop(\"ld_PeriodType\",\"ld_currentvalue\",\"ld_categoryid\")\n",
    "\n",
    "window_spec3 = Window.partitionBy(\"CastingAdjustmentsID\").orderBy(\"CastingAdjustmentsID\")\n",
    "\n",
    "castingsdadjustment_df = castingsdadjustment_df.withColumn(\"row_number\", row_number().over(window_spec3))\n",
    "castingsdadjustment_df = castingsdadjustment_df.filter(castingsdadjustment_df.row_number == 1).drop(\"row_number\")\n",
    "\n",
    "# ---Date range data fetch------\n",
    "\n",
    "legacydateslite_df = (\n",
    "    legacysddates_final_df\n",
    "    .select(\"LegacyPeriod\", \"PeriodType\")\n",
    "    .distinct()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e05e29ff-1610-40d3-864c-71e030e2b00c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "tempcategory_df = categorydropdown_src_df.filter((col(\"categoryname\") == CategorySDName) & (col(\"EntityId\") == EntityID) & (col(\"AccountingStandardId\") == AccountingStandardID)).select(\"categoryid\")\n",
    "\n",
    "allperiodftpscoa_df = spark.read.parquet(adls_path+folder_path+\"allperiodftpscoa\", schema= allperiodftpscoa_schema)\n",
    "ytddates_df = spark.read.parquet(adls_path+folder_path+\"ytddates\", schema = ytddates_schema)\n",
    "\n",
    "#  --standard Period data fetch--\n",
    "\n",
    "tbf_df = (\n",
    "trialbalance_dim_df.alias(\"tb\")\n",
    ".join(legacydateslite_df.alias(\"ld\"),( col(\"tb.Period\") == col(\"ld.legacyperiod\")) & (col(\"EntityID_fk\") == EntityID) & (col(\"AccountingStdID_fk\") == AccountingStandardID))\n",
    ".select(col(\"ld.PeriodType\").alias(\"PeriodType\"), F.substring(\"tb.Period\", 1, 6).alias(\"Period\"), \"userglcode\", \"closingbalance\", col(\"CurrencyID_fk\").alias(\"currency\"),*alldimcols)\n",
    ")\n",
    "\n",
    "saf_df = (\n",
    "    standaloneadjustments_dim_df.alias(\"sd\").join(legacydateslite_df.alias(\"ld\"), col(\"sd.Period\") == \"ld.legacyperiod\") \\\n",
    "    .join(tempcategory_df.alias(\"tc\"), col(\"tc.categoryid\") == col(\"sd.CategoryID_fk\"))\n",
    "    .filter((col(\"EntityID_fk\") == EntityID) &\n",
    "            (col(\"AccountingStdID_fk\") == AccountingStandardID) &\n",
    "            (col(\"IsUserGLCodeAdjustment\")  == True))\n",
    "    .select(col(\"ld.PeriodType\").alias(\"PeriodType\"), F.substring(\"sd.Period\", 1, 6).alias(\"Period\"), \"userglcode\", col(\"amount\").alias(\"closingbalance\"), col(\"CurrencyID_fk\").alias(\"currency\"),*alldimcols)\n",
    ")\n",
    "\n",
    "ljf_df = (\n",
    "    ladjjfinal_updated_df\n",
    "    .filter(ladjjfinal_updated_df[\"ismappedglcode\"] == True)\n",
    "    .select(\"PeriodType\", F.substring(\"convertedperiod\", 1, 6).alias(\"Period\"), \"userglcode\",col(\"amount\").alias(\"closingbalance\"), col(\"CurrencyID_Fk\").alias(\"currency\"),*alldimcols)\n",
    ")\n",
    "\n",
    "# Union of the three DataFrames\n",
    "unioned_standard_df = tbf_df.union(saf_df).union(ljf_df).distinct()\n",
    "\n",
    "# Group by PeriodType, Period, userglcode, currency, and calculate the sum of closingbalance\n",
    "tbfinal_df = (\n",
    "    unioned_standard_df\n",
    "    .groupBy(\"PeriodType\", \"Period\", \"userglcode\", \"currency\",*alldimcols)\n",
    "    .agg(F.sum(\"closingbalance\").alias(\"closingbalance\"))\n",
    "    .select(\"PeriodType\",\"Period\",\"userglcode\",\"currency\",\"closingbalance\",*alldimcols).distinct()\n",
    ")\n",
    "tbfinal_df.coalesce(1).write.format(\"parquet\").mode(\"overwrite\").save(adls_path+folder_path+\"tbfinal\")\n",
    "# # # -----YTD -------------\n",
    "\n",
    "\n",
    "tbytd_df = (\n",
    "    trialbalance_dim_df.alias(\"tb\")\n",
    "    .join(ytddates_df.alias(\"yt\"), col(\"tb.Period\") == col(\"yt.FTPValue\"))\n",
    "    .filter(( col(\"EntityID_fk\") == EntityID) &\n",
    "            ( col(\"AccountingStdID_fk\") == AccountingStandardID))\n",
    "    .groupBy(\"PeriodType\", F.substring(\"Period\", 1, 6).alias(\"Period\"), \"userglcode\", col(\"CurrencyID_fk\").alias(\"currency\"),*alldimcols)\n",
    "    .agg(F.sum(\"closingbalance\").alias(\"closingbalance\"))\n",
    "    .select(\"PeriodType\", \"Period\", \"userglcode\", \"currency\", \"closingbalance\",*alldimcols)\n",
    ")\n",
    "\n",
    "saytd_df = standaloneadjustments_dim_df.alias(\"sd\") \\\n",
    "    .join(ytddates_df.alias(\"yt\"), col(\"sd.Period\") == col(\"yt.FTPValue\")) \\\n",
    "    .join(tempcategory_df.alias(\"tc\"), col(\"tc.categoryid\") == col(\"sd.CategoryID_fk\")) \\\n",
    "    .filter((col(\"EntityID_fk\") == EntityID) &\n",
    "            (col(\"AccountingStdID_fk\") == AccountingStandardID) &\n",
    "            (col(\"IsUserGLCodeAdjustment\")  == True)) \\\n",
    "    .groupBy(\"PeriodType\", F.substring(\"Period\", 1, 6).alias(\"Period\"), \"userglcode\", col(\"CurrencyID_fk\").alias(\"currency\"),*alldimcols) \\\n",
    "    .agg(F.sum(\"amount\").alias(\"closingbalance\")) \\\n",
    "    .select(\"PeriodType\", \"Period\", \"userglcode\", \"currency\", \"closingbalance\",*alldimcols)\n",
    "\n",
    "\n",
    "ljytd_df = (\n",
    "    ladjjfinal_updated_df\n",
    "    .filter(ladjjfinal_updated_df[\"ismappedglcode\"] == True)\n",
    "    .groupBy(\"PeriodType\", F.substring(\"convertedperiod\", 1, 6).alias(\"Period\"), \"userglcode\",col(\"CurrencyID_Fk\").alias(\"currency\"),*alldimcols)\n",
    "    .agg(F.sum(\"amount\").alias(\"closingbalance\"))\n",
    "    .select(\"PeriodType\", \"Period\", \"userglcode\", \"currency\", \"closingbalance\",*alldimcols)\n",
    ")\n",
    "\n",
    "# Union of the three DataFrames\n",
    "unioned_ytd_df = tbytd_df.union(saytd_df).union(ljytd_df).distinct()\n",
    "\n",
    "# Group by PeriodType, Period, userglcode, currency, and calculate the sum of closingbalance\n",
    "tbfinalytd_df = (\n",
    "    unioned_ytd_df\n",
    "    .groupBy(\"PeriodType\", \"Period\", \"userglcode\", \"currency\",*alldimcols)\n",
    "    .agg(F.sum(\"closingbalance\").alias(\"closingbalance\")).distinct()\n",
    ")\n",
    "tbfinalytd_df.coalesce(1).write.format(\"parquet\").mode(\"overwrite\").save(adls_path+folder_path+\"tbfinalytd\")\n",
    "#     #----select bulk upload disclsore data  if category not there else categorywise data-------------\n",
    "\n",
    "disclosures_df = standaloneadjustments_dim_df.alias(\"tg\").join(legacydateslite_df.alias(\"ld\"),col(\"tg.Period\") == col(\"ld.LegacyPeriod\")) \\\n",
    "    .join(allperiodftpscoa_df.alias(\"sc\"),(col(\"tg.Member\") == col(\"sc.Member\")) & (col(\"tg.AccountSubType\") == (col(\"sc.AccountSubType\")))) \\\n",
    "    .filter((col(\"EntityID_fk\") == EntityID) & (col(\"AccountingStdID_fk\") == AccountingStandardID)).drop(\"sc.column27\")\\\n",
    ".select(\n",
    "    F.col(\"PeriodType\"),\n",
    "    F.substring(F.col(\"Period\"), 1, 6).alias(\"Period\"),\n",
    "    F.col(\"EntityID_fk\").alias(\"entityid\"),\n",
    "    F.col(\"sc.ultimateparent\").alias(\"ultimateparent\"),\n",
    "    F.col(\"CategoryID_fk\").alias(\"categoryid\"),\n",
    "    F.col(\"tg.member\").alias(\"member\"),\n",
    "    F.col(\"accounttype\"),\n",
    "    F.col(\"amount\"),\n",
    "    F.col(\"CurrencyID_fk\").alias(\"currencyid\"),\n",
    "    F.lit(\"\").alias(\"column27\"),\n",
    "    F.col(\"tg.AccountSubType\").alias(\"AccountSubType\"),\n",
    "    *alldimcols\n",
    ").distinct()\n",
    "disclosures_df.coalesce(1).write.format(\"parquet\").mode(\"overwrite\").save(adls_path+folder_path+\"disclosures\")\n",
    "\n",
    "# # # ---YTD disclsoures---\n",
    "disclosuresytd_df = standaloneadjustments_dim_df.alias(\"tg\").join(allperiodftpscoa_df.alias(\"sc\"),(col(\"tg.Member\") == col(\"sc.Member\")) & (col(\"tg.AccountSubType\") == col(\"sc.AccountSubType\"))) \\\n",
    ".join(ytddates_df.alias(\"yt\"), col(\"tg.Period\") == col(\"yt.FTPValue\")).filter((col(\"EntityID_fk\") == EntityID) & (col(\"AccountingStdID_fk\") == AccountingStandardID)).drop(\"sc.column27\") \\\n",
    ".groupBy(\n",
    "    \"PeriodType\",\n",
    "    \"Period\",\n",
    "    \"EntityID_fk\",\n",
    "    \"sc.ultimateparent\",\n",
    "    \"CategoryID_fk\",\n",
    "    \"tg.member\",\n",
    "    \"accounttype\",\n",
    "    \"CurrencyID_fk\",\n",
    "    \"tg.AccountSubType\",\n",
    "    *alldimcols\n",
    ").agg(F.sum(col(\"amount\")).alias(\"amount\")).select(\n",
    "    \"PeriodType\",\n",
    "    F.substring(\"Period\", 1, 6).alias(\"Period\"),\n",
    "    col(\"EntityID_fk\").alias(\"entityid\"),\n",
    "    col(\"sc.ultimateparent\").alias(\"ultimateparent\"),\n",
    "    col(\"CategoryID_fk\").alias(\"categoryid\"),\n",
    "    col(\"tg.member\").alias(\"member\"),\n",
    "    \"accounttype\",\n",
    "    \"amount\",\n",
    "    col(\"CurrencyID_fk\").alias(\"currencyid\"),\n",
    "    F.lit(\"\").alias(\"column27\"),\n",
    "    col(\"tg.AccountSubType\").alias(\"AccountSubType\"),\n",
    "    *alldimcols\n",
    ").distinct()\n",
    "\n",
    "disclosuresytd_df.coalesce(1).write.format(\"parquet\").mode(\"overwrite\").save(adls_path+folder_path+\"disclosuresytd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3c5a44ac-2e11-4985-888b-8457daf288eb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "tbfinal_df = spark.read.parquet(adls_path+folder_path+\"tbfinal\",schema=tbfinal_schema)\n",
    "tbfinalytd_df = spark.read.parquet(adls_path+folder_path+\"tbfinalytd\",schema=tbfinalytd_schema)\n",
    "disclosures_df = spark.read.parquet(adls_path+folder_path+\"disclosures\",schema=disclosures_schema)\n",
    "disclosuresytd_df = spark.read.parquet(adls_path+folder_path+\"disclosuresytd\", schema=disclosuresytd_schema)\n",
    "\n",
    "# -----update all ftp child member of input file to ftp---------\n",
    "\n",
    "updatechildftp_tmp_df = disclosures_df.alias(\"d\").join(allperiodftpscoa_df.alias(\"sc\"), (col(\"d.member\") == col(\"sc.member\")) & (col(\"d.AccountSubType\") == col(\"sc.AccountSubType\"))).where(col(\"sc.column27\") == \"FTP\").select(\"sc.member\", \"sc.path\")\n",
    "\n",
    "updatechildftp_df = (\n",
    "    updatechildftp_tmp_df.alias(\"up\").join(allperiodftpscoa_df.alias(\"sc\"), F.expr(\"instr(sc.Path, up.Path) > 0\"))\n",
    "    .select(\"sc.member\", \"sc.Path\", F.lit(\"FTP\").alias(\"column27\"))\n",
    "    \n",
    ")\n",
    "\n",
    "disclosures_upt_df = disclosures_df.alias(\"d\").join(updatechildftp_df.alias(\"sc\"), col(\"d.member\") == col(\"sc.member\"), \"left_outer\").select(\"d.*\",col(\"sc.column27\").alias(\"sc_column27\"),col(\"sc.member\").alias(\"sc_member\")) \\\n",
    ".withColumn(\"column27\",when(col(\"sc_member\") == col(\"d.member\"),coalesce(col(\"sc_column27\"), lit(\"\"))).otherwise(col(\"d.column27\"))).drop(\"sc_column27\",\"sc_member\")\n",
    "\n",
    "# Left outer join and update #disclosuresytd DataFrame\n",
    "disclosuresytd_upt_df = disclosuresytd_df.alias(\"d\").join(updatechildftp_df.alias(\"sc\"),col(\"d.member\") == col(\"sc.member\"), \"left_outer\").select(\"d.*\",col(\"sc.column27\").alias(\"sc_column27\"),col(\"sc.member\").alias(\"sc_member\"))\\\n",
    ".withColumn(\"column27\",when(col(\"sc_member\") == col(\"d.member\"),coalesce(col(\"sc_column27\"),lit(\"\"))).otherwise(col(\"d.column27\"))).drop(\"sc_column27\",\"sc_member\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "665947b6-5ce3-4c70-afa3-bf43ee1ce8b0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# #     # ----drilldown-------\n",
    "tbdd_df = trialbalance_dim_df.alias(\"tb\").join(legacydateslite_df.alias(\"ld\"),(col(\"tb.Period\") == col(\"ld.LegacyPeriod\")),\"inner\").filter((col(\"EntityID_fk\") == EntityID) & (col(\"AccountingStdID_fk\") == AccountingStandardID)).select(col(\"ld.PeriodType\"),\n",
    "    F.substring(col(\"Period\"), 1, 6).alias(\"Period\"),\n",
    "    col(\"tb.userglcode\"),\n",
    "    col(\"tb.usergldescription\"),\n",
    "    col(\"closingbalance\"),\n",
    "    col(\"CurrencyID_fk\").alias(\"currency\"),\n",
    "    lit(\"TB\").alias(\"source\"),\n",
    "    *alldimcols)\n",
    "\n",
    "\n",
    "sddd_df = standaloneadjustments_dim_df.alias(\"sd\").join(legacydateslite_df.alias(\"ld\"),col(\"sd.Period\") == col(\"ld.LegacyPeriod\")).join(tempcategory_df.alias(\"tc\"), col(\"tc.categoryid\") == col(\"sd.CategoryID_fk\")).filter((col(\"EntityID_fk\") == EntityID) & (col(\"AccountingStdID_fk\") == AccountingStandardID) & (col(\"IsUserGLCodeAdjustment\") == True)\n",
    "    ).select(\n",
    "    col(\"PeriodType\"),\n",
    "F.substring(col(\"Period\"), 1, 6).alias(\"Period\"),\n",
    "col(\"sd.userglcode\"),\n",
    "F.lit(None).alias(\"UserGLDescription\"),\n",
    "col(\"amount\").alias(\"closingbalance\"),\n",
    "col(\"CurrencyID_fk\").alias(\"currency\"),\n",
    "lit(\"Adjustments\").alias(\"source\"),\n",
    "*alldimcols)\n",
    "\n",
    "\n",
    "ljdd_df = ladjjfinal_updated_df.filter(col(\"ismappedglcode\") == True).select(\n",
    "col(\"PeriodType\"),\n",
    "F.substring(col(\"convertedperiod\"), 1, 6).alias(\"Period\"),\n",
    "col(\"userglcode\"),\n",
    "F.lit(None).alias(\"UserGLDescription\"),\n",
    "col(\"amount\").alias(\"closingbalance\"),\n",
    "col(\"CurrencyID_Fk\").alias(\"currency\"),\n",
    "lit(\"Adjustments\").alias(\"source\"),\n",
    "*alldimcols)\n",
    "\n",
    "\n",
    "dd_union_df = tbdd_df.union(sddd_df).union(ljdd_df).distinct()\n",
    "\n",
    "tbfinaldd_df = dd_union_df.groupBy(\"PeriodType\", \"Period\", \"userglcode\", \"usergldescription\", \"currency\", \"source\",*alldimcols).agg(F.sum(col(\"closingbalance\")).alias(\"closingbalance\")).select(\"PeriodType\",\"Period\", \"userglcode\", \"usergldescription\", \"currency\",\"closingbalance\",\"source\",*alldimcols).distinct()\n",
    "\n",
    "tbfinaldd_df.coalesce(1).write.format(\"parquet\").mode(\"overwrite\").save(adls_path+folder_path+\"tbfinaldd\")\n",
    "#     #----YTD------------\n",
    "\n",
    "tbddytd_df = trialbalance_dim_df.alias(\"tb\").join(ytddates_df.alias(\"yt\"), col(\"tb.Period\") == col(\"yt.FTPValue\")).filter((col(\"EntityID_fk\") == EntityID) & (col(\"AccountingStdID_fk\") == AccountingStandardID)).select(\n",
    "    col(\"PeriodType\"),\n",
    "    F.substring(col(\"tb.Period\"), 1, 6).alias(\"Period\"),\n",
    "    col(\"userglcode\"),\n",
    "    col(\"usergldescription\"),\n",
    "    col(\"closingbalance\"),\n",
    "    col(\"CurrencyID_fk\").alias(\"currency\"),\n",
    "    lit(\"TB\").alias(\"source\"),\n",
    "    *alldimcols\n",
    ")\n",
    "\n",
    "\n",
    "sdddytd_df = standaloneadjustments_dim_df.alias(\"sd\").join(ytddates_df.alias(\"yt\"), col(\"sd.Period\") == col(\"yt.FTPValue\")).join(tempcategory_df.alias(\"tc\"), col(\"tc.categoryid\") == col(\"sd.CategoryID_fk\")).filter(\n",
    "    (col(\"EntityID_fk\") == EntityID) & (col(\"AccountingStdID_fk\") == AccountingStandardID) & (col(\"IsUserGLCodeAdjustment\") == True)\n",
    ").select(\n",
    "    col(\"PeriodType\"),\n",
    "    F.substring(col(\"sd.Period\"), 1, 6).alias(\"Period\"),\n",
    "    col(\"userglcode\"),\n",
    "    F.lit(None).alias(\"UserGLDescription\"),\n",
    "    col(\"amount\").alias(\"closingbalance\"),\n",
    "    col(\"CurrencyID_fk\").alias(\"currency\"),\n",
    "    lit(\"Adjustments\").alias(\"source\"),\n",
    "    *alldimcols\n",
    ")\n",
    "\n",
    "\n",
    "ljddytd_df = ladjjfinal_updated_df.filter(col(\"ismappedglcode\") == True).select(\n",
    "    col(\"PeriodType\"),\n",
    "    F.substring(col(\"convertedperiod\"), 1, 6).alias(\"Period\"),\n",
    "    col(\"userglcode\"),\n",
    "    F.lit(None).alias(\"UserGLDescription\"),\n",
    "    col(\"amount\").alias(\"closingbalance\"),\n",
    "    col(\"CurrencyID_Fk\").alias(\"currency\"),\n",
    "    lit(\"Adjustments\").alias(\"source\"),\n",
    "    *alldimcols\n",
    ")\n",
    "\n",
    "tbfinalddytd_df = tbddytd_df.union(sdddytd_df).union(ljddytd_df).groupBy(\"PeriodType\", \"Period\", \"userglcode\", \"usergldescription\", \"currency\", \"source\",*alldimcols).agg(F.sum(col(\"closingbalance\")).alias(\"closingbalance\")).select(\"PeriodType\", \"Period\", \"userglcode\", \"usergldescription\", \"currency\",\"closingbalance\",\"source\",*alldimcols).distinct()\n",
    "\n",
    "tbfinalddytd_df.coalesce(1).write.format(\"parquet\").mode(\"overwrite\").save(adls_path+folder_path+\"tbfinalddytd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ef67c30b-1585-4aeb-8629-1080c9740cf3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "tbfinaldd_df  = spark.read.parquet(adls_path+folder_path+\"tbfinaldd\",schema=tbfinaldd_schema)\n",
    "tbfinalddytd_df  = spark.read.parquet(adls_path+folder_path+\"tbfinalddytd\",schema=tbfinalddytd_schema)\n",
    "\n",
    "# ----------------------standalone FTP & YTD data processing--------------------\n",
    "SCOAMappingTable_df  = spark.read.parquet(adls_path+folder_path+\"SCOAMappingTable\",schema=SCOAMappingTable_schema)\n",
    "\n",
    "tbfinal_updated_df = tbfinal_df.alias('mb').join(\n",
    "    legacysddates_final_df.alias('ld'),\n",
    "    (col('mb.period') == col('ld.currentvalue')) & (col('mb.PeriodType') == col('ld.periodtype')),\n",
    "    'left_outer'\n",
    ").select(\"mb.*\",col(\"ld.pcurrentvalue\").alias(\"ld_pcurrentvalue\"),col(\"ld.currentvalue\").alias(\"ld_currentvalue\"),col(\"ld.periodtype\").alias(\"ld_periodtype\")) \\\n",
    ".withColumn(\n",
    "    'Period',when((col('period') == col('ld_currentvalue')) & (col('PeriodType') == col('ld_periodtype')),col('ld_pcurrentvalue')).otherwise(col(\"period\"))\n",
    ").drop(\"ld_currentvalue\",\"ld_periodtype\",\"ld_pcurrentvalue\")\n",
    "\n",
    "tbfinalytd_updated_df = tbfinalytd_df.alias('mb').join(\n",
    "    ytddates_df.alias('yt'),\n",
    "    (col('mb.period') == col('yt.ftpvalue').substr(1, 6)) & (col('mb.PeriodType') == col('yt.periodtype')),\n",
    "    'left_outer'\n",
    ").select(\"mb.*\",col(\"yt.currentvalue\").alias(\"yt_currentvalue\"),col(\"yt.ftpvalue\").alias(\"yt_ftpvalue\"),col(\"yt.periodtype\").alias(\"yt_periodtype\")) \\\n",
    "    .withColumn(\n",
    "    'Period',when((col('period') == col('yt_ftpvalue').substr(1, 6)) & (col('PeriodType') == col('yt_periodtype')),col('yt_currentvalue')).otherwise(col(\"period\"))) \\\n",
    "    .drop(\"yt_currentvalue\",\"yt_periodtype\",\"yt_ftpvalue\")\n",
    "\n",
    "tbfinaldd_updated_df = tbfinaldd_df.alias('mb').join(\n",
    "    legacysddates_final_df.alias('ld'),\n",
    "    (col('mb.period') == col('ld.currentvalue')) & (col('mb.PeriodType') == col('ld.periodtype')),\n",
    "    'left_outer'\n",
    ").select(\"mb.*\",col(\"ld.pcurrentvalue\").alias(\"ld_pcurrentvalue\"),col('ld.currentvalue').alias(\"ld_currentvalue\"),col('ld.periodtype').alias(\"ld_periodtype\")) \\\n",
    "    .withColumn(\n",
    "    'Period',when((col('period') == col('ld_currentvalue')) & (col('PeriodType') == col('ld_periodtype')),col('ld_pcurrentvalue')).otherwise(col(\"period\"))) \\\n",
    ".drop(\"ld_pcurrentvalue\",\"ld_currentvalue\",\"ld_periodtype\")\n",
    "\n",
    "tbfinalddytd_updated_df = tbfinalddytd_df.alias('mb').join(\n",
    "    ytddates_df.alias('yt'),\n",
    "    (col('mb.period') == col('yt.ftpvalue').substr(1, 6)) & (col('mb.PeriodType') == col('yt.periodtype')),\n",
    "    'left_outer'\n",
    ").select(\"mb.*\",col(\"yt.currentvalue\").alias(\"yt_currentvalue\"),col('yt.ftpvalue').alias(\"yt_ftpvalue\"),col('yt.periodtype').alias(\"yt_periodtype\")) \\\n",
    ".withColumn(\n",
    "    'Period',when((col('period') == col('yt_ftpvalue').substr(1, 6)) & (col('PeriodType') == col('yt_periodtype')),col('yt_currentvalue')).otherwise(col(\"period\"))) \\\n",
    ".drop(\"yt_currentvalue\",\"yt_ftpvalue\",\"yt_periodtype\")\n",
    "\n",
    "disclosures_updated_df = disclosures_upt_df.alias('mb').join(\n",
    "   legacysddates_final_df.alias('ld'),\n",
    "    (col('mb.period') == col('ld.currentvalue')) & (col('mb.PeriodType') == col('ld.periodtype')),\n",
    "    'left_outer'\n",
    ").select(\"mb.*\",col(\"ld.pcurrentvalue\").alias(\"ld_pcurrentvalue\"),col('ld.currentvalue').alias(\"ld_currentvalue\"),col('ld.periodtype').alias(\"ld_periodtype\")) \\\n",
    "    .withColumn(\n",
    "    'Period',when((col('period') == col('ld_currentvalue')) & (col('PeriodType') == col('ld_periodtype')),col('ld_pcurrentvalue')).otherwise(col(\"period\"))) \\\n",
    ".drop(\"ld_pcurrentvalue\",\"ld_currentvalue\",\"ld_periodtype\")\n",
    "\n",
    "disclosuresytd_updated_df = disclosuresytd_upt_df.alias('mb').join(\n",
    "    ytddates_df.alias('yt'),\n",
    "    (col('mb.period') == col('yt.ftpvalue').substr(1, 6)) & (col('mb.PeriodType') == col('yt.periodtype')),\n",
    "    'left_outer'\n",
    ").select(\"mb.*\",col(\"yt.currentvalue\").alias(\"yt_currentvalue\"),col('yt.ftpvalue').alias(\"yt_ftpvalue\"),col('yt.periodtype').alias(\"yt_periodtype\"))\\\n",
    "    .withColumn(\n",
    "    'Period',when((col('period') == col('yt_ftpvalue').substr(1, 6)) & (col('PeriodType') == col('yt_periodtype')),col('yt_currentvalue')).otherwise(col(\"period\"))) \\\n",
    ".drop(\"yt_currentvalue\",\"yt_ftpvalue\",\"yt_periodtype\")\n",
    "\n",
    "legacydatesperiodtypelite_df = legacysddates_final_df.filter(\n",
    "    (col('periodtype').like('%YTD')) | (col('periodtype').like('%Year'))\n",
    ").select(\n",
    "    'pcurrentvalue',\n",
    "    'periodtype'\n",
    ").distinct()\n",
    "\n",
    "legacydatesperiodtypelite_df.coalesce(1).write.format(\"parquet\").mode(\"overwrite\").save(adls_path+folder_path+\"legacydatesperiodtypelite\")\n",
    "\n",
    "legacydatesnotperiodtypelite_df = legacysddates_final_df.select(\"pcurrentvalue\", \"PeriodType\") \\\n",
    "    .distinct() \\\n",
    "    .filter(~col(\"PeriodType\").like(\"%YTD\")) \\\n",
    "    .filter(~col(\"PeriodType\").like(\"%Year\"))\n",
    "\n",
    "legacydatesnotperiodtypelite_df.coalesce(1).write.format(\"parquet\").mode(\"overwrite\").save(adls_path+folder_path+\"legacydatesnotperiodtypelite\")\n",
    "\n",
    "# ----------YTD-------------\n",
    "\n",
    "mapped_sdt_by_td_tbfinalytd_df = tbfinalytd_updated_df.alias(\"tb\").join(SCOAMappingTable_df.alias(\"mt\"),\n",
    "    col(\"tb.UserGLCode\") == col(\"mt.UserGLCode\"),\n",
    "    \"inner\"\n",
    ").join(legacydatesperiodtypelite_df.alias(\"ld\"),(col(\"tb.Period\") == col(\"ld.pcurrentvalue\")) & (col(\"tb.PeriodType\") == col(\"ld.PeriodType\")),\n",
    "    \"inner\"\n",
    ").filter(col(\"UltimateParent\") == \"IS_GROUP\").select(\"tb.PeriodType\",\"tb.Period\",\"tb.UserGLCode\",\"tb.Currency\",\"ClosingBalance\",\"CategoryID\",col(\"mt.eyglcode\").alias(\"Member\"),lit(\"Y\").alias(\"ytdtype\"),*alldimcols) \n",
    "\n",
    "mapped_sdt_by_td_tbfinal_df = tbfinal_updated_df.alias(\"tb\").join(\n",
    "    SCOAMappingTable_df.alias(\"mt\"),\n",
    "    col(\"tb.UserGLCode\") == col(\"mt.UserGLCode\"),\n",
    "    \"inner\"\n",
    ").join(\n",
    "    legacydatesperiodtypelite_df.alias(\"ld\"),\n",
    "    (col(\"tb.Period\") == col(\"ld.pcurrentvalue\")) &\n",
    "    (col(\"tb.PeriodType\") == col(\"ld.PeriodType\")),\n",
    "    \"inner\"\n",
    ").filter(col(\"UltimateParent\") == \"BS_GROUP\").select(\"tb.PeriodType\", \"tb.Period\", \"tb.UserGLCode\", \"tb.Currency\", \"ClosingBalance\", \"CategoryID\",col(\"mt.eyglcode\").alias(\"Member\"),lit(\"Y\").alias(\"ytdtype\"),*alldimcols)\n",
    " \n",
    "ytd_tbfinal_tbfinalytd = mapped_sdt_by_td_tbfinalytd_df.union(mapped_sdt_by_td_tbfinal_df)\n",
    "\n",
    "ytd_tbfinal_tbfinalytd_grp = ytd_tbfinal_tbfinalytd.groupBy(\n",
    "    \"PeriodType\", \"Period\", \"UserGLCode\", \"Currency\", \"CategoryID\", \"member\",*alldimcols\n",
    ").agg(\n",
    "    F.sum(\"ClosingBalance\").alias(\"ClosingBalance\"),\n",
    ").select(\n",
    "    \"PeriodType\", \"Period\", \"UserGLCode\", \"Currency\", \"ClosingBalance\", \"CategoryID\", \"Member\", lit(\"Y\").alias(\"ytdtype\"),*alldimcols\n",
    ")\n",
    "\n",
    "mapped_sdt_by_td_disclosuresytd_df = disclosuresytd_updated_df.filter(\n",
    "    (~(col(\"UltimateParent\").isin(\"BS_GROUP\", \"IS_GROUP\"))) &\n",
    "    (col(\"Column27\") == \"FTP\")) \\\n",
    ".groupBy(\n",
    "    \"PeriodType\", \"Period\", \"currencyid\", \"CategoryID\", \"Member\", \"UltimateParent\", \"Column27\",*alldimcols\n",
    ").agg(\n",
    "    F.sum(\"Amount\").alias(\"ClosingBalance\"),\n",
    ").select(\n",
    "    \"PeriodType\", \"Period\", F.lit(None).alias(\"UserGLCode\"), col(\"currencyid\").alias(\"Currency\"), \"ClosingBalance\",\"CategoryID\", \"Member\", lit(\"Y\").alias(\"ytdtype\"),*alldimcols\n",
    ")\n",
    "\n",
    "mapped_sdt_by_td_disclosures_df = disclosures_updated_df.alias(\"disc\").join(\n",
    "    legacydatesperiodtypelite_df.alias(\"ld\"),\n",
    "    (col(\"disc.Period\") == col(\"ld.pcurrentvalue\")) &\n",
    "    (col(\"disc.PeriodType\") == col(\"ld.PeriodType\")),\n",
    "    \"inner\"\n",
    ").filter(\n",
    "    (~col(\"UltimateParent\").isin(\"BS_GROUP\", \"IS_GROUP\")) &\n",
    "    (col(\"Column27\") != \"FTP\")).groupBy(\n",
    "    \"disc.PeriodType\", \"Period\", \"Currencyid\", \"CategoryID\", \"Member\", \"UltimateParent\", \"Column27\",*alldimcols\n",
    ").agg(\n",
    "    F.sum(\"Amount\").alias(\"ClosingBalance\")\n",
    ").select(\n",
    "    \"PeriodType\", \"Period\", F.lit(None).alias(\"UserGLCode\"),col(\"Currencyid\").alias(\"Currency\"), \"ClosingBalance\", \"CategoryID\", \"Member\", lit(\"Y\").alias(\"ytdtype\"),*alldimcols\n",
    ")\n",
    "\n",
    "# Union all the DataFrames\n",
    "mappedsdtbytd_df = ytd_tbfinal_tbfinalytd_grp.union(mapped_sdt_by_td_disclosuresytd_df).union(mapped_sdt_by_td_disclosures_df).distinct()\n",
    "mappedsdtbytd_df.coalesce(1).write.format(\"parquet\").mode(\"overwrite\").save(adls_path+folder_path+\"mappedsdtbytd\")\n",
    "\n",
    "# # # # --------------FTP--------\n",
    "\n",
    "mapped_sdt_by_ftp_tbfinal_df = tbfinal_updated_df.alias(\"tb\").join(\n",
    "    SCOAMappingTable_df.alias(\"mt\"),\n",
    "    col(\"tb.UserGLCode\") == col(\"mt.UserGLCode\"),\n",
    "    \"inner\"\n",
    ").join(\n",
    "    legacydatesnotperiodtypelite_df.alias(\"ld\"),\n",
    "    (col(\"tb.Period\") == col(\"ld.pcurrentvalue\")) &\n",
    "    (col(\"tb.PeriodType\") == col(\"ld.PeriodType\")),\n",
    "    \"inner\"\n",
    ").select(\n",
    "        \"tb.PeriodType\",\n",
    "        col(\"tb.Period\").alias(\"Period\"),\n",
    "        col(\"tb.UserGLCode\").alias(\"UserGLCode\"),\n",
    "        col(\"tb.Currency\"),\n",
    "        col(\"ClosingBalance\"),\n",
    "        col(\"mt.categoryid\").alias(\"categoryid\"),\n",
    "        col(\"mt.eyglcode\").alias(\"member\"),\n",
    "        lit('N').alias(\"ytdtype\"),\n",
    "        *alldimcols\n",
    "    )\n",
    "\n",
    "\n",
    "mapped_sdt_by_ftp_disclosure_df =  disclosures_updated_df.alias(\"d\").join(legacydatesnotperiodtypelite_df.alias(\"ld\"),(col(\"d.Period\") == col(\"ld.pcurrentvalue\")) & (col(\"d.PeriodType\") == col(\"ld.PeriodType\")),\n",
    "        \"inner\"\n",
    "    ).select(\n",
    "        col(\"d.PeriodType\").alias(\"PeriodType\"),\n",
    "        col(\"Period\"),\n",
    "        F.lit(None).alias(\"UserGLCode\"),\n",
    "        col(\"currencyid\").alias(\"currency\"),\n",
    "        col(\"Amount\").alias(\"ClosingBalance\"),\n",
    "        col(\"CategoryID\"),\n",
    "        col(\"Member\"),\n",
    "        lit('N').alias(\"ytdtype\"),\n",
    "        *alldimcols\n",
    "    )\n",
    "legacysddates_final_mapped_df = legacysddates_final_df.filter(~((col(\"PeriodType\").like('%YTD')) | (col(\"PeriodType\").like('%Year')))).select(\"LegacyPeriod\", \"PeriodType\").distinct()\n",
    "mapped_sdt_by_ftp_adj_df = standaloneadjustments_dim_df.alias(\"sd\").join(\n",
    "        legacysddates_final_mapped_df.alias(\"ld\"),\n",
    "        col(\"sd.Period\") == col(\"ld.LegacyPeriod\"),\n",
    "        \"inner\"\n",
    "    ).join(\n",
    "        tempcategory_df.alias(\"tc\"),\n",
    "        col(\"sd.CategoryID_fk\") == col(\"tc.CategoryID\"),\n",
    "        \"inner\"\n",
    "    ).filter(\n",
    "        (col(\"EntityID_fk\") == EntityID) &\n",
    "        (col(\"AccountingStdID_fk\") == AccountingStandardID) &\n",
    "        (col(\"IsUserGLCodeAdjustment\") == True)\n",
    "    ).select(\n",
    "        col(\"ld.PeriodType\"),\n",
    "        col(\"Period\").substr(1, 6).alias(\"Period\"),\n",
    "        col(\"sd.UserGLCode\"),\n",
    "        col(\"CurrencyID_fk\").alias(\"CurrencyID\"),\n",
    "        col(\"Amount\").alias(\"ClosingBalance\"),\n",
    "        col(\"tc.CategoryID\"),\n",
    "        col(\"Member\"),\n",
    "        lit('N').alias(\"ytdtype\"),\n",
    "        *alldimcols\n",
    "    )\n",
    "mapped_sdt_by_ftp_ladj_df = ladjjfinal_updated_df.filter(\n",
    "        ladjjfinal_updated_df.IsMappedGLCode == True\n",
    "    ).select(\n",
    "        \"PeriodType\",\n",
    "        col(\"ConvertedPeriod\").substr(1, 6).alias(\"Period\"),\n",
    "        F.lit(None).alias(\"UserGLCode\"),\n",
    "        col(\"CurrencyID_Fk\").alias(\"CurrencyID\"),\n",
    "        col(\"Amount\").alias(\"ClosingBalance\"),\n",
    "        col(\"CategoryID\"),\n",
    "        col(\"Member\"),\n",
    "        lit('N').alias(\"ytdtype\"),\n",
    "        *alldimcols\n",
    "    )\n",
    "\n",
    "mappedsdtbftp_df = mapped_sdt_by_ftp_tbfinal_df.union(mapped_sdt_by_ftp_disclosure_df).union(mapped_sdt_by_ftp_adj_df).union(mapped_sdt_by_ftp_ladj_df).distinct()\n",
    "\n",
    "mappedsdtbftp_df.coalesce(1).write.format(\"parquet\").mode(\"overwrite\").save(adls_path+folder_path+\"mappedsdtbftp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c4ec43f3-1fa0-4d7c-a809-7c484f305e12",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ----------------drilldown YTD & FTP data------------------------- \n",
    "\n",
    "  #    -----------------------YTD------------\n",
    "legacysddates_final_mapped1_df = legacysddates_final_df.filter((col(\"PeriodType\").like('%YTD')) | (col(\"PeriodType\").like('%Year'))).select(\"pcurrentvalue\",\"PeriodType\")\n",
    "mapped_tbfinalddytd_dd_df = tbfinalddytd_updated_df.alias(\"tb\").join(\n",
    "    SCOAMappingTable_df.alias(\"mt\"),\n",
    "    col(\"tb.UserGLCode\") == col(\"mt.UserGLCode\"),\n",
    "    \"inner\"\n",
    ").join(\n",
    "    legacysddates_final_mapped1_df.alias(\"ld\"), \\\n",
    "    (col(\"tb.Period\") == col(\"ld.PCurrentValue\")) &\n",
    "    (col(\"tb.PeriodType\") == col(\"ld.PeriodType\")),\n",
    "    \"inner\"\n",
    ").filter(col(\"UltimateParent\") == \"IS_GROUP\").groupBy(\n",
    "    \"tb.PeriodType\", \"tb.Period\", \"tb.UserGLCode\", \"tb.UserGLDescription\", \"mt.eyglcode\", \"tb.Source\",*alldimcols\n",
    ").agg(\n",
    "    F.sum(\"ClosingBalance\").alias(\"ClosingBalance\")\n",
    ").select(\"tb.PeriodType\",\n",
    "                 col(\"tb.Period\").alias(\"Period\"),\n",
    "                 col(\"userglcode\").alias(\"UserGLCode\"),\n",
    "                 col(\"UserGLDescription\").alias(\"UserGLDescription\"),\n",
    "                 \"closingbalance\",\n",
    "                 col(\"mt.eyglcode\").alias(\"member\"),\n",
    "                 \"tb.Source\",\n",
    "                 lit('Y').alias(\"ytdtype\"),\n",
    "                 *alldimcols).distinct()\n",
    "mapped_tbfinalddytd_dd_df = tbfinaldd_updated_df.alias(\"tb\").join(\n",
    "    SCOAMappingTable_df.alias(\"mt\"),\n",
    "    col(\"tb.UserGLCode\") == col(\"mt.UserGLCode\"),\n",
    "    \"inner\"\n",
    ").join(\n",
    "    legacysddates_final_mapped1_df.alias(\"ld\"), \\\n",
    "    (col(\"tb.Period\") == col(\"ld.PCurrentValue\"))&\n",
    "    (col(\"tb.PeriodType\") == col(\"ld.PeriodType\")),\n",
    "    \"inner\"\n",
    ").filter(col(\"UltimateParent\") == \"BS_GROUP\").groupBy(\n",
    "    \"tb.PeriodType\", \"tb.Period\", \"tb.UserGLCode\", \"tb.UserGLDescription\", \"mt.eyglcode\", \"tb.Source\",*alldimcols\n",
    ").agg(\n",
    "    F.sum(\"ClosingBalance\").alias(\"ClosingBalance\")\n",
    ").select(\"tb.PeriodType\",\n",
    "                 col(\"tb.Period\").alias(\"Period\"),\n",
    "                 col(\"tb.userglcode\").alias(\"UserGLCode\"),\n",
    "                 col(\"tb.UserGLDescription\").alias(\"UserGLDescription\"),\n",
    "                 \"closingbalance\",\n",
    "                 col(\"mt.eyglcode\").alias(\"member\"),\n",
    "                 \"tb.Source\",\n",
    "                 lit('Y').alias(\"ytdtype\"),\n",
    "                 *alldimcols).distinct()\n",
    "\n",
    "mapped_disclosuresytd_dd_df = disclosuresytd_updated_df.filter(~(col(\"UltimateParent\").isin(\"BS_GROUP\", \"IS_GROUP\")) &  (col(\"Column27\") == 'FTP')\n",
    ").groupBy(\"PeriodType\", \"Period\", \"Member\", \"ultimateparent\", \"Column27\",*alldimcols\n",
    ").agg(F.sum(\"Amount\").alias(\"ClosingBalance\")\n",
    ").select(\"PeriodType\",\n",
    "                 \"Period\",\n",
    "                 F.lit(None).alias(\"UserGLCode\"),\n",
    "                 F.lit(None).alias(\"UserGLDescription\"),\n",
    "                 \"closingbalance\",\n",
    "                 \"member\",\n",
    "                 lit(\"Disclosures\").alias(\"Source\"),\n",
    "                 lit('Y').alias(\"ytdtype\"),\n",
    "                 *alldimcols).distinct()\n",
    "mapped_disclosures_dd_df = disclosures_updated_df.alias(\"disc\").join(\n",
    "    legacysddates_final_mapped1_df.alias(\"ld\"),\n",
    "    (col(\"disc.Period\") == col(\"ld.pcurrentvalue\")) &\n",
    "    (col(\"disc.PeriodType\") == col(\"ld.PeriodType\")),\n",
    "    \"inner\"\n",
    ").filter((~(col(\"UltimateParent\").isin(\"BS_GROUP\", \"IS_GROUP\"))) & (col(\"Column27\") != 'FTP')\n",
    ").groupBy(\n",
    "    \"disc.PeriodType\", \"Period\", \"Member\", \"ultimateparent\", \"Column27\",*alldimcols\n",
    ").agg(\n",
    "    F.sum(\"Amount\").alias(\"ClosingBalance\")\n",
    ").select(\"disc.PeriodType\",\n",
    "                 \"Period\",\n",
    "                  F.lit(None).alias(\"UserGLCode\"),\n",
    "                 F.lit(None).alias(\"UserGLDescription\"),\n",
    "                 \"closingbalance\",\n",
    "                 \"member\",\n",
    "                lit(\"Disclosures\").alias(\"Source\"),\n",
    "                 lit('Y').alias(\"ytdtype\"),\n",
    "                 *alldimcols).distinct()\n",
    "\n",
    "mappedtbddytd_df =  mapped_tbfinalddytd_dd_df.union(mapped_tbfinalddytd_dd_df).union(mapped_disclosuresytd_dd_df).union(mapped_disclosures_dd_df).distinct()\n",
    "mappedtbddytd_df.coalesce(1).write.format(\"parquet\").mode(\"overwrite\").save(adls_path+folder_path+\"mappedtbddytd\")\n",
    "# ------------FTP-------\n",
    "\n",
    "mapped_tbfinaldd_dd_ftp_df = tbfinaldd_updated_df.alias(\"tb\").join(SCOAMappingTable_df.alias(\"mt\"),col(\"tb.UserGLCode\") == col(\"mt.UserGLCode\"),\"inner\").join( \\\n",
    "    legacydatesnotperiodtypelite_df.alias(\"ld\"),(col(\"tb.Period\") == col(\"ld.pcurrentvalue\")) & (col(\"tb.PeriodType\") == col(\"ld.PeriodType\")),\"inner\") \\\n",
    "    .select(\"tb.PeriodType\",\n",
    "                 col(\"tb.Period\").alias(\"Period\"),\n",
    "                 col(\"tb.UserGLCode\").alias(\"UserGLCode\"),\n",
    "                 col(\"tb.UserGLDescription\").alias(\"UserGLDescription\"),\n",
    "                 \"closingbalance\",\n",
    "                 col(\"mt.eyglcode\").alias(\"member\"),\n",
    "                 \"tb.Source\",\n",
    "                 lit(\"N\").alias(\"ytdtype\"),\n",
    "                 *alldimcols).distinct()\n",
    "mapped_disclosures_dd_ftp_df = disclosures_updated_df.alias(\"d\").join(legacydatesnotperiodtypelite_df.alias(\"ld\"),(col(\"d.Period\") == col(\"ld.pcurrentvalue\")) & (col(\"d.PeriodType\") == col(\"ld.PeriodType\")),\n",
    "    \"inner\").select(\"d.PeriodType\",\n",
    "                    \"Period\",\n",
    "                     F.lit(None).alias(\"UserGLCode\"),\n",
    "                     F.lit(None).alias(\"UserGLDescription\"),\n",
    "                    col(\"amount\").alias(\"closingbalance\"),\n",
    "                    \"member\",\n",
    "                    lit(\"Disclosures\").alias(\"Source\"),\n",
    "                    lit(\"N\").alias(\"ytdtype\"),\n",
    "                    *alldimcols).distinct()\n",
    "legacysddates_final_mapped2_df = legacysddates_final_df.filter(~((col(\"PeriodType\").like('%YTD')) | (col(\"PeriodType\").like('%Year')))).select(\"LegacyPeriod\", \"PeriodType\").distinct()\n",
    "mapped_adj_dd_ftp_df = standaloneadjustments_dim_df.alias(\"sd\").join(\n",
    "        legacysddates_final_mapped2_df.alias(\"ld\"),\n",
    "        col(\"ld.LegacyPeriod\") == col(\"sd.Period\"),\n",
    "        \"inner\"\n",
    "    ).join(\n",
    "        tempcategory_df.alias(\"tc\"),\n",
    "        col(\"sd.CategoryID_fk\") == col(\"tc.CategoryID\"),\n",
    "        \"inner\"\n",
    "    ).filter(\n",
    "        (col(\"EntityID_fk\") == EntityID) &\n",
    "        (col(\"AccountingStdID_fk\") == AccountingStandardID) &\n",
    "        (col(\"IsUserGLCodeAdjustment\") == True)\n",
    "    ).select(\n",
    "        \"PeriodType\",\n",
    "        col(\"Period\").substr(1, 6).alias(\"Period\"),\n",
    "        col(\"UserGLCode\"),\n",
    "        F.lit(None).alias(\"UserGLDescription\"),\n",
    "        col(\"Amount\").alias(\"ClosingBalance\"),\n",
    "        col(\"Member\"),\n",
    "        lit('Adjustments').alias(\"Source\"),\n",
    "        lit('N').alias(\"ytdtype\"),\n",
    "        *alldimcols \n",
    "    ).distinct()\n",
    "mapped_ldj_dd_ftp_df = ladjjfinal_updated_df.filter(\n",
    "        col(\"IsMappedGLCode\") == True\n",
    "    ).select(\n",
    "        \"PeriodType\",\n",
    "        col(\"ConvertedPeriod\").substr(1, 6).alias(\"Period\"),\n",
    "        F.lit(None).alias(\"UserGLCode\"),\n",
    "        F.lit(None).alias(\"UserGLDescription\"),\n",
    "        col(\"Amount\").alias(\"ClosingBalance\"),\n",
    "        col(\"Member\"),\n",
    "        lit('Adjustments').alias(\"Source\"),\n",
    "        lit('N').alias(\"ytdtype\"),\n",
    "        *alldimcols\n",
    "    ).distinct()\n",
    "mappedtbddftp_df = mapped_tbfinaldd_dd_ftp_df.union(mapped_disclosures_dd_ftp_df).union(mapped_adj_dd_ftp_df).union(mapped_ldj_dd_ftp_df).distinct()\n",
    "\n",
    "mappedtbddftp_df.coalesce(1).write.format(\"parquet\").mode(\"overwrite\").save(adls_path+folder_path+\"mappedtbddftp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9208bd78-45b5-42ed-9d96-2e753586650d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mappedsdtbytd_df = spark.read.parquet(adls_path+folder_path+\"mappedsdtbytd\",schema=mappedsdtbytd_schema)\n",
    "mappedsdtbftp_df = spark.read.parquet(adls_path+folder_path+\"mappedsdtbftp\",schema=mappedsdtbftp_schema)\n",
    "mappedtbddytd_df = spark.read.parquet(adls_path+folder_path+\"mappedtbddytd\",schema=mappedtbddytd_schema)\n",
    "mappedtbddftp_df = spark.read.parquet(adls_path+folder_path+\"mappedtbddftp\",schema=mappedtbddftp_schema)\n",
    "\n",
    "if PeriodValueInYHQM == 0:\n",
    "    mappedsdtbftp_df = mappedsdtbftp_df.withColumn(\"periodtype\", expr(\"REPLACE(periodtype, '_YTD', '')\"))\n",
    "\n",
    "    mappedsdtbytd_df = mappedsdtbytd_df.withColumn(\"periodtype\", expr(\"REPLACE(periodtype, '_YTD', '')\"))\n",
    "\n",
    "    mappedtbddftp_df = mappedtbddftp_df.withColumn(\"periodtype\", expr(\"REPLACE(periodtype, '_YTD', '')\"))\n",
    "\n",
    "    mappedtbddytd_df = mappedtbddytd_df.withColumn(\"periodtype\", expr(\"REPLACE(periodtype, '_YTD', '')\"))\n",
    "\n",
    "    legacysddates_final_df = legacysddates_final_df.withColumn(\"periodtype\", expr(\"REPLACE(periodtype, '_YTD', '')\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d991c3b1-cebe-4a3d-a061-2abb15cbe25a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ------------IS Period having prev period data--------\n",
    "legacysddates_final_nodata_df = legacysddates_final_df.filter(legacysddates_final_df[\"pcurrentvalue\"] != legacysddates_final_df[\"currentvalue\"]).select( \"periodtype\" ,\"legacyperiod\" )\n",
    "is_no_data_df = legacysddates_final_nodata_df.alias(\"lg\") \\\n",
    "    .join(ytddates_df.alias(\"yt\"), (col(\"yt.FTPValue\") == col(\"lg.legacyperiod\")) & (col(\"yt.PeriodType\") == col(\"lg.periodtype\")), \"left_outer\") \\\n",
    "    .filter(col(\"yt.PeriodType\").isNull()) \\\n",
    "    .select(\"lg.periodtype\",\"lg.legacyperiod\")\n",
    "\n",
    "mappedsdtbftp_del_tmp_df = mappedsdtbftp_df.alias(\"mf\").join(SCOAMappingTable_df.alias(\"sc\"),(col(\"mf.member\") == col(\"sc.EYGLCode\"))) \\\n",
    "    .join(is_no_data_df.alias(\"nd\"), col(\"mf.PeriodType\") == col(\"nd.periodtype\"),\"inner\") \\\n",
    "    .filter(col(\"sc.UltimateParent\") != 'IS_GROUP') \\\n",
    "    .select(\"mf.*\")\n",
    "mappedsdtbftp_deleted_df = mappedsdtbftp_df.exceptAll(mappedsdtbftp_del_tmp_df)\n",
    "\n",
    "mappedtbddftp_del_tmp_df = mappedtbddftp_df.alias(\"mf\").join(SCOAMappingTable_df.alias(\"sc\"),(col(\"mf.member\") == col(\"sc.EYGLCode\"))) \\\n",
    "     .join(is_no_data_df.alias(\"nd\"), col(\"mf.PeriodType\") == col(\"nd.periodtype\"),\"inner\") \\\n",
    "    .filter(col(\"sc.UltimateParent\") != 'IS_GROUP') \\\n",
    "    .select(\"mf.*\")\n",
    "mappedtbddftp_deleted_df = mappedtbddftp_df.exceptAll(mappedtbddftp_del_tmp_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b073fe9d-f7e9-4f96-8638-3e79756a58cc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "allperioddata_df1 = legacysddates_final_df.select(\n",
    "    col(\"id\").alias(\"id\"),\n",
    "    col(\"PeriodType\"),\n",
    "\tF.lit(None).alias(\"Period\"),\n",
    "\tF.lit(None).alias(\"UserGLCode\"),\n",
    "\tF.lit(None).alias(\"currency\"),\n",
    "\tF.lit(None).alias(\"closingbalance\"),\n",
    "\tcol(\"categoryid\"),\n",
    "\tF.lit(None).alias(\"member\"),\n",
    "\tF.lit(None).alias(\"ytdtype\"),\n",
    "\tF.lit(None).alias(\"usergldescription\"),\n",
    "\tF.lit(None).alias(\"Source\"),\n",
    "    col(\"Value\"),\n",
    "    col(\"pcurrentvalue\"),\n",
    "    col(\"legacyperiod\"),\n",
    "    col(\"financialyear\"),\n",
    "    col(\"openingperiodvalue\"),\n",
    "    col(\"castingperiod\"),\n",
    "    col(\"p\"),\n",
    "    col(\"ppp\"),\n",
    "    col(\"py\"),\n",
    "    col(\"pypp\"),\n",
    "    col(\"ppy\"),\n",
    "    col(\"p3y\"),\n",
    "    col(\"p4y\"),\n",
    "\tF.lit(None).alias(\"CastingAdjustmentsID\"),\n",
    "\tF.lit(None).alias(\"Fk_AccountingStandardID\"),\n",
    " \tF.lit(None).alias(\"Fk_ValidationID\"),\n",
    "\tF.lit(None).alias(\"PeriodValue\"),\n",
    "\tF.lit(None).alias(\"Fk_EntityID\"),\n",
    "\tF.lit(None).alias(\"EntityName\"),\n",
    "\tF.lit(None).alias(\"JournalNumber\"),\n",
    "\tF.lit(None).alias(\"JournalDate\"),\n",
    "\tF.lit(None).alias(\"StandardGLCode\"),\n",
    "\tF.lit(None).alias(\"StandardGLDescription\"),\n",
    "\tF.lit(None).alias(\"DebitCredit\"),\n",
    "\tF.lit(None).alias(\"Fk_CurrencyID\"),\n",
    "\tF.lit(None).alias(\"Amount\"),\n",
    "\tF.lit(None).alias(\"JournalType\"),\n",
    "\tF.lit(None).alias(\"Narration\"),\n",
    "\tF.lit(None).alias(\"Fk_BusinessUnitID\"),\n",
    "\tF.lit(None).alias(\"Roundoff\"),\n",
    "\tF.lit(None).alias(\"AmountsIn\"),\n",
    "\tF.lit(None).alias(\"ReportType\"),\n",
    "\tF.lit(None).alias(\"CreatedDate\"),\n",
    "\tF.lit(None).alias(\"CreatedBy\"),\n",
    "\tF.lit(None).alias(\"ModifiedDate\"),\n",
    "\tF.lit(None).alias(\"ModifiedBy\"),\n",
    "\tF.lit(None).alias(\"IsUserGLCode\"),\n",
    "\tF.lit(None).alias(\"RoundTypeLevel\"),\n",
    "\tF.lit(1).alias(\"typeofdata\"),\n",
    "\t*[F.lit(None).cast(\"integer\").alias(col) for col in alldimcols]\n",
    ")\n",
    "\n",
    "allperioddata_df1 = allperioddata_df1.select(\"id\",\"PeriodType\",\"Period\",\"userglcode\",\"currency\",\"closingbalance\",\"categoryid\",\"member\",\"ytdtype\",\"usergldescription\",\"Source\",\"Value\",\"pcurrentvalue\",\"legacyperiod\",\"financialyear\",\"openingperiodvalue\",\"castingperiod\",\"p\",\"ppp\",\"py\",\"pypp\",\"ppy\",\"p3y\",\"p4y\",\"CastingAdjustmentsID\",\"Fk_AccountingStandardID\",\"Fk_ValidationID\",\"PeriodValue\",\"Fk_EntityID\",\"EntityName\",\"JournalNumber\",\"JournalDate\",\"StandardGLCode\",\"StandardGLDescription\",\"DebitCredit\",\"Fk_CurrencyID\",\"Amount\",\"JournalType\",\"Narration\",\"Fk_BusinessUnitID\",\"Roundoff\",\"AmountsIn\",\"ReportType\",\"CreatedDate\",\"CreatedBy\",\"ModifiedDate\",\"ModifiedBy\",\"IsUserGLCode\",\"RoundTypeLevel\",\"typeofdata\",*alldimcols)\n",
    "\n",
    "allperioddata_df2 = castingsdadjustment_df.withColumn(\"IsUserGLCode\", \n",
    "                                               when(col(\"isuserglcode\") == True, 1)\n",
    "                                               .when(col(\"isuserglcode\") == False, 0)\n",
    "                                               .otherwise(None)).select(\n",
    "    col(\"castingadjustmentsid\").alias(\"id\"),\n",
    "\tF.lit(None).alias(\"PeriodType\"),\n",
    "\tF.lit(None).alias(\"Period\"),\n",
    "\tcol(\"userglcode\"),\n",
    "\tF.lit(None).alias(\"currency\"),\n",
    "\tF.lit(None).alias(\"closingbalance\"),\n",
    "\tcol(\"categoryid\"),\n",
    "\tF.lit(None).alias(\"member\"),\n",
    "\tF.lit(None).alias(\"ytdtype\"),\n",
    "\tcol(\"usergldescription\"),\n",
    "\tF.lit(\"Recast Adj.\").alias(\"Source\"),\n",
    "\tF.lit(None).alias(\"Value\"),\n",
    "\tF.lit(None).alias(\"pcurrentvalue\"),\n",
    "\tF.lit(None).alias(\"legacyperiod\"),\n",
    "\tcol(\"financialyear\"),\n",
    "\tF.lit(None).alias(\"openingperiodvalue\"),\n",
    "\tF.lit(None).alias(\"castingperiod\"),\n",
    "\tF.lit(None).alias(\"p\"),\n",
    "\tF.lit(None).alias(\"ppp\"),\n",
    "\tF.lit(None).alias(\"py\"),\n",
    "\tF.lit(None).alias(\"pypp\"),\n",
    "\tF.lit(None).alias(\"ppy\"),\n",
    "\tF.lit(None).alias(\"p3y\"),\n",
    "\tF.lit(None).alias(\"p4y\"),\n",
    "\tcol(\"castingadjustmentsid\"),\n",
    "    col(\"fk_accountingstandardid\"),\n",
    "    col(\"fk_validationid\"),\n",
    "    col(\"periodvalue\"),\n",
    "\tcol(\"fk_entityid\"),\n",
    "\tcol(\"entityname\"),\n",
    "    col(\"journalnumber\"),\n",
    "    col(\"journaldate\"),\n",
    "    col(\"standardglcode\"),\n",
    "    col(\"standardgldescription\"),\n",
    "    col(\"debitcredit\"),\n",
    "    col(\"fk_currencyid\"),\n",
    "    col(\"amount\"),\n",
    "    col(\"journaltype\"),\n",
    "    col(\"narration\"),\n",
    "    col(\"fk_businessunitid\"),\n",
    "\tcol(\"roundoff\"),\n",
    "    col(\"amountsin\"),\n",
    "    col(\"reporttype\"),\n",
    "    col(\"createddate\"),\n",
    "    col(\"createdby\"),\n",
    "    col(\"modifieddate\"),\n",
    "    col(\"modifiedby\"),\n",
    "    col(\"isuserglcode\"),\n",
    "    col(\"roundtypelevel\"),\n",
    "\tF.lit(2).alias(\"typeofdata\"),\n",
    " \t*alldimcols)\n",
    "\n",
    "allperioddata_df2 = allperioddata_df2.select(\"id\",\"PeriodType\",\"Period\",\"userglcode\",\"currency\",\"closingbalance\",\"categoryid\",\"member\",\"ytdtype\",\"usergldescription\",\"Source\",\"Value\",\"pcurrentvalue\",\"legacyperiod\",\"financialyear\",\"openingperiodvalue\",\"castingperiod\",\"p\",\"ppp\",\"py\",\"pypp\",\"ppy\",\"p3y\",\"p4y\",\"CastingAdjustmentsID\",\"Fk_AccountingStandardID\",\"Fk_ValidationID\",\"PeriodValue\",\"Fk_EntityID\",\"EntityName\",\"JournalNumber\",\"JournalDate\",\"StandardGLCode\",\"StandardGLDescription\",\"DebitCredit\",\"Fk_CurrencyID\",\"Amount\",\"JournalType\",\"Narration\",\"Fk_BusinessUnitID\",\"Roundoff\",\"AmountsIn\",\"ReportType\",\"CreatedDate\",\"CreatedBy\",\"ModifiedDate\",\"ModifiedBy\",\"IsUserGLCode\",\"RoundTypeLevel\",\"typeofdata\",*alldimcols)\n",
    "\n",
    "allperioddata_result_df = allperioddata_df.union(allperioddata_df1).union(allperioddata_df2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2166271f-5837-4ccc-b0da-c566fa7fe090",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#write output table for staging\n",
    "if ((mappedsdtbftp_deleted_df.count() > 0) &  (mappedsdtbytd_df.count() > 0)):\n",
    "    print(\"inside if\")\n",
    "    allperioddata_df3 = mappedsdtbftp_deleted_df.select(\n",
    "        col(\"PeriodType\"), col(\"Period\"), col(\"userglcode\"), col(\"currency\"), col(\"closingbalance\"),\n",
    "        col(\"categoryid\"), col(\"member\"), col(\"ytdtype\"),*alldimcols\n",
    "    ).union(mappedsdtbytd_df.select(\n",
    "        col(\"PeriodType\"), col(\"Period\"), col(\"userglcode\"), col(\"currency\"), col(\"closingbalance\"),\n",
    "        col(\"categoryid\"), col(\"member\"), col(\"ytdtype\"),*alldimcols\n",
    "    )).withColumn(\"typeofdata\", lit(\"0\")).distinct()\n",
    "\n",
    "    allperioddata_df3 = allperioddata_df3.select(\n",
    "    F.lit(None).alias(\"id\"),\n",
    "    col(\"PeriodType\"),\n",
    "    col(\"Period\"),\n",
    "    col(\"UserGLCode\"),\n",
    "    col(\"currency\"),\n",
    "    col(\"closingbalance\"),\n",
    "    col(\"categoryid\"),\n",
    "    col(\"member\"),\n",
    "    col(\"ytdtype\"),\n",
    "    F.lit(None).alias(\"usergldescription\"),\n",
    "    F.lit(None).alias(\"Source\"),\n",
    "    F.lit(None).alias(\"Value\"),\n",
    "    F.lit(None).alias(\"pcurrentvalue\"),\n",
    "    F.lit(None).alias(\"legacyperiod\"),\n",
    "    F.lit(None).alias(\"financialyear\"),\n",
    "    F.lit(None).alias(\"openingperiodvalue\"),\n",
    "    F.lit(None).alias(\"castingperiod\"),\n",
    "    F.lit(None).alias(\"p\"),\n",
    "    F.lit(None).alias(\"ppp\"),\n",
    "    F.lit(None).alias(\"py\"),\n",
    "    F.lit(None).alias(\"pypp\"),\n",
    "    F.lit(None).alias(\"ppy\"),\n",
    "    F.lit(None).alias(\"p3y\"),\n",
    "    F.lit(None).alias(\"p4y\"),\n",
    "    F.lit(None).alias(\"CastingAdjustmentsID\"),\n",
    "    F.lit(None).alias(\"Fk_AccountingStandardID\"),\n",
    "    F.lit(None).alias(\"Fk_ValidationID\"),\n",
    "    F.lit(None).alias(\"PeriodValue\"),\n",
    "    F.lit(None).alias(\"Fk_EntityID\"),\n",
    "    F.lit(None).alias(\"EntityName\"),\n",
    "    F.lit(None).alias(\"JournalNumber\"),\n",
    "    F.lit(None).alias(\"JournalDate\"),\n",
    "    F.lit(None).alias(\"StandardGLCode\"),\n",
    "    F.lit(None).alias(\"StandardGLDescription\"),\n",
    "    F.lit(None).alias(\"DebitCredit\"),\n",
    "    F.lit(None).alias(\"Fk_CurrencyID\"),\n",
    "    F.lit(None).alias(\"Amount\"),\n",
    "    F.lit(None).alias(\"JournalType\"),\n",
    "    F.lit(None).alias(\"Narration\"),\n",
    "    F.lit(None).alias(\"Fk_BusinessUnitID\"),\n",
    "    F.lit(None).alias(\"Roundoff\"),\n",
    "    F.lit(None).alias(\"AmountsIn\"),\n",
    "    F.lit(None).alias(\"ReportType\"),\n",
    "    F.lit(None).alias(\"CreatedDate\"),\n",
    "    F.lit(None).alias(\"CreatedBy\"),\n",
    "    F.lit(None).alias(\"ModifiedDate\"),\n",
    "    F.lit(None).alias(\"ModifiedBy\"),\n",
    "    F.lit(None).alias(\"IsUserGLCode\"),\n",
    "    F.lit(None).alias(\"RoundTypeLevel\"),\n",
    "    col(\"typeofdata\"),\n",
    "    *alldimcols\n",
    ")\n",
    "\n",
    "    allperioddata_result_final_df = allperioddata_result_df.union(allperioddata_df3)\n",
    "    \n",
    "    allperioddata_result_final_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(staging_tbl)\n",
    "else:\n",
    "\n",
    "    print(\"drilldown is 0 but mappeddf are empty\")\n",
    "    allperioddata_result_final_df = allperioddata_result_df\n",
    "    allperioddata_result_final_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(staging_tbl)\n",
    "\n",
    "\n",
    "#write output table for drilldown   \n",
    "if ((mappedtbddftp_deleted_df.count() > 0) & (mappedtbddytd_df.count() > 0)):\n",
    "    print(\"inside else\")\n",
    "    allperioddata_df4 = mappedtbddftp_deleted_df.select(\n",
    "        col(\"PeriodType\"), col(\"Period\"), col(\"userglcode\"), col(\"usergldescription\"), col(\"closingbalance\"),\n",
    "        col(\"member\"), col(\"source\"), col(\"ytdtype\"),*alldimcols\n",
    "    ).union(mappedtbddytd_df.select(\n",
    "        col(\"PeriodType\"), col(\"Period\"), col(\"userglcode\"), col(\"usergldescription\"), col(\"closingbalance\"),\n",
    "        col(\"member\"), col(\"source\"), col(\"ytdtype\"),*alldimcols)\n",
    "    ).withColumn(\"typeofdata\", lit(\"0\")).distinct()\n",
    "\n",
    "    allperioddata_df4 = allperioddata_df4.select(\n",
    "    F.lit(None).alias(\"id\"),\n",
    "    col(\"PeriodType\"),\n",
    "    col(\"Period\"),\n",
    "    col(\"UserGLCode\"),\n",
    "    F.lit(None).alias(\"currency\"),\n",
    "    col(\"closingbalance\"),\n",
    "    F.lit(None).alias(\"categoryid\"),\n",
    "    col(\"member\"),\n",
    "    col(\"ytdtype\"),\n",
    "    col(\"usergldescription\"),\n",
    "    col(\"Source\"),\n",
    "    F.lit(None).alias(\"Value\"),\n",
    "    F.lit(None).alias(\"pcurrentvalue\"),\n",
    "    F.lit(None).alias(\"legacyperiod\"),\n",
    "    F.lit(None).alias(\"financialyear\"),\n",
    "    F.lit(None).alias(\"openingperiodvalue\"),\n",
    "    F.lit(None).alias(\"castingperiod\"),\n",
    "    F.lit(None).alias(\"p\"),\n",
    "    F.lit(None).alias(\"ppp\"),\n",
    "    F.lit(None).alias(\"py\"),\n",
    "    F.lit(None).alias(\"pypp\"),\n",
    "    F.lit(None).alias(\"ppy\"),\n",
    "    F.lit(None).alias(\"p3y\"),\n",
    "    F.lit(None).alias(\"p4y\"),\n",
    "    F.lit(None).alias(\"CastingAdjustmentsID\"),\n",
    "    F.lit(None).alias(\"Fk_AccountingStandardID\"),\n",
    "    F.lit(None).alias(\"Fk_ValidationID\"),\n",
    "    F.lit(None).alias(\"PeriodValue\"),\n",
    "    F.lit(None).alias(\"Fk_EntityID\"),\n",
    "    F.lit(None).alias(\"EntityName\"),\n",
    "    F.lit(None).alias(\"JournalNumber\"),\n",
    "    F.lit(None).alias(\"JournalDate\"),\n",
    "    F.lit(None).alias(\"StandardGLCode\"),\n",
    "    F.lit(None).alias(\"StandardGLDescription\"),\n",
    "    F.lit(None).alias(\"DebitCredit\"),\n",
    "    F.lit(None).alias(\"Fk_CurrencyID\"),\n",
    "    F.lit(None).alias(\"Amount\"),\n",
    "    F.lit(None).alias(\"JournalType\"),\n",
    "    F.lit(None).alias(\"Narration\"),\n",
    "    F.lit(None).alias(\"Fk_BusinessUnitID\"),\n",
    "    F.lit(None).alias(\"Roundoff\"),\n",
    "    F.lit(None).alias(\"AmountsIn\"),\n",
    "    F.lit(None).alias(\"ReportType\"),\n",
    "    F.lit(None).alias(\"CreatedDate\"),\n",
    "    F.lit(None).alias(\"CreatedBy\"),\n",
    "    F.lit(None).alias(\"ModifiedDate\"),\n",
    "    F.lit(None).alias(\"ModifiedBy\"),\n",
    "    F.lit(None).alias(\"IsUserGLCode\"),\n",
    "    F.lit(None).alias(\"RoundTypeLevel\"),\n",
    "    col(\"typeofdata\"),\n",
    "    *alldimcols\n",
    ")\n",
    "    \n",
    "    allperioddata_result_final_df = allperioddata_result_df.union(allperioddata_df4)\n",
    "    allperioddata_result_final_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(drilldown_tbl)\n",
    "else:\n",
    "    print(\"drilldown is 1 but mappeddf are empty\")\n",
    "    allperioddata_result_final_df = allperioddata_result_df\n",
    "    allperioddata_result_final_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(drilldown_tbl)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "GetMultiPeriodAllSourceData_old",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
