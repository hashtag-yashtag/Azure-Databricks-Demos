{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2bb28457-d88a-4b7b-a576-433833708335",
     "showTitle": true,
     "title": "Variable Declaration"
    }
   },
   "outputs": [],
   "source": [
    "Bunit=0\n",
    "PeriodValuePrefix=PeriodValue[:1]\n",
    "\n",
    "if PeriodValuePrefix in ('Y','9','H','Q','M'):\n",
    "    PeriodValueInYHQM=1\n",
    "else:\n",
    "    PeriodValueInYHQM=0\n",
    "\n",
    "returnResult=\"Success\"\n",
    "showlog=False\n",
    "\n",
    "config = Configuration(engagement)\n",
    "dbschema = get_tbl_nm('')[:-1]\n",
    "catalogname = dbschema.split(\".\")[0]\n",
    "engagementname = dbschema.split(\".\")[1]\n",
    "dbschema=catalogname+\".\"+engagementname\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ec9512c6-c951-4401-943e-ee1f4b914702",
     "showTitle": true,
     "title": "Import Python Libs"
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime , date\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from databricks import sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "badb416b-1119-47f6-9854-dda4d6632924",
     "showTitle": true,
     "title": "SQL Warehouse Query Executor"
    }
   },
   "outputs": [],
   "source": [
    "def sqlwarehouse_queryExecutor(queryString):\n",
    "    vSQLEPHost=config.adb_host\n",
    "    vSQLEPHttp=config.adb_path\n",
    "    vSQLEPPId=config.adb_token\n",
    "    with sql.connect(server_hostname=f\"{vSQLEPHost}\",http_path=f\"{vSQLEPHttp}\",access_token=f\"{vSQLEPPId}\") as connection:\n",
    "        with connection.cursor() as cursor:\n",
    "            for query in queryString.split(\";\"):\n",
    "                if len(query.strip()) > 0:\n",
    "                    if showlog:print(\"\\n\" + \"[Info] Executing query: \"+ query )\n",
    "                    cursor.execute(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "608bcbb0-3144-4939-bce7-47bc5b6cc75c",
     "showTitle": true,
     "title": "Function Read Data From MS SQL Server"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def read_mssql(tblName):\n",
    "    try:\n",
    "        df_sqldata=spark.read.format(\"sqlserver\").option(\"host\",config.sqlhost).option(\"port\",1433).option(\"user\",config.sqlusername).option(\"password\",config.sqlpassword).option(\"database\",config.sqldbName).option(\"dbtable\", tblName).load()\n",
    "        return df_sqldata\n",
    "    except Exception as e:\n",
    "        returnResult=\"Failure\"\n",
    "        # print(\"Error Occured While Fetching Data From MS SQL Server : \"+ str(e))\n",
    "        raise e\n",
    "        dbutils.notebook.exit(returnResult)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "069cc1ba-ae63-433d-a44c-e0d01d7586d6",
     "showTitle": true,
     "title": "Fetch Entity Master"
    }
   },
   "outputs": [],
   "source": [
    "df_entitymaster=read_mssql(\"master.entitymaster\")\n",
    "df_entitymaster=df_entitymaster.filter(df_entitymaster[\"entitymasterid\"] == EntityID).select(\"fk_fiscalperiodid\",\"fk_functionalcurrencyid\", \"entityname\",\"fk_fiscalperiodid\").first()\n",
    "\n",
    "FK_FiscalPeriodID = df_entitymaster[\"fk_fiscalperiodid\"]\n",
    "SDCurrencyid = df_entitymaster[\"fk_functionalcurrencyid\"]\n",
    "EntitySDName = df_entitymaster[\"entityname\"]\n",
    "FinancialSDCycleID = df_entitymaster[\"fk_fiscalperiodid\"]\n",
    "if showlog:\n",
    "    # print(\"FK_FiscalPeriodID:\",FK_FiscalPeriodID)\n",
    "    # print(\"SDCurrencyid:\",SDCurrencyid)\n",
    "    print(\"EntitySDName:\",EntitySDName)\n",
    "    # print(\"FinancialSDCycleID:\",FinancialSDCycleID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0837cb4a-a2f9-4b8a-88c9-745ce1168378",
     "showTitle": true,
     "title": "Fetch Financial Cycle"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "df_financialcycle=read_mssql(\"master.financialcycle\")\n",
    "df_financialcycle_f = df_financialcycle.filter(df_financialcycle[\"financialcycleid\"] == FK_FiscalPeriodID).select(\"financialcycle\",\"financialcycleid\").first()\n",
    "FiscalPeriod = df_financialcycle_f[\"financialcycle\"]\n",
    "FiscalPeriodID = df_financialcycle_f[\"financialcycleid\"]\n",
    "\n",
    "df_financialcycle_f1 = df_financialcycle.filter(df_financialcycle[\"financialcycleid\"] == FiscalPeriodID).select(\"Q1Start\").first()\n",
    "startVal=df_financialcycle_f1[\"Q1Start\"]\n",
    "\n",
    "if showlog:\n",
    "    print(\"FiscalPeriod : \",FiscalPeriod)\n",
    "    # print(\"FiscalPeriodID : \",FiscalPeriodID)\n",
    "    # print(\"startVal : \",startVal)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dc74b628-c932-4649-9ecc-8f84872e391d",
     "showTitle": true,
     "title": "Fetch SQL Table"
    }
   },
   "outputs": [],
   "source": [
    "df_periodicaldates=read_mssql(\"master.periodicaldates\")\n",
    "df_periodicaldates.createOrReplaceTempView(\"v_periodicaldates\")\n",
    "df_categorydropdown=read_mssql(\"master.categorydropdown\")\n",
    "df_categorydropdown.createOrReplaceTempView(\"v_categorydropdown\")\n",
    "df_EntityPeriodDetails=read_mssql(\"master.EntityPeriodDetails\")\n",
    "df_EntityPeriodDetails.createOrReplaceTempView(\"v_EntityPeriodDetails\")\n",
    "df_PeriodicalDatesForExtensionPeriods=read_mssql(\"master.PeriodicalDatesForExtensionPeriod\")\n",
    "df_PeriodicalDatesForExtensionPeriods.createOrReplaceTempView(\"v_PeriodicalDatesForExtensionPeriod\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "93ac12a7-95e2-4aee-bdde-b464060a451d",
     "showTitle": true,
     "title": "Temp Table Creation [ daterangeconverttoperiod , daterangetoperiod ]"
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"DROP TABLE IF EXISTS \"+dbschema+\".temp_daterangeconverttoperiod_\" + str(parameterID) + \"\")\n",
    "spark.sql(\"CREATE TABLE \"+dbschema+\".temp_daterangeconverttoperiod_\" + str(parameterID) + \"(id INT,currentperiod VARCHAR(50),startdate date, enddate date,periodstart VARCHAR(50),monthdiff int, periodmonthvalue VARCHAR(50),tbperiodvalue VARCHAR(50))\")\n",
    "spark.sql(\"DROP TABLE IF EXISTS \"+dbschema+\".temp_daterangetoperiod_\" + str(parameterID) + \"\")\n",
    "spark.sql(\"CREATE TABLE \"+dbschema+\".temp_daterangetoperiod_\" + str(parameterID) + \"(id INT,cpid VARCHAR(10),currentperiod VARCHAR(50),startdate DATE,enddate DATE,periodstart VARCHAR(50),monthdiff INT,periodmonthvalue VARCHAR(50),tbperiodvalue VARCHAR(50),Year VARCHAR(4),periodvalue VARCHAR(50),customperiod VARCHAR(50),customdaterange INT,openperiodvalue VARCHAR(50),opencyclevalue VARCHAR(50))\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d585a6be-201b-4cad-b293-06b36c3d8e64",
     "showTitle": true,
     "title": "Dimension Columns"
    }
   },
   "outputs": [],
   "source": [
    "alldimcols= \"DimType1ID_fk,DimType2ID_fk,DimType3ID_fk,DimType4ID_fk,DimType5ID_fk,DimType6ID_fk,DimType7ID_fk,DimType8ID_fk,DimType9ID_fk,DimType10ID_fk,DimType11ID_fk,DimType12ID_fk,DimType13ID_fk,DimType14ID_fk,DimType15ID_fk,DimType16ID_fk,DimType17ID_fk,DimType18ID_fk,DimType19ID_fk,DimType20ID_fk,DimType21ID_fk,DimType22ID_fk,DimType23ID_fk,DimType24ID_fk,DimType25ID_fk\"\n",
    "\n",
    "alldimcols_dt= \"DimType1ID_fk INT,DimType2ID_fk INT,DimType3ID_fk INT,DimType4ID_fk INT,DimType5ID_fk INT,DimType6ID_fk INT,DimType7ID_fk INT,DimType8ID_fk INT,DimType9ID_fk INT,DimType10ID_fk INT,DimType11ID_fk INT,DimType12ID_fk INT,DimType13ID_fk INT,DimType14ID_fk INT,DimType15ID_fk INT,DimType16ID_fk INT,DimType17ID_fk INT,DimType18ID_fk INT,DimType19ID_fk INT,DimType20ID_fk INT,DimType21ID_fk INT,DimType22ID_fk INT,DimType23ID_fk INT,DimType24ID_fk INT,DimType25ID_fk INT\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f70e51e5-6a66-4cab-b8ce-0ad6f635095b",
     "showTitle": true,
     "title": "Table Load [ daterangeconverttoperiod , daterangecustomperiod , daterangetoperiod ]"
    }
   },
   "outputs": [],
   "source": [
    "if PeriodValueInYHQM == 0:\n",
    "\n",
    "  spark.sql(\"INSERT INTO \"+dbschema+\".temp_daterangeconverttoperiod_\" + str(parameterID) + \" SELECT monotonically_increasing_id()+1,'\"+str(PeriodValue)+\"',to_date(substr('\"+str(PeriodValue)+\"',0,10), 'dd-MM-yyyy' ),to_date(substring('\"+str(PeriodValue)+\"',12,LEN('\"+str(PeriodValue)+\"')-1), 'dd-MM-yyyy'),\"+str(startVal)+\",0,'','' \")\n",
    "\n",
    "  if len(ComparitivePeriodValue)>0:\n",
    "    \n",
    "    spark.sql(\"INSERT INTO \"+dbschema+\".temp_daterangeconverttoperiod_\" + str(parameterID) + \" SELECT monotonically_increasing_id()+2,col_period,to_date(substr(col_period,0,10), 'dd-MM-yyyy' ),to_date(substring(col_period,12,LEN(col_period)-1), 'dd-MM-yyyy'),\"+str(startVal)+\",0,'','' FROM (SELECT explode(split('\"+str(ComparitivePeriodValue)+\"', ',')) as col_period ) as temp\")\n",
    "  \n",
    "  spark.sql(\"UPDATE \"+dbschema+\".temp_daterangeconverttoperiod_\" + str(parameterID) + \" SET periodmonthvalue= concat('M',(month(enddate)-2+12)%12+1) , monthdiff= Datediff(month, startdate, enddate) + 1 , tbperiodvalue=REPLACE(SUBSTR(enddate,0,7),'-')\")\n",
    "\n",
    "  ############### Populating temp_daterangecustomperiod Data ###############\n",
    "\n",
    "  spark.sql(\"DROP TABLE IF EXISTS \"+dbschema+\".temp_daterangecustomperiod_\" + str(parameterID) + \"\")\n",
    "\n",
    "  spark.sql(\"CREATE TABLE \"+dbschema+\".temp_daterangecustomperiod_\" + str(parameterID) + \" as SELECT pt.*, year, Concat(value, '-' , opening, '-' ,closing) AS periodvalue,  CASE WHEN periodmonthvalue = 'M12' AND  monthdiff = 12 THEN 'Year' WHEN periodmonthvalue = 'M12' AND  monthdiff = 6 THEN 'H2' WHEN periodmonthvalue = 'M12' AND  monthdiff = 3 THEN 'Q4' WHEN periodmonthvalue = 'M3' AND  monthdiff = 3 THEN 'Q1' WHEN periodmonthvalue = 'M6' AND  monthdiff = 6 THEN 'H1' WHEN periodmonthvalue = 'M6' AND  monthdiff = 3 THEN 'Q2' WHEN periodMonthValue ='M9'  AND monthdiff=9  then '9M' WHEN periodmonthvalue = 'M9' AND  monthdiff = 3 THEN 'Q3' ELSE concat('C-',substr(opening,0,4),SUBSTRING(startdate,6,2),'-',closing)  END as customperiod,  0 as customdaterange  FROM v_periodicaldates pd JOIN  \"+dbschema+\".temp_daterangeconverttoperiod_\" + str(parameterID) + \"  pt ON  pd.closing=pt.tbperiodvalue AND substring( pd.period, 1, charindex(' ', pd.period)-1)  = pt.periodmonthvalue AND  financialcycleid=\"+str(FiscalPeriodID)+\" \")\n",
    "\n",
    "  spark.sql(\"UPDATE \"+dbschema+\".temp_daterangecustomperiod_\" + str(parameterID) + \" SET periodvalue = CASE WHEN LEFT(customperiod,1)='C' THEN periodvalue ELSE REPLACE(periodvalue,Substring( periodvalue, 1, Charindex('-', periodvalue)-1 ),customperiod) END, customdaterange=CASE WHEN LEFT(customperiod,1)='C' THEN 1 ELSE 0 END\")\n",
    "  \n",
    "  ############### Populating temp_daterangetoperiod Data ###############\n",
    "\n",
    "  spark.sql(\"INSERT INTO \"+dbschema+\".temp_daterangetoperiod_\" + str(parameterID) + \" SELECT id, concat('CP',id-1) as cpid, currentperiod, startdate, enddate, periodstart, CASE WHEN LEFT(periodmonthvalue, 1) = 'Y' THEN 12 WHEN left(periodmonthvalue,1) = '9' THEN 9  WHEN LEFT(periodmonthvalue, 1) = 'H' THEN 6 WHEN LEFT(periodmonthvalue, 1) = 'Q' THEN 3 WHEN LEFT(periodmonthvalue, 1) = 'M' THEN 1 END, periodmonthvalue, tbperiodvalue, Year, periodvalue, customperiod, customdaterange, '', '' FROM  \"+dbschema+\".temp_daterangecustomperiod_\" + str(parameterID) + \" \")\n",
    "\n",
    "  spark.sql(\"MERGE INTO \"+dbschema+\".temp_daterangetoperiod_\" + str(parameterID) + \" ptb USING v_periodicaldates pd ON ptb.periodmonthvalue=pd.value and financialcycleid=\"+str(FiscalPeriodID)+\" and  pd.year = ptb.year WHEN MATCHED THEN UPDATE SET openperiodvalue = REPLACE(SUBSTR(startdate,0,7),'-') , opencyclevalue = pd.opening\")\n",
    "\n",
    "  PeriodValue=\"\"\n",
    "  PeriodValue_df=spark.sql(\"SELECT periodvalue FROM \"+dbschema+\".temp_daterangetoperiod_\" + str(parameterID) + \" WHERE ID=1 LIMIT 1\")\n",
    "  if PeriodValue_df.count()>0 : PeriodValue=PeriodValue_df.collect()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ba0ddee0-8a4a-4c11-9d4a-a61f1769ef14",
     "showTitle": true,
     "title": "Date Range Variables"
    }
   },
   "outputs": [],
   "source": [
    "PeriodValuePrefix=PeriodValue[:1]\n",
    "PeriodType=PeriodValuePrefix\n",
    "endperiod=PeriodValue[-6:]\n",
    "NumericalValueofPeriodType=\"\"\n",
    "match PeriodType:\n",
    "\tcase 'Y':  NumericalValueofPeriodType=-12\n",
    "\tcase '9':  NumericalValueofPeriodType=-9\n",
    "\tcase 'H':  NumericalValueofPeriodType=-6\n",
    "\tcase 'Q':  NumericalValueofPeriodType=-3\n",
    "\tcase 'M':  NumericalValueofPeriodType=-1\n",
    "\n",
    "if PeriodValueInYHQM == 0:\n",
    "    NumericalValueofPeriodType_df = spark.sql(\"SELECT monthdiff FROM \"+dbschema+\".temp_daterangetoperiod_\" + str(parameterID) + \" WHERE ID=1 LIMIT 1\")\n",
    "    if NumericalValueofPeriodType_df.count() > 0 : NumericalValueofPeriodType=NumericalValueofPeriodType_df.collect()[0][0]\n",
    "\n",
    "YearEndinDateFormat = datetime.strptime(\"01 \"+FiscalPeriod[-3:]+\" \"+FinancialYear, \"%d %b %Y\").date()\n",
    "PeriodEndInDateFormat  = datetime.strptime(endperiod+\"01\", \"%Y%m%d\").date()\n",
    "\n",
    "CurrentPeriod=endperiod\n",
    "CurrentPeriod_YTD=endperiod\n",
    "PreviousPeriod=\"\"\n",
    "\n",
    "if PeriodValuePrefix != '9' :\n",
    "\tPreviousPeriod = (PeriodEndInDateFormat + relativedelta(months=NumericalValueofPeriodType)).strftime(\"%Y%m\")\n",
    "else:\n",
    "\tPreviousPeriod = (PeriodEndInDateFormat + relativedelta(years=-1)).strftime(\"%Y%m\")\n",
    "\n",
    "PrevPreviousPeriod = (PeriodEndInDateFormat + relativedelta(months=2*NumericalValueofPeriodType)).strftime(\"%Y%m\")\n",
    "PreviousYear = (YearEndinDateFormat + relativedelta(years=-1)).strftime(\"%Y%m\")\n",
    "PrevPreviousYear  = (YearEndinDateFormat + relativedelta(years=-2)).strftime(\"%Y%m\")\n",
    "PrevYearCurrPeriod = (PeriodEndInDateFormat + relativedelta(years=-1)).strftime(\"%Y%m\")\n",
    "PrevYearCurrPeriod_YTD =  (PeriodEndInDateFormat + relativedelta(years=-1)).strftime(\"%Y%m\")\n",
    "PrevYearPrevPeriod = (PeriodEndInDateFormat + relativedelta(months=-12 + NumericalValueofPeriodType)).strftime(\"%Y%m\")\n",
    "PPrevPreviousYear = (YearEndinDateFormat + relativedelta(years=-3)).strftime(\"%Y%m\")\n",
    "P4PreviousYear = (YearEndinDateFormat + relativedelta(years=-4)).strftime(\"%Y%m\")\n",
    "\n",
    "if showlog:\n",
    "\tprint(\"PeriodValue                : \" , PeriodValue)\n",
    "\t# print(\"PeriodValuePrefix          : \" , PeriodValuePrefix)\n",
    "\t# print(\"PeriodType                 : \" , PeriodType)\n",
    "\t# print(\"endperiod                  : \" , endperiod)\n",
    "\t# print(\"NumericalValueofPeriodType : \" , NumericalValueofPeriodType)\n",
    "\t# print(\"YearEndinDateFormat        : \" , YearEndinDateFormat)\n",
    "\t# print(\"PeriodEndInDateFormat      : \" , PeriodEndInDateFormat)\n",
    "\t# print(\"CurrentPeriod              : \" , CurrentPeriod)\n",
    "\t# print(\"CurrentPeriod_YTD          : \" , CurrentPeriod_YTD)\n",
    "\t# print(\"PreviousPeriod             : \" , PreviousPeriod)\n",
    "\t# print(\"PrevPreviousPeriod         : \" , PrevPreviousPeriod)\n",
    "\t# print(\"PreviousYear               : \" , PreviousYear)\n",
    "\t# print(\"PrevPreviousYear           : \" , PrevPreviousYear)\n",
    "\t# print(\"PrevYearCurrPeriod         : \" , PrevYearCurrPeriod)\n",
    "\t# print(\"PrevYearCurrPeriod_YTD     : \" , PrevYearCurrPeriod_YTD)\n",
    "\t# print(\"PrevYearPrevPeriod         : \" , PrevYearPrevPeriod)\n",
    "\t# print(\"P4PreviousYear             : \" , P4PreviousYear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "098eef8e-9be3-4ae6-a404-588a3280c8db",
     "showTitle": true,
     "title": "Comparative Period Calculation"
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"DROP TABLE IF EXISTS \"+dbschema+\".temp_complist_\" + str(parameterID) + \"\")\n",
    "spark.sql(\"CREATE TABLE \"+dbschema+\".temp_complist_\" + str(parameterID) + \"(compperiod VARCHAR(50))\")\n",
    "\n",
    "if PeriodValueInYHQM == 1:\n",
    "\tspark.sql(\"INSERT INTO \"+dbschema+\".temp_complist_\" + str(parameterID) + \" SELECT explode(split('\"+str(ComparitivePeriodValue)+\"', ',')) as col_period\")\n",
    "else:\n",
    "\tspark.sql(\"INSERT INTO \"+dbschema+\".temp_complist_\" + str(parameterID) + \" SELECT CONCAT(CASE WHEN customdaterange = 1 THEN periodmonthvalue ELSE customperiod  END,'-',openperiodvalue,'-',tbperiodvalue)  FROM \"+dbschema+\".temp_daterangetoperiod_\" + str(parameterID) + \" WHERE ID<>1 \")\n",
    " \n",
    "countofcomp = \"\"\n",
    "countofcomp_df=spark.sql(\"SELECT count(*) as complist_count FROM  \"+dbschema+\".temp_complist_\" + str(parameterID) + \"\")\n",
    "if countofcomp_df.count() > 0 : countofcomp=countofcomp_df.collect()[0][0]\n",
    "\n",
    "spark.sql(\"DROP TABLE IF EXISTS \"+dbschema+\".temp_usergivenmulticomp_\" + str(parameterID) + \"\")\n",
    "spark.sql(\"DROP TABLE IF EXISTS \"+dbschema+\".temp_compperioddetails_\" + str(parameterID) + \"\")\n",
    "\n",
    "spark.sql(\"CREATE TABLE \"+dbschema+\".temp_usergivenmulticomp_\" + str(parameterID) + \"(id INT, compperiodname VARCHAR(500), compperiodvalue VARCHAR(50), comptype VARCHAR(10), compperiod VARCHAR(6), Value VARCHAR(10)) \")\n",
    "spark.sql(\"CREATE TABLE \"+dbschema+\".temp_compperioddetails_\" + str(parameterID) + \"(comptype VARCHAR(5), alias VARCHAR(500))\")\n",
    "ComparitiveSelectionEndPeriod=\"\"\n",
    "CompYearEndinDateFormat =\"\"\n",
    "CompPeriodEndInDateFormat =\"\"\n",
    "gapincompperiod =\"\"\n",
    "if ComparitivePeriodValue is not None and len(ComparitivePeriodValue) > 0:\n",
    "\tif countofcomp==1:\n",
    "\t\tspark.sql(\"INSERT INTO \"+dbschema+\".temp_compperioddetails_\" + str(parameterID) + \" VALUES ('CP1', 'PreviousPeriod'),('CP1', 'PrevYearCurrPeriod_YTD'),('CP1', 'PrevYearCurrPeriod'),('CP1', 'PreviousYear')\")\n",
    "\telif countofcomp==2:\n",
    "\t\tspark.sql(\"INSERT INTO \"+dbschema+\".temp_compperioddetails_\" + str(parameterID) + \" VALUES ('CP1', 'PreviousPeriod'),('CP1', 'PreviousYear'),('CP2', 'PrevPreviousYear')\")\n",
    "\telif countofcomp==3:\n",
    "\t\tspark.sql(\"INSERT INTO \"+dbschema+\".temp_compperioddetails_\" + str(parameterID) + \" VALUES ('CP1', 'PreviousPeriod'), ('CP1', 'PreviousYear'), ('CP2', 'PrevPreviousYear'), ('CP3', 'PPrevPreviousYear')\")\n",
    "\telif countofcomp==4:\n",
    "\t\tspark.sql(\"INSERT INTO \"+dbschema+\".temp_compperioddetails_\" + str(parameterID) + \" VALUES ('CP1', 'PreviousPeriod'), ('CP1', 'PreviousYear'), ('CP2', 'PrevPreviousYear'), ('CP3', 'PPrevPreviousYear'), ('CP4','P4PreviousYear')\")\n",
    "\telif countofcomp==5:\n",
    "\t\tspark.sql(\"INSERT INTO \"+dbschema+\".temp_compperioddetails_\" + str(parameterID) + \" VALUES ('CP1', 'PreviousPeriod'), ('CP1', 'PreviousYear'), ('CP2', 'PrevPreviousYear'), ('CP3', 'PPrevPreviousYear'), ('CP4','P4PreviousYear'), ('CP5','P5PreviousYear')\")\n",
    "\telif countofcomp==6:\n",
    "\t\tspark.sql(\"INSERT INTO \"+dbschema+\".temp_compperioddetails_\" + str(parameterID) + \" VALUES ('CP1', 'PreviousPeriod'), ('CP1', 'PreviousYear'), ('CP2', 'PrevPreviousYear'), ('CP3', 'PPrevPreviousYear'), ('CP4','P4PreviousYear'), ('CP5','P5PreviousYear'), ('CP6','P6PreviousYear') \")\n",
    "\telif countofcomp==7:\n",
    "\t\tspark.sql(\"INSERT INTO \"+dbschema+\".temp_compperioddetails_\" + str(parameterID) + \" VALUES ('CP1', 'PreviousPeriod'), ('CP1', 'PreviousYear'), ('CP2', 'PrevPreviousYear'), ('CP3', 'PPrevPreviousYear'), ('CP4','P4PreviousYear'), ('CP5','P5PreviousYear'), ('CP6','P6PreviousYear'), ('CP7','P7PreviousYear') \")\n",
    "\telif countofcomp==8:\n",
    "\t\tspark.sql(\"INSERT INTO \"+dbschema+\".temp_compperioddetails_\" + str(parameterID) + \" VALUES ('CP1', 'PreviousPeriod'), ('CP1', 'PreviousYear'), ('CP2', 'PrevPreviousYear'), ('CP3', 'PPrevPreviousYear'), ('CP4','P4PreviousYear'), ('CP5','P5PreviousYear'), ('CP6','P6PreviousYear'), ('CP7','P7PreviousYear'), ('CP8','P8PreviousYear') \")\n",
    "\n",
    "\tspark.sql(\" INSERT INTO \"+dbschema+\".temp_usergivenmulticomp_\" + str(parameterID) + \" SELECT monotonically_increasing_id()+1 as id,alias, CASE WHEN \"+str(PeriodValueInYHQM)+\"=1 THEN compperiodvalue ELSE CONCAT(CASE WHEN customdaterange = 1 THEN periodmonthvalue ELSE customperiod END,'-',openperiodvalue,'-', tbperiodvalue) END as compperiodvalue,pt.comptype, CASE WHEN \"+str(PeriodValueInYHQM)+\"=1 THEN substring(compperiodvalue,-6)  ELSE  tbperiodvalue END as compperiod,'' FROM (SELECT concat('CP',monotonically_increasing_id()+1) as comptype,t.compperiodvalue FROM ( SELECT explode(split('\"+ComparitivePeriodValue+\"', ',')) as compperiodvalue) as t ) as ct JOIN \"+dbschema+\".temp_compperioddetails_\" + str(parameterID) + \" pt ON ct.comptype = pt.comptype LEFT JOIN \"+dbschema+\".temp_daterangetoperiod_\" + str(parameterID) + \" dp on ct.comptype=dp.cpid\")\n",
    "\n",
    "\tspark.sql(\"UPDATE \"+dbschema+\".temp_usergivenmulticomp_\" + str(parameterID) + \" SET Value=substring(compperiodvalue,0, charindex('-', compperiodvalue)-1)\")\n",
    "\tCompFinancialyear=\"\"\n",
    "\tCompFinancialyear_df = spark.sql(\"SELECT CASE WHEN '\"+FiscalPeriod+\"' = 'Jan-Dec' THEN substring(substring(compperiodvalue,-6),0,4) ELSE CAST(Substring( compperiodvalue,Charindex('-', compperiodvalue)+ 1, 4 ) + 1 as INT) END FROM \"+dbschema+\".temp_usergivenmulticomp_\" + str(parameterID) + \" WHERE id=1  \")\n",
    "\tif CompFinancialyear_df.count() > 0 : CompFinancialyear=CompFinancialyear_df.collect()[0][0]\n",
    "    \n",
    "    \n",
    "\tComparitiveSelectionEndPeriod_df = spark.sql(\"SELECT substring(compperiodvalue,-6) FROM \"+dbschema+\".temp_usergivenmulticomp_\" + str(parameterID) + \" WHERE id=1 \")\n",
    "\tif ComparitiveSelectionEndPeriod_df.count() > 0 : ComparitiveSelectionEndPeriod=ComparitiveSelectionEndPeriod_df.collect()[0][0]\n",
    "\t\n",
    "\tCompYearEndinDateFormat = datetime.strptime(\"01 \"+FiscalPeriod[-3:]+\" \"+CompFinancialyear, \"%d %b %Y\").date()\n",
    "\tCompPeriodEndInDateFormat = datetime.strptime(ComparitiveSelectionEndPeriod+\"01\", \"%Y%m%d\").date()\n",
    "\tgapincompperiod = relativedelta(CompYearEndinDateFormat, YearEndinDateFormat).years\n",
    "\n",
    "if showlog:\n",
    "    print(\"ComparitiveSelectionEndPeriod          : \" , ComparitiveSelectionEndPeriod)\n",
    "    # print(\"CompYearEndinDateFormat                : \" , CompYearEndinDateFormat)\n",
    "    # print(\"CompPeriodEndInDateFormat              : \" , CompPeriodEndInDateFormat)\n",
    "    # print(\"gapincompperiod                        : \" , gapincompperiod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "783f057d-01ce-4794-be07-bbaa1a07f47d",
     "showTitle": true,
     "title": "Remove normal period data when one comparative is selected"
    }
   },
   "outputs": [],
   "source": [
    "countofcomp_1=0\n",
    "countofcomp_1_df=spark.sql(\"SELECT count(*) as complist_count FROM  \"+dbschema+\".temp_complist_\" + str(parameterID) + \"\")\n",
    "if countofcomp_1_df.count() > 0 : countofcomp_1=countofcomp_1_df.collect()[0][0]\n",
    "\n",
    "if countofcomp_1 == 1 and PeriodValueInYHQM == 1 :\n",
    "    spark.sql(\"TRUNCATE TABLE \"+dbschema+\".temp_usergivenmulticomp_\" + str(parameterID) + \"\")\n",
    "    if ComparitivePeriodValue is not None and len(ComparitivePeriodValue) > 0:\n",
    "        PreviousPeriod = ComparitiveSelectionEndPeriod\n",
    "        PrevPreviousPeriod = (CompPeriodEndInDateFormat + relativedelta(months=NumericalValueofPeriodType)).strftime(\"%Y%m%d\")\n",
    "        PreviousYear= (CompYearEndinDateFormat + relativedelta(years=0)).strftime(\"%Y%m%d\")\n",
    "        PrevPreviousYear= (CompYearEndinDateFormat + relativedelta(years=-1)).strftime(\"%Y%m%d\")\n",
    "        PrevYearCurrPeriod = (CompPeriodEndInDateFormat + relativedelta(years=0)).strftime(\"%Y%m%d\")\n",
    "        PrevYearCurrPeriod_YTD = (CompPeriodEndInDateFormat + relativedelta(years=0)).strftime(\"%Y%m%d\")\n",
    "        PrevYearPrevPeriod=(CompPeriodEndInDateFormat + relativedelta(months=NumericalValueofPeriodType)).strftime(\"%Y%m%d\")\n",
    "        PPrevPreviousYear= (CompYearEndinDateFormat + relativedelta(years=-2)).strftime(\"%Y%m%d\")\n",
    "        P4PreviousYear= (CompYearEndinDateFormat + relativedelta(years=-3)).strftime(\"%Y%m%d\")\n",
    "\n",
    "\n",
    "spark.sql(\"DROP TABLE IF EXISTS \"+dbschema+\".temp_tmpdates_\" + str(parameterID) + \"\")\n",
    "spark.sql(\"DROP TABLE IF EXISTS \"+dbschema+\".temp_dates_\" + str(parameterID) + \"\")\n",
    "spark.sql(\"CREATE TABLE \"+dbschema+\".temp_tmpdates_\" + str(parameterID) + \"(periodtype VARCHAR(50),currentvalue VARCHAR(6), legacyperiod VARCHAR(20)) \")\n",
    "spark.sql(\"CREATE TABLE \"+dbschema+\".temp_dates_\" + str(parameterID) + \"(periodtype VARCHAR(50),currentvalue VARCHAR(6), legacyperiod VARCHAR(20)) \")\n",
    "\n",
    "if (countofcomp == 0 or countofcomp == 1) and PeriodValueInYHQM == 1 :\n",
    "    spark.sql(\"INSERT INTO \"+dbschema+\".temp_tmpdates_\" + str(parameterID) + \" VALUES ('currentPeriod',LEFT('\"+CurrentPeriod+\"',6), ''),('currentPeriod_YTD',LEFT('\"+CurrentPeriod_YTD+\"',6),''),('PreviousPeriod',LEFT('\"+PreviousPeriod+\"',6),''),('PreviousYear', LEFT('\"+PreviousYear+\"',6),''),('PrevYearCurrPeriod', LEFT('\"+PrevYearCurrPeriod+\"',6),''), ('PrevYearCurrPeriod_YTD', LEFT('\"+PrevYearCurrPeriod_YTD+\"',6),'') \")\n",
    "    spark.sql(\"DELETE FROM \"+dbschema+\".temp_tmpdates_\" + str(parameterID) + \" WHERE periodtype IN (SELECT compperiodname DISTINCT FROM \"+dbschema+\".temp_usergivenmulticomp_\" + str(parameterID) + \")\")\n",
    "    spark.sql(\"INSERT INTO \"+dbschema+\".temp_dates_\" + str(parameterID) + \" SELECT periodtype,currentvalue,legacyperiod FROM \"+dbschema+\".temp_tmpdates_\" + str(parameterID) + \" UNION SELECT  compperiodname as periodtype,compperiod as currentvalue,'' as legacyperiod FROM \"+dbschema+\".temp_usergivenmulticomp_\" + str(parameterID) + \"\")\n",
    "    spark.sql(\"UPDATE \"+dbschema+\".temp_dates_\" + str(parameterID) + \" SET legacyperiod = CASE WHEN \"+str(PeriodValueInYHQM)+\" = 1 THEN CASE WHEN PeriodType LIKE '%Year' THEN concat(currentvalue,'-12') ELSE CASE '\"+str(PeriodValuePrefix)+\"'  WHEN 'Y' THEN concat(currentvalue,'-12') WHEN '9' THEN concat(currentvalue,'-09') WHEN 'H' THEN concat(currentvalue,'-06') WHEN 'Q' THEN concat(currentvalue,'-03') WHEN 'M' THEN concat(currentvalue,'-01') END END END\" )\n",
    "    spark.sql(\"MERGE INTO \"+dbschema+\".temp_dates_\" + str(parameterID) + \" d USING \"+dbschema+\".temp_complist_\" + str(parameterID) + \" c ON d.currentvalue = RIGHT(c.compperiod, 6) WHEN MATCHED THEN UPDATE SET legacyperiod = CASE LEFT( Substring( compperiod, 0, Charindex('-', compperiod)-1 ), 1 ) WHEN 'Y' THEN concat(currentvalue,'-12') WHEN '9' THEN concat(currentvalue,'-09') WHEN 'H' THEN concat(currentvalue,'-06') WHEN 'Q' THEN concat(currentvalue,'-03') WHEN 'M' THEN concat(currentvalue,'-01') END\")\n",
    "\n",
    "    NumericalPeriodTypeValue = abs(NumericalValueofPeriodType)\n",
    "\n",
    "    spark.sql(\"UPDATE \"+dbschema+\".temp_dates_\" + str(parameterID) + \" SET legacyperiod = concat(currentvalue,'-',CASE WHEN len(\"+str(NumericalPeriodTypeValue)+\")=1 THEN concat('0','\"+str(NumericalPeriodTypeValue)+\"') ELSE '\"+str(NumericalPeriodTypeValue)+\"'END)  WHERE legacyperiod IS NULL\" )\n",
    "\n",
    "else:\n",
    "    spark.sql(\"INSERT INTO \"+dbschema+\".temp_tmpdates_\" + str(parameterID) + \" VALUES ('currentPeriod','\"+CurrentPeriod+\"', '')\")\n",
    "    spark.sql(\"INSERT INTO \"+dbschema+\".temp_dates_\" + str(parameterID) + \" SELECT periodtype,currentvalue,CASE '\"+PeriodValuePrefix+\"' WHEN  'Y' THEN concat(currentvalue ,'-12') WHEN  '9' THEN concat(currentvalue,'-09') WHEN  'H' THEN concat(currentvalue,'-06') WHEN  'Q' THEN concat(currentvalue,'-03') WHEN  'M' THEN concat(currentvalue, '-01') END FROM \"+dbschema+\".temp_tmpdates_\" + str(parameterID) + \" UNION SELECT  compperiodname as periodtype,compperiod as currentvalue,CASE LEFT(Value,1) WHEN  'Y' THEN concat(compperiod ,'-12') WHEN  '9' THEN concat(compperiod,'-09') WHEN  'H' THEN concat(compperiod,'-06') WHEN  'Q' THEN concat(compperiod,'-03') WHEN  'M' THEN concat(compperiod, '-01') END legacyperiod FROM \"+dbschema+\".temp_usergivenmulticomp_\" + str(parameterID) + \"\")\n",
    "\n",
    "spark.sql(\"DROP TABLE IF EXISTS \"+dbschema+\".temp_daterangetoperiodtype_\" + str(parameterID) + \"\")\n",
    "spark.sql(\"CREATE TABLE  \"+dbschema+\".temp_daterangetoperiodtype_\" + str(parameterID) + \"  as SELECT DISTINCT dp.*,CASE WHEN alias IS NULL THEN 'CurrentPeriod' ELSE cp.alias END periodtype FROM \"+dbschema+\".temp_daterangetoperiod_\" + str(parameterID) + \" dp LEFT JOIN \"+dbschema+\".temp_compperioddetails_\" + str(parameterID) + \" cp ON dp.cpid = cp.comptype\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d77e8d5c-c0e5-431e-81da-fcb01ef7a071",
     "showTitle": true,
     "title": "Data Fetch"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "spark.sql(\"DROP TABLE IF EXISTS \"+dbschema+\".temp_tempcategory_\" + str(parameterID) + \"\")\n",
    "spark.sql(\"DROP TABLE IF EXISTS \"+dbschema+\".temp_legacysddates_\" + str(parameterID) + \"\")\n",
    "spark.sql(\"DROP TABLE IF EXISTS \"+dbschema+\".temp_ladjjfinal_\" + str(parameterID) + \"\")\n",
    "spark.sql(\"DROP TABLE IF EXISTS \"+dbschema+\".temp_castingsdadjustment_\" + str(parameterID) + \"\")\n",
    "spark.sql(\"DROP TABLE IF EXISTS \"+dbschema+\".temp_castingsdadjustment1_\" + str(parameterID) + \"\")\n",
    "spark.sql(\"DROP TABLE IF EXISTS \"+dbschema+\".temp_allperiodftpscoa_\" + str(parameterID) + \"\")\n",
    "spark.sql(\"DROP TABLE IF EXISTS \"+dbschema+\".temp_standaloneadj_\" + str(parameterID) + \"\")\n",
    "spark.sql(\"DROP TABLE IF EXISTS \"+dbschema+\".temp_Trialbalance_\" + str(parameterID) + \"\")\n",
    "spark.sql(\"DROP TABLE IF EXISTS \"+dbschema+\".temp_legacyadjustment_\" + str(parameterID) + \"\")\n",
    "spark.sql(\"DROP TABLE IF EXISTS \"+dbschema+\".temp_TB_\" + str(parameterID) + \"\")\n",
    "spark.sql(\"DROP TABLE IF EXISTS \"+dbschema+\".temp_SCOAMappingTable_\" + str(parameterID) + \"\")\n",
    "spark.sql(\"DROP TABLE IF EXISTS \"+dbschema+\".temp_SCOAMappingTable1_\" + str(parameterID) + \"\")\n",
    "\n",
    "CategorySDName_df=spark.sql(\"SELECT categoryname FROM v_categorydropdown where categoryid=\"+str(CategoryID)+\"\")\n",
    "if CategorySDName_df.count() > 0 : CategorySDName=CategorySDName_df.collect()[0][0]\n",
    "\n",
    "spark.sql(\"CREATE TABLE  \"+dbschema+\".temp_tempcategory_\" + str(parameterID) + \" as SELECT categoryid from v_categorydropdown where categoryname='\"+str(CategorySDName)+\"' and entityid=\"+str(EntityID)+\" and accountingstandardid=\"+str(AccountingStandardID)+\"\")\n",
    "\n",
    "############### SCOA Mapping Data Creation ###############\n",
    "\n",
    "spark.sql(\"CREATE TABLE \"+dbschema+\".temp_allperiodftpscoa_\" + str(parameterID) + \"(SCOAID int,Member varchar(1000),Path varchar(8000),Column27 varchar(1000),AccountSubType varchar(500),UltimateParent varchar(200) )\")\n",
    "spark.sql(\"INSERT INTO \"+dbschema+\".temp_allperiodftpscoa_\" + str(parameterID) + \"(SCOAID) SELECT SCOAID FROM \"+dbschema+\".scoa WHERE mcoayear=\"+str(FinancialYear)+\" and accountingtype=\"+str(AccountingStandardID)+\"\")\n",
    "spark.sql(\"MERGE INTO \"+dbschema+\".temp_allperiodftpscoa_\" + str(parameterID) + \" ap USING \"+dbschema+\".scoa sc ON ap.SCOAID = sc.SCOAID WHEN MATCHED THEN UPDATE SET ap.Path=sc.Path, ap.Member = sc.Member,ap.column27 = sc.column27,ap.AccountSubType=sc.AccountSubType,ap.UltimateParent=sc.UltimateParent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "80ff05d7-90bc-418e-9d9d-b5f41c855663",
     "showTitle": true,
     "title": "Read Reportrunlog and Executioncontrol tables"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "temp_drilldown_tbl = get_tbl_nm('multiperiod_drilldown_'+str(parameterID)).split('.')[-1]\n",
    "temp_staging_tbl = get_tbl_nm('multiperiod_staging_'+str(parameterID)).split('.')[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "52a14e1f-f4a6-4ff0-9e8d-49a33ef93ebb",
     "showTitle": true,
     "title": "Legacys Dates"
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"DROP TABLE IF EXISTS \"+dbschema+\".temp_legacysddates_\" + str(parameterID) + \"\")\n",
    "\n",
    "spark.sql(\"CREATE TABLE \"+dbschema+\".temp_legacysddates_\" + str(parameterID) + \"( id INT, periodtype VARCHAR(500), Value VARCHAR(5), pcurrentvalue VARCHAR(50), currentvalue VARCHAR(50), legacyperiod VARCHAR(20), categoryid INT, financialyear INT, openingperiodvalue VARCHAR(20), castingperiod VARCHAR(20), numericvalueofperiod INT, ftpperiod INT, periodformat VARCHAR(20), p VARCHAR(20), ppp VARCHAR(20), py VARCHAR(20), pypp VARCHAR(20), ppy VARCHAR(20), p3y VARCHAR(20), p4y VARCHAR(20) )\")\n",
    "\n",
    "spark.sql(\"INSERT INTO \"+dbschema+\".temp_legacysddates_\" + str(parameterID) + \"(id,periodtype,currentvalue,pcurrentvalue,legacyperiod,categoryid,castingperiod) SELECT monotonically_increasing_id()+1,periodtype,currentvalue,currentvalue,legacyperiod,0,12 from \"+dbschema+\".temp_dates_\" + str(parameterID) + \"\")\n",
    "\n",
    "spark.sql(\"merge into \"+dbschema+\".temp_legacysddates_\" + str(parameterID) + \" s using (select ld.id,dp.customdaterange from  \"+dbschema+\".temp_legacysddates_\" + str(parameterID) + \" ld left join \"+dbschema+\".temp_daterangetoperiodtype_\" + str(parameterID) + \" dp on ld.currentvalue=dp.tbperiodvalue and ld.periodtype=dp.periodtype) t on s.id=t.id when matched then update set s.periodformat=case when t.customdaterange<>1 or t.customdaterange is null then case right(s.legacyperiod,2) WHEN '12' THEN 'Yearly' WHEN '06' THEN 'Halfyearly' WHEN '03' THEN 'Quarterly' WHEN '01' THEN 'Monthly' WHEN '09' THEN '9 Months' ELSE 'Monthly' END WHEN t.customdaterange = 1 THEN 'Monthly' END\")\n",
    "\n",
    "spark.sql(\"merge into \"+dbschema+\".temp_legacysddates_\" + str(parameterID) + \" s using (select ld.id,pd.Value,pd.year,opening  from v_periodicaldates pd inner join \"+dbschema+\".temp_legacysddates_\" + str(parameterID) + \" ld on pd.closing =ld.currentvalue left join \"+dbschema+\".temp_daterangetoperiodtype_\" + str(parameterID) + \" dp on ld.currentvalue=dp.tbperiodvalue and ld.periodtype=dp.periodtype  where financialcycleid = 2 and Type = periodformat ) as t on s.id=t.id when matched then update set s.Value=t.Value ,  s.financialyear=t.year , s.openingperiodvalue= case when s.openingperiodvalue is not null then s.openingperiodvalue else t.opening end \")\n",
    "\n",
    "spark.sql(\"merge into \"+dbschema+\".temp_legacysddates_\" + str(parameterID) + \" s using (select ld.id,monthdiff,openperiodvalue from  \"+dbschema+\".temp_legacysddates_\" + str(parameterID) + \" ld left join \"+dbschema+\".temp_daterangetoperiodtype_\" + str(parameterID) + \" dp on ld.currentvalue=dp.tbperiodvalue and ld.periodtype=dp.periodtype) t on s.id=t.id when matched then update set numericvalueofperiod = CASE WHEN \"+str(PeriodValueInYHQM)+\" = 1 THEN CASE LEFT(Value, 1) WHEN 'Y' THEN -12 WHEN '9' THEN -9 WHEN 'H' THEN -6 WHEN 'Q' THEN -3 WHEN 'M' THEN -1 END WHEN \"+str(PeriodValueInYHQM)+\" = 0 THEN CASE WHEN s.PeriodType LIKE '%Year' AND openperiodvalue IS NULL THEN -12 ELSE -monthdiff END END\")\n",
    "\n",
    "numericvalueofperiod=\"\"\n",
    "numericvalueofperiod_df=spark.sql(\"SELECT -monthdiff FROM \"+dbschema+\".temp_daterangetoperiod_\" + str(parameterID) + \" where id=1 \")\n",
    "if numericvalueofperiod_df.count()>0 :numericvalueofperiod=numericvalueofperiod_df.collect()[0][0]\n",
    "spark.sql(\"update \"+dbschema+\".temp_legacysddates_\" + str(parameterID) + \" set numericvalueofperiod= CASE WHEN length('\"+str(numericvalueofperiod)+\"')>0 THEN numericvalueofperiod ELSE NULL END where numericvalueofperiod is null \")\n",
    "\n",
    "spark.sql(\"UPDATE \"+dbschema+\".temp_legacysddates_\" + str(parameterID) + \" SET castingperiod = CONCAT(Value, '-', openingperiodvalue, '-', currentvalue), FTPPeriod = CASE WHEN \"+str(PeriodValueInYHQM)+\" = 1 THEN CASE LEFT(Value, 1) WHEN 'Y' THEN 5 WHEN '9' THEN 4 WHEN 'H' THEN 3 WHEN 'Q' THEN 2 WHEN 'M' THEN 1 ELSE 1 END WHEN \"+str(PeriodValueInYHQM)+\" = 0 THEN 1 END,p=CONCAT(LEFT(DATE_ADD(MONTH,numericvalueofperiod,to_date(CONCAT('01-',RIGHT(currentvalue, 2),'-',LEFT(currentvalue, 4)),'dd-MM-yyyy')),4),RIGHT(LEFT(DATE_ADD(MONTH,numericvalueofperiod,to_date(CONCAT('01-',RIGHT(currentvalue, 2),'-',LEFT(currentvalue, 4)),'dd-MM-yyyy')),7),2)), ppp=CONCAT(LEFT(DATE_ADD(MONTH,2*numericvalueofperiod,to_date(CONCAT('01-',RIGHT(currentvalue, 2),'-',LEFT(currentvalue, 4)),'dd-MM-yyyy')),4),RIGHT(LEFT(DATE_ADD(MONTH,2*numericvalueofperiod,to_date(CONCAT('01-',RIGHT(currentvalue, 2),'-',LEFT(currentvalue, 4)),'dd-MM-yyyy')),7),2)), pypp=CONCAT(LEFT(DATE_ADD(MONTH, -12 + numericvalueofperiod,to_date(CONCAT('01-',RIGHT(currentvalue, 2),'-',LEFT(currentvalue, 4)),'dd-MM-yyyy')),4),RIGHT(LEFT(DATE_ADD(MONTH, -12 + numericvalueofperiod,to_date(CONCAT('01-',RIGHT(currentvalue, 2),'-',LEFT(currentvalue, 4)),'dd-MM-yyyy')),7),2)), py=CONCAT(LEFT(DATE_ADD(YEAR,-1,to_date(CONCAT('01-',RIGHT(currentvalue, 2),'-',LEFT(currentvalue, 4)),'dd-MM-yyyy')),4),RIGHT(LEFT(DATE_ADD(YEAR,-1,to_date(CONCAT('01-',RIGHT(currentvalue, 2),'-',LEFT(currentvalue, 4)),'dd-MM-yyyy')),7),2)), ppy=CONCAT(LEFT(DATE_ADD(YEAR,-2,to_date(CONCAT('01-',RIGHT(currentvalue, 2),'-',LEFT(currentvalue, 4)),'dd-MM-yyyy')),4),RIGHT(LEFT(DATE_ADD(YEAR,-2,to_date(CONCAT('01-',RIGHT(currentvalue, 2),'-',LEFT(currentvalue, 4)),'dd-MM-yyyy')),7),2)), p3y=CONCAT(LEFT(DATE_ADD(YEAR,-3,to_date(CONCAT('01-',RIGHT(currentvalue, 2),'-',LEFT(currentvalue, 4)),'dd-MM-yyyy')),4),RIGHT(LEFT(DATE_ADD(YEAR,-3,to_date(CONCAT('01-',RIGHT(currentvalue, 2),'-',LEFT(currentvalue, 4)),'dd-MM-yyyy')),7),2)), p4y=CONCAT(LEFT(DATE_ADD(YEAR,-4,to_date(CONCAT('01-',RIGHT(currentvalue, 2),'-',LEFT(currentvalue, 4)),'dd-MM-yyyy')),4),RIGHT(LEFT(DATE_ADD(YEAR,-4,to_date(CONCAT('01-',RIGHT(currentvalue, 2),'-',LEFT(currentvalue, 4)),'dd-MM-yyyy')),7),2)) \") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b6ce7237-22b5-4e04-a50c-d89257dcfd75",
     "showTitle": true,
     "title": "Load  Legacy Adjustment, Trialbalance, Casting Adjustment and Standalone Adjjustment"
    }
   },
   "outputs": [],
   "source": [
    "period_value_ld_df = spark.sql(\"SELECT SUBSTRING(legacyperiod, 1, 6) as period_value_ld from \"+dbschema+\".temp_legacysddates_\" + str(parameterID) + \"\")\n",
    "period_value_ld_list = [row.period_value_ld for row in period_value_ld_df.collect()]\n",
    "period_value_ld = \",\".join([\"'\" + value + \"'\" for value in period_value_ld_list])\n",
    "\n",
    "spark.sql(\"DROP TABLE IF EXISTS \"+dbschema+\".temp_standaloneadj_\" + str(parameterID) + \"\")\n",
    "spark.sql(\"CREATE TABLE \"+dbschema+\".temp_standaloneadj_\" + str(parameterID) + \" TBLPROPERTIES('delta.feature.allowColumnDefaults' = 'supported') as SELECT * FROM \"+dbschema+\".adjustments where EntityID_fk=\"+str(EntityID)+\" and AccountingStdID_fk=\"+str(AccountingStandardID)+\" and CategoryID_fk=\"+str(CategoryID)+\" and IsActive = true\")\n",
    "\n",
    "spark.sql(\"DROP TABLE IF EXISTS \"+dbschema+\".temp_trialbalance_\" + str(parameterID) + \"\")\n",
    "spark.sql(\"CREATE TABLE \"+dbschema+\".temp_trialbalance_\" + str(parameterID) + \" TBLPROPERTIES ('delta.feature.allowColumnDefaults' = 'supported') AS SELECT * FROM \"+dbschema+\".trialbalance WHERE EntityID_fk=\"+str(EntityID)+\" and AccountingStdID_fk=\"+str(AccountingStandardID)+\" and SUBSTRING(Period,1,6) in (select closing from v_periodicaldates where financialcycleid=\"+str(FiscalPeriodID)+\")  and IsActive = true and SUBSTRING(Period,1,6) IN (\"+period_value_ld+\") \")\n",
    "\n",
    "spark.sql(\"DROP TABLE IF EXISTS \"+dbschema+\".temp_castingsdadjustment_\" + str(parameterID) + \"\")\n",
    "spark.sql(\"CREATE TABLE \"+dbschema+\".temp_castingsdadjustment_\" + str(parameterID) + \" TBLPROPERTIES ('delta.feature.allowColumnDefaults' = 'supported') AS SELECT * FROM \"+dbschema+\".CastingAdjustmentTable WHERE fk_entityid = \"+str(EntityID)+\" AND fk_accountingstandardid = \"+str(AccountingStandardID)+\"\")\n",
    "\n",
    "spark.sql(\"DROP TABLE IF EXISTS \"+dbschema+\".temp_legacyadjustment_\" + str(parameterID) + \"\")\n",
    "spark.sql(\"CREATE TABLE \"+dbschema+\".temp_legacyadjustment_\" + str(parameterID) + \" TBLPROPERTIES('delta.feature.allowColumnDefaults' = 'supported') as SELECT * FROM  \"+dbschema+\".LegacyAdjustment WHERE  EntityID_fk = \"+str(EntityID)+\" AND AccountingStdID_fk = \"+str(AccountingStandardID)+\"  and IsActive = true\")\n",
    "\n",
    "spark.sql(\"DROP TABLE IF EXISTS \"+dbschema+\".temp_LegRecords_\" + str(parameterID) + \"\")\n",
    "\n",
    "spark.sql(\"CREATE TABLE IF NOT EXISTS \"+dbschema+\".temp_t_legacyadjustmentsdrilldown_\" + str(parameterID) + \" (LegacyAdjustmentID INT,AccountingStdID_fk INT,Period STRING,EntityID_Fk INT,JournalNumber INT,JournalDate TIMESTAMP,UserGLCode STRING,Member STRING,DebitCredit STRING,CurrencyID_Fk INT,Amount DECIMAL(20,5),JournalType STRING,Narration STRING,CreatedDate TIMESTAMP,CreatedBy STRING,ModifiedDate TIMESTAMP,ModifiedBy STRING,BUID_fk INT,IsImported BOOLEAN,CategoryID INT,IsMappedGLCode BOOLEAN DEFAULT TRUE,EndPeriod STRING,NotApplicable STRING,DimType1ID_fk INT,DimType2ID_fk INT,DimType3ID_fk INT,DimType4ID_fk INT,DimType5ID_fk INT,DimType6ID_fk INT,DimType7ID_fk INT,DimType8ID_fk INT,DimType9ID_fk INT,DimType10ID_fk INT,DimType11ID_fk INT,DimType12ID_fk INT,DimType13ID_fk INT,DimType14ID_fk INT,DimType15ID_fk INT,DimType16ID_fk INT,DimType17ID_fk INT,DimType18ID_fk INT,DimType19ID_fk INT,DimType20ID_fk INT,DimType21ID_fk INT,DimType22ID_fk INT,DimType23ID_fk INT,DimType24ID_fk INT,DimType25ID_fk INT)TBLPROPERTIES ('delta.feature.allowColumnDefaults' = 'supported')\")\n",
    "\n",
    "spark.sql(\"CREATE TABLE IF NOT EXISTS \"+dbschema+\".temp_LegRecords_\" + str(parameterID) + \" (LegacyAdjustmentID INT,AccountingStdID_fk INT,Period STRING,EntityID_Fk INT,JournalNumber INT,JournalDate TIMESTAMP,UserGLCode STRING,Member STRING,DebitCredit STRING,CurrencyID_Fk INT,Amount DECIMAL(20,5),JournalType STRING,Narration STRING,CreatedDate TIMESTAMP,CreatedBy STRING,ModifiedDate TIMESTAMP,ModifiedBy STRING,BUID_fk INT,IsImported BOOLEAN,CategoryID INT,IsMappedGLCode BOOLEAN DEFAULT TRUE,EndPeriod STRING,NotApplicable STRING,DimType1ID_fk INT,DimType2ID_fk INT,DimType3ID_fk INT,DimType4ID_fk INT,DimType5ID_fk INT,DimType6ID_fk INT,DimType7ID_fk INT,DimType8ID_fk INT,DimType9ID_fk INT,DimType10ID_fk INT,DimType11ID_fk INT,DimType12ID_fk INT,DimType13ID_fk INT,DimType14ID_fk INT,DimType15ID_fk INT,DimType16ID_fk INT,DimType17ID_fk INT,DimType18ID_fk INT,DimType19ID_fk INT,DimType20ID_fk INT,DimType21ID_fk INT,DimType22ID_fk INT,DimType23ID_fk INT,DimType24ID_fk INT,DimType25ID_fk INT)TBLPROPERTIES ('delta.feature.allowColumnDefaults' = 'supported')\")\n",
    "\n",
    "spark.sql(\"TRUNCATE TABLE \"+dbschema+\".temp_t_legacyadjustmentsdrilldown_\" + str(parameterID) + \"\")\n",
    "\n",
    "spark.sql(\"DROP TABLE IF EXISTS \"+dbschema+\".temp_tempLegcat_\" + str(parameterID) + \"\")\n",
    "spark.sql(\"DROP TABLE IF EXISTS \"+dbschema+\".temp_tmpLegacywithoutendperiod_\" + str(parameterID) + \"\")\n",
    "spark.sql(\"DROP TABLE IF EXISTS \"+dbschema+\".temp_tmplegacywithoutNA_\" + str(parameterID) + \"\")\n",
    "spark.sql(\"DROP TABLE IF EXISTS \"+dbschema+\".temp_tempLegacy_\" + str(parameterID) + \"\")\n",
    "\n",
    "spark.sql(\"CREATE TABLE \"+dbschema+\".temp_tempLegcat_\" + str(parameterID) + \" AS SELECT explode(split('\"+str(CategoryID)+\"', ',')) as categorid  \")\n",
    "\n",
    "spark.sql(\"CREATE TABLE \"+dbschema+\".temp_tmpLegacywithoutendperiod_\" + str(parameterID) + \" TBLPROPERTIES ('delta.feature.allowColumnDefaults' = 'supported') AS SELECT LegacyAdjustmentID,AccountingStdID_fk,Period,EntityID_fk,JournalNumber,JournalDate,UserGLCode,DebitCredit,CurrencyID_fk,Amount,JournalType,Narration,CreatedDate,CreatedBy,ModifiedDate,ModifiedBy,BUID_fk,IsImported,CategoryID,IsMappedGLCode,EndPeriod,NotApplicable, \"+alldimcols+\" FROM \"+dbschema+\".temp_legacyadjustment_\" + str(parameterID) + \" WHERE BUID_fk='\"+str(Bunit)+\"' and CategoryID in (select categoryid from \"+dbschema+\".temp_tempLegcat_\" + str(parameterID) + \" ) and EntityID_fk = \"+str(EntityID)+\" and AccountingStdID_fk = \"+str(AccountingStandardID)+\" and NVL(EndPeriod,999999-99) > Period \")\n",
    "\n",
    "\n",
    "spark.sql(\"CREATE TABLE \"+dbschema+\".temp_tmplegacywithoutNA_\" + str(parameterID) + \" TBLPROPERTIES ('delta.feature.allowColumnDefaults' = 'supported') SELECT *,case when Period = value then 0 when (NotApplicable is not null and NotApplicable !='' and value='') then 0 else 1 end IsmappedGLUpdatedCode FROM (SELECT *,explode(split(NVL(NotApplicable,''), ',')) as value  FROM \"+dbschema+\".temp_tmpLegacywithoutendperiod_\" + str(parameterID) + \") as t\")\n",
    "\n",
    "\n",
    "spark.sql(\"CREATE TABLE \"+dbschema+\".temp_tempLegacy_\" + str(parameterID) + \" TBLPROPERTIES ('delta.feature.allowColumnDefaults' = 'supported') AS SELECT DISTINCT LegacyAdjustmentID,AccountingStdID_fk,Period,EntityID_fk,JournalNumber,JournalDate,UserGLCode,DebitCredit,CurrencyID_fk,Amount, JournalType,Narration,CreatedDate,CreatedBy,ModifiedDate,ModifiedBy,BUID_fk,IsImported,CategoryID,IsMappedGLCode,EndPeriod,NotApplicable, \"+alldimcols+\" FROM \"+dbschema+\".temp_tmplegacywithoutNA_\" + str(parameterID) + \" WHERE IsmappedGLUpdatedCode=1 and period not in (select distinct value from   \"+dbschema+\".temp_tmplegacywithoutNA_\" + str(parameterID) + \" )\")\n",
    "\n",
    "spark.sql(\"INSERT INTO  \"+dbschema+\".temp_t_legacyadjustmentsdrilldown_\" + str(parameterID) + \" (LegacyAdjustmentID, AccountingStdID_fk, Period, EntityID_fk, JournalNumber, JournalDate, UserGLCode, DebitCredit, CurrencyID_fk, Amount, JournalType, Narration , CreatedDate, CreatedBy, ModifiedDate, ModifiedBy, BUID_fk, IsImported, CategoryID, IsMappedGLCode, EndPeriod, NotApplicable, \"+alldimcols+\" ) select LegacyAdjustmentID, AccountingStdID_fk, Period, EntityID_fk, JournalNumber, JournalDate, UserGLCode, DebitCredit, CurrencyID_fk, Amount, JournalType, Narration , CreatedDate, CreatedBy, ModifiedDate, ModifiedBy, BUID_fk, IsImported, CategoryID, IsMappedGLCode, EndPeriod, NotApplicable, \"+alldimcols+\" from \"+dbschema+\".temp_tempLegacy_\" + str(parameterID) + \" \")\n",
    "\n",
    "spark.sql(\"INSERT INTO  \"+dbschema+\".temp_LegRecords_\" + str(parameterID) + \" SELECT *  from \"+dbschema+\".temp_t_legacyadjustmentsdrilldown_\" + str(parameterID) + \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a1a77091-faa6-49fa-84d9-6d4b3b0c1123",
     "showTitle": true,
     "title": "Combaing TB , ADJ and Leagcy ADJ"
    }
   },
   "outputs": [],
   "source": [
    "# Combaing TB , ADJ and Leagcy ADJ\n",
    "\n",
    "########### Action Item : UserGLDescription is not avaliable in LegRecords and Unit is not avaliable -- Businnes Unit in Dimension\n",
    "\n",
    "spark.sql(\"DROP TABLE IF EXISTS \"+dbschema+\".temp_tb_\" + str(parameterID) + \"\")\n",
    "\n",
    "spark.sql(\"CREATE TABLE \"+dbschema+\".temp_tb_\" + str(parameterID) + \" AS SELECT DISTINCT UserGLCode,UserGLDescription,EntityID_fk,0 as unit FROM (SELECT UserGLCode,UserGLDescription,EntityID_fk,0 as unit FROM \"+dbschema+\".temp_trialbalance_\" + str(parameterID) + \" UNION ALL SELECT UserGLcode, NULL as UserGLDescription, EntityID_fk, 0 as unit FROM \"+dbschema+\".temp_standaloneadj_\" + str(parameterID) + \"  UNION ALL SELECT UserGLCode, NULL as UserGLDescription, EntityID_fk, 0 as unit FROM \"+dbschema+\".temp_LegRecords_\" + str(parameterID) + \") as t \")\n",
    "\n",
    "yearperiod = spark.sql(\"SELECT closing FROM v_periodicaldates WHERE financialcycleid = '\"+str(FiscalPeriodID)+\"'  AND year = '\"+str(FinancialYear)+\"' AND Type = 'Yearly'\").collect()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a0cab54a-e888-4f02-982a-f2356cf77502",
     "showTitle": true,
     "title": "Load SCOAMappingTable and  SCOAMappingTable 1"
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"DROP TABLE IF EXISTS \"+dbschema+\".temp_SCOAMappingTable1_\" + str(parameterID) + \"\")\n",
    "\n",
    "spark.sql(\"CREATE TABLE \"+dbschema+\".temp_SCOAMappingTable1_\" + str(parameterID) + \" (EntityId int ,Period varchar(12) ,Accountingtype int ,EYGLCode varchar(500) ,UserGLCode varchar(500) ,CategoryID int ,UltimateParent varchar(100) ,Unit varchar(100) ,CreatedBy varchar(100) ,ModifiedBy varchar(100) ,CreatedDate varchar(100) ,ModifiedDate varchar(100) ,PresentationLayer varchar(100) ,UserGLDescription varchar(500) ) \")\n",
    "\n",
    "spark.sql(\"DROP TABLE IF EXISTS \"+dbschema+\".temp_SCOAMappingTable_\" + str(parameterID) + \"\")\n",
    "\n",
    "spark.sql(\"CREATE TABLE \"+dbschema+\".temp_SCOAMappingTable_\" + str(parameterID) + \"(EntityId int ,Period varchar(12) ,Accountingtype int ,EYGLCode varchar(500) ,UserGLCode varchar(500) ,CategoryID int ,UltimateParent varchar(100) ,Unit varchar(100) ,CreatedBy varchar(100) ,ModifiedBy varchar(100) ,CreatedDate varchar(100) ,ModifiedDate varchar(100) ,PresentationLayer varchar(100) ,UserGLDescription varchar(500) )\")\n",
    "\n",
    "yearperiod=\"\"\n",
    "yearperiod_df = spark.sql(\"SELECT closing FROM v_periodicaldates WHERE financialcycleid = '\"+str(FiscalPeriodID)+\"'  AND year = '\"+str(FinancialYear)+\"' AND Type = 'Yearly'\")\n",
    "if yearperiod_df.count() > 0 : yearperiod=yearperiod_df.collect()[0][0]\n",
    "\n",
    "\n",
    "IsAutomapped=\"\"\n",
    "IsAutomapped_df=spark.sql(\"SELECT IsAutomapped FROM v_categorydropdown where categoryid=\"+str(CategoryID)+\"\")\n",
    "if IsAutomapped_df.count() > 0 : IsAutomapped=IsAutomapped_df.collect()[0][0]\n",
    "\n",
    "if IsAutomapped==1:\n",
    "  spark.sql(\"INSERT INTO \"+dbschema+\".temp_SCOAMappingTable1_\" + str(parameterID) + \" SELECT tb.EntityID_fk AS EntityId, concat('\"+str(yearperiod)+\"', '-12') AS Period, '\"+str(AccountingStandardID)+\"' AS AccountingType, scoa.Member AS EYGLCode, tb.UserGLCode AS UserGLCode, '\"+str(CategoryID)+\"' AS CategoryID, scoa.UltimateParent AS UltimateParent, tb.Unit AS Unit, 'admin' AS CreatedBy, 'admin' AS ModifiedBy, current_timestamp() AS CreatedDate, current_timestamp() AS ModifiedDate, NULL AS PresentationLayer, tb.UserGLDescription AS UserGLDescription FROM \"+dbschema+\".temp_tb_\" + str(parameterID) + \" tb JOIN \"+dbschema+\".temp_allperiodftpscoa_\" + str(parameterID) + \" scoa ON scoa.Member = tb.UserGLCode WHERE NOT EXISTS ( SELECT UserGlcode FROM \"+dbschema+\".scoamappingtable sm WHERE EntityID = '\"+str(EntityID)+\"' AND CategoryID = '\"+str(CategoryID)+\"' AND mapped = 1 and tb.UserGLCode=sm.UserGLCode)\")\n",
    "\n",
    "  spark.sql(\"INSERT INTO  \"+dbschema+\".temp_SCOAMappingTable_\" + str(parameterID) + \" SELECT DISTINCT * FROM (select distinct  EntityID,Period,accountingtype,EYGLCode,UserGLCode,CategoryID,UltimateParent,Unit BUCode,'admin' CreatedBy,'admin' modifiedby,current_timestamp() CreatedDate,current_timestamp() ModifiedDate,PresentationLayer,UserGLDescription from \"+dbschema+\".temp_SCOAMappingTable1_\" + str(parameterID) + \" where UserGLCode not in (select UserGLCode from \"+dbschema+\".scoamappingtable where categoryid='\"+str(CategoryID)+\"' and unmapped=1) UNION select entityid,reportingfrequency,accountingtype,eyglcode,userglcode,categoryid,ultimateparent,bucode,createdby,modifiedby,createddate,modifieddate,presentationlayer,usergldescription from \"+dbschema+\".scoamappingtable where entityid = '\"+str(EntityID)+\"' and  categoryid = '\"+str(CategoryID)+\"'and mapped=1 ) as t\")\n",
    "else:\n",
    "  spark.sql(\"INSERT INTO \"+dbschema+\".temp_SCOAMappingTable1_\" + str(parameterID) + \" SELECT EntityID,ReportingFrequency,accountingtype,EYGLCode,UserGLCode,CategoryID,UltimateParent,BuCode,CreatedBy,modifiedby,CreatedDate,ModifiedDate,PresentationLayer,UserGLDescription FROM \"+dbschema+\".scoamappingtable where entityid = '\"+str(EntityID)+\"' and  categoryid = '\"+str(CategoryID)+\"'and mapped=1\")\n",
    "\n",
    "  spark.sql(\"INSERT INTO \"+dbschema+\".temp_SCOAMappingTable_\" + str(parameterID) + \" select distinct  EntityID,Period,accountingtype,EYGLCode,UserGLCode,CategoryID,UltimateParent,Unit BUCode,'admin' CreatedBy,'admin' modifiedby,current_timestamp() CreatedDate,current_timestamp() ModifiedDate,PresentationLayer,UserGLDescription from \"+dbschema+\".temp_SCOAMappingTable1_\" + str(parameterID) + \" where UserGLCode not in (select UserGLCode from \"+dbschema+\".scoamappingtable where categoryid='\"+str(CategoryID)+\"' and unmapped=1)\")\n",
    "\n",
    "  spark.sql(f\"\"\" MERGE INTO {dbschema}.temp_legacysddates_\" + str(parameterID) + \" AS t USING {dbschema}.temp_daterangetoperiodtype_\" + str(parameterID) + \" AS s ON t.currentvalue = s.tbperiodvalue AND t.periodtype = s.periodtype WHEN MATCHED THEN UPDATE SET t.periodformat = CASE WHEN s.customdaterange <> 1 OR s.customdaterange IS NULL THEN CASE RIGHT(t.legacyperiod, 2) WHEN '12' THEN 'Yearly' WHEN '06' THEN 'Halfyearly' WHEN '03' THEN 'Quarterly' WHEN '01' THEN 'Monthly' WHEN '09' THEN '9 Months' ELSE 'Monthly' END WHEN s.customdaterange = 1 THEN 'Monthly' END \"\"\") \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e89d0e17-d069-4ba0-aada-ce80ddafeca5",
     "showTitle": true,
     "title": "Table Creation"
    }
   },
   "outputs": [],
   "source": [
    "queryString=\"DROP TABLE IF EXISTS {vSQLDB}.temp_priorityseq_\" + str(parameterID) + \";DROP TABLE IF EXISTS {vSQLDB}.temp_prioritylevel2_\" + str(parameterID) + \";DROP TABLE IF EXISTS {vSQLDB}.temp_priorityseq1_\" + str(parameterID) + \";DROP TABLE IF EXISTS {vSQLDB}.temp_prioritylevel2temp_\" + str(parameterID) + \";DROP TABLE IF EXISTS {vSQLDB}.temp_tballperiod_\" + str(parameterID) + \";DROP TABLE IF EXISTS {vSQLDB}.temp_tblegacymatchperiod_\" + str(parameterID) + \";DROP TABLE IF EXISTS {vSQLDB}.temp_tblegacymatchperiod1_\" + str(parameterID) + \";DROP TABLE IF EXISTS {vSQLDB}.temp_ytddates_\" + str(parameterID) + \";\"\n",
    "\n",
    "queryString=queryString.replace(\"{vSQLDB}\",f\"{dbschema}\")\n",
    "\n",
    "sqlwarehouse_queryExecutor(queryString)\n",
    "\n",
    "queryString=\"CREATE TABLE {vSQLDB}.temp_priorityseq_\" + str(parameterID) + \"(Period VARCHAR(20),stdperiod VARCHAR(20),PeriodType VARCHAR(50),closing VARCHAR(20),legacyperiod VARCHAR(20),numericvalueofperiod INT,priorityseq INT,Order INT,FTPPeriod INT);CREATE TABLE {vSQLDB}.temp_prioritylevel2_\" + str(parameterID) + \"(Period VARCHAR(20),stdperiod VARCHAR(20),PeriodType VARCHAR(50),closing VARCHAR(20),legacyperiod VARCHAR(20),numericvalueofperiod INT,priorityseq INT,Order INT);CREATE TABLE {vSQLDB}.temp_priorityseq1_\" + str(parameterID) + \"(Period VARCHAR(20),stdperiod VARCHAR(20),PeriodType VARCHAR(50),closing VARCHAR(20),legacyperiod VARCHAR(20),numericvalueofperiod INT,FTPPeriod INT);CREATE TABLE {vSQLDB}.temp_prioritylevel2temp_\" + str(parameterID) + \"(Period VARCHAR(20),stdperiod VARCHAR(20),PeriodType VARCHAR(50),closing VARCHAR(20),legacyperiod VARCHAR(20),numericvalueofperiod INT);CREATE TABLE {vSQLDB}.temp_tballperiod_\" + str(parameterID) + \"(Period VARCHAR(20),stdperiod VARCHAR(20),PeriodType VARCHAR(50),closing VARCHAR(20),LegacyPeriod VARCHAR(20),numericvalueofperiod INT);CREATE TABLE {vSQLDB}.temp_tblegacymatchperiod_\" + str(parameterID) + \"(Period VARCHAR(20),stdperiod VARCHAR(20),PeriodType VARCHAR(50),closing VARCHAR(20),LegacyPeriod VARCHAR(20),numericvalueofperiod INT);CREATE TABLE {vSQLDB}.temp_tblegacymatchperiod1_\" + str(parameterID) + \"(Period VARCHAR(20),stdperiod VARCHAR(20),PeriodType VARCHAR(50),closing VARCHAR(20),LegacyPeriod VARCHAR(20),numericvalueofperiod INT);CREATE TABLE {vSQLDB}.temp_ytddates_\" + str(parameterID) + \"(PeriodType VARCHAR(50),CurrentValue VARCHAR(6),FTPValue VARCHAR(10));\"\n",
    "            \n",
    "queryString=queryString.replace(\"{vSQLDB}\",f\"{dbschema}\")\n",
    "\n",
    "sqlwarehouse_queryExecutor(queryString)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0f8d67fa-b112-431d-998e-1bb26cc9b17f",
     "showTitle": true,
     "title": "Calculate month difference for extended cycle entties"
    }
   },
   "outputs": [],
   "source": [
    "Extendedmonthdiff=0\n",
    "Extendedcycleyear=\"\"\n",
    "ExtValue=\"\"\n",
    "extflag=0\n",
    "countPriority=0\n",
    "\n",
    "Extendedmonthdiff_df=spark.sql(\"SELECT DATEDIFF(MONTH, startdate, enddate)+ 1 FROM v_EntityPeriodDetails where fk_entitymasterid='\"+str(EntityID)+\"' \")\n",
    "if Extendedmonthdiff_df.count()>0 :Extendedmonthdiff=Extendedmonthdiff_df.collect()[0][0]\n",
    "\n",
    "extflag_df=spark.sql(\"SELECT 1 FROM v_EntityPeriodDetails where fk_entitymasterid='\"+str(EntityID)+\"' and isExtendedPeriod=1 \")\n",
    "if extflag_df.count()>0 :extflag=extflag_df.collect()[0][0]\n",
    "\n",
    "ExtValue=spark.sql(\"SELECT SUBSTRING('\"+str(PeriodValue)+\"',1,CHARINDEX('-','\"+str(PeriodValue)+\"')-1) as extValue\").collect()[0][0]\n",
    "\n",
    "Extendedcycleyear_df=spark.sql(\"SELECT Year FROM v_PeriodicalDatesForExtensionPeriod where Entityid='\"+str(EntityID)+\"' and Value=\"+str(extflag)+\" and FinancialCycleID=\"+str(FiscalPeriodID)+\" and Year is not null \")\n",
    "if Extendedcycleyear_df.count()>0 :Extendedcycleyear=Extendedcycleyear_df.collect()[0][0]\n",
    "\n",
    "if showlog:\n",
    "    print(\"Extendedmonthdiff                : \" , Extendedmonthdiff)\n",
    "    # print(\"Extendedcycleyear                : \" , Extendedcycleyear)\n",
    "    # print(\"ExtValue                         : \" , ExtValue)\n",
    "    # print(\"extflag                          : \" , extflag)\n",
    "    # print(\"countPriority                    : \" , countPriority)\n",
    "\n",
    "if extflag != 1 or (extflag == 1 and Extendedmonthdiff <= 12) or (extflag == 1 and Extendedmonthdiff > 12 and Extendedcycleyear != FinancialYear):\n",
    "\n",
    "    queryString=\"DROP TABLE IF EXISTS {vSQLDB}.temp_periodicaldateswithlargeperiod_\" + str(parameterID) + \";DROP TABLE IF EXISTS {vSQLDB}.temp_periodicaldateswithequalperiod_\" + str(parameterID) + \";DROP TABLE IF EXISTS {vSQLDB}.temp_legacyDatesLite_\" + str(parameterID) + \";DROP TABLE IF EXISTS {vSQLDB}.temp_prioritylevel2temp_01_\" + str(parameterID) + \";\" \n",
    "    \n",
    "    queryString=queryString.replace(\"{vSQLDB}\",f\"{dbschema}\")\n",
    "    \n",
    "    sqlwarehouse_queryExecutor(queryString)\n",
    "\n",
    "\n",
    "    if PeriodValueInYHQM == 0:\n",
    "        spark.sql(\"DELETE FROM \"+dbschema+\".temp_legacysddates_\" + str(parameterID) + \" where PeriodType LIKE '%YTD'\")\n",
    "        spark.sql(\"UPDATE \"+dbschema+\".temp_legacysddates_\" + str(parameterID) + \" SET periodtype=periodtype + '_YTD' \")\n",
    "    \n",
    "    spark.sql(\"CREATE TABLE \"+dbschema+\".temp_periodicaldateswithlargeperiod_\" + str(parameterID) + \" AS SELECT DISTINCT PeriodType, closing, LegacyPeriod, numericvalueofperiod FROM \"+dbschema+\".temp_legacysddates_\" + str(parameterID) + \" ld JOIN v_PeriodicalDates mp ON ld.FTPPeriod >= mp.Order WHERE financialcycleid = \"+str(FiscalPeriodID)+\" AND mp.closing <= ld.currentvalue AND mp.closing >= ld.openingperiodvalue\")\n",
    "\n",
    "    spark.sql(\"CREATE TABLE \"+dbschema+\".temp_periodicaldateswithequalperiod_\" + str(parameterID) + \" AS SELECT DISTINCT PeriodType,closing, LegacyPeriod, numericvalueofperiod FROM \"+dbschema+\".temp_legacysddates_\" + str(parameterID) + \" ld JOIN v_PeriodicalDates mp ON ld.FTPPeriod = mp.Order WHERE financialcycleid = \"+str(FiscalPeriodID)+\" AND mp.closing <= ld.currentvalue AND mp.closing >= ld.openingperiodvalue \")\n",
    "\n",
    "    if PeriodValueInYHQM == 0:\n",
    "        spark.sql(\"INSERT INTO \"+dbschema+\".temp_tballperiod_\" + str(parameterID) + \" SELECT Period, LEFT(Period, 6) stdperiod, ld.*,\"+alldimcols+\" FROM \"+dbschema+\".temp_trialbalance_\" + str(parameterID) + \" tb JOIN \"+dbschema+\".temp_periodicaldateswithlargeperiod_\" + str(parameterID) + \" ld ON LEFT(tb.Period, 6)= ld.closing AND RIGHT(tb.Period, 2) <= ABS(ld.numericvalueofperiod) WHERE EntityID_fk = \"+str(EntityID)+\" \")\n",
    "\n",
    "        spark.sql(\"INSERT INTO \"+dbschema+\".temp_tblegacymatchperiod_\" + str(parameterID) + \" SELECT Period, LEFT(Period, 6) stdperiod, ld.*,\"+alldimcols+\" FROM \"+dbschema+\".temp_trialbalance_\" + str(parameterID) + \" tb JOIN \"+dbschema+\".temp_periodicaldateswithequalperiod_\" + str(parameterID) + \" ld ON tb.Period = CONCAT(closing, '-', RIGHT(legacyperiod, 2)) WHERE EntityID_fk = \"+str(EntityID)+\" \" )\n",
    "\n",
    "        spark.sql(\"INSERT INTO \"+dbschema+\".temp_priorityseq1_\" + str(parameterID) + \" SELECT tb.*, ld.FTPPeriod FROM \"+dbschema+\".temp_tballperiod_\" + str(parameterID) + \" tb JOIN ( SELECT DISTINCT ld.LegacyPeriod, ld.periodtype, ld.FTPPeriod FROM \"+dbschema+\".temp_legacysddates_\" + str(parameterID) + \" ld LEFT JOIN \"+dbschema+\".temp_tblegacymatchperiod_\" + str(parameterID) + \" mp ON ld.LegacyPeriod = mp.LegacyPeriod AND ld.periodtype = mp.periodtype WHERE mp.LegacyPeriod IS NULL ) ld ON LEFT(tb.Period, 6)<= LEFT(ld.LegacyPeriod, 6) AND tb.periodtype = ld.periodtype WHERE RIGHT(tb.Period, 2)<> RIGHT(ld.LegacyPeriod, 2)\" )\n",
    "\n",
    "        spark.sql(\"INSERT INTO \"+dbschema+\".temp_priorityseq_\" + str(parameterID) + \" SELECT Period, stdperiod, PeriodType, closing, legacyperiod, numericvalueofperiod, ROW_NUMBER() OVER (PARTITION BY stdperiod, periodtype ORDER BY Period DESC) priorityseq, CASE WHEN SUBSTRING(Period, LENGTH(Period) - 1, 2) >= '12' THEN 4 WHEN SUBSTRING(Period, LENGTH(Period) - 1, 2) = '06' THEN 3 WHEN SUBSTRING(Period, LENGTH(Period) - 1, 2) = '03' THEN 2 ELSE 1 END, FTPPeriod FROM \"+dbschema+\".temp_priorityseq1_\" + str(parameterID) + \" ORDER BY periodtype\" )\n",
    "\n",
    "        spark.sql(\"DELETE FROM \"+dbschema+\".temp_priorityseq_\" + str(parameterID) + \" WHERE priorityseq<>1\")\n",
    "\n",
    "        spark.sql(\"MERGE INTO \"+dbschema+\".temp_legacysddates_\" + str(parameterID) + \" AS t USING ( select ld.id,ps.Period from \"+dbschema+\".temp_legacysddates_\" + str(parameterID) + \" ld inner join \"+dbschema+\".temp_priorityseq_\" + str(parameterID) + \" ps on ld.periodtype = ps.PeriodType AND ld.currentvalue = ps.stdperiod ) as s ON t.id=s.id WHEN MATCHED THEN UPDATE SET t.LegacyPeriod = s.Period\")\n",
    "\n",
    "        spark.sql(\"INSERT INTO \"+dbschema+\".temp_ytddates_\" + str(parameterID) + \" SELECT PeriodType, LEFT(legacyperiod, 6) stdperiod, Period FROM \"+dbschema+\".temp_tblegacymatchperiod_\" + str(parameterID) + \" WHERE (Periodtype LIKE '%YTD' OR Periodtype LIKE '%Year') UNION SELECT PeriodType, LEFT(legacyperiod, 6), Period FROM \"+dbschema+\".temp_priorityseq_\" + str(parameterID) + \" WHERE (Periodtype LIKE '%YTD' OR Periodtype LIKE '%Year')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d8c17dc7-0cca-4274-9f7a-fe35d03e52ea",
     "showTitle": true,
     "title": "Calculate month difference for extended cycle entties"
    }
   },
   "outputs": [],
   "source": [
    "if extflag != 1 or (extflag == 1 and Extendedmonthdiff <= 12) or (extflag == 1 and Extendedmonthdiff > 12 and Extendedcycleyear != FinancialYear):\n",
    "    if PeriodValueInYHQM == 1:\n",
    "        spark.sql(\"INSERT INTO \"+dbschema+\".temp_tballperiod_\" + str(parameterID) + \" SELECT Period, LEFT(Period, 6) stdperiod, ld.* FROM \"+dbschema+\".temp_trialbalance_\" + str(parameterID) + \" tb JOIN \"+dbschema+\".temp_periodicaldateswithlargeperiod_\" + str(parameterID) + \" ld ON LEFT(tb.Period, 6)= ld.closing AND RIGHT(tb.Period, 2) <= ABS(ld.numericvalueofperiod) WHERE EntityID_fk = \"+str(EntityID)+\" \")\n",
    "\n",
    "        spark.sql(\"INSERT INTO \"+dbschema+\".temp_tblegacymatchperiod_\" + str(parameterID) + \" SELECT Period, LEFT(Period, 6) stdperiod, ld.* FROM \"+dbschema+\".temp_trialbalance_\" + str(parameterID) + \" tb JOIN \"+dbschema+\".temp_periodicaldateswithequalperiod_\" + str(parameterID) + \" ld ON tb.Period = CONCAT(closing, '-', RIGHT(legacyperiod, 2)) WHERE EntityID_fk = \"+str(EntityID)+\" \" )\n",
    "\n",
    "        spark.sql(\"DELETE FROM \"+dbschema+\".temp_tblegacymatchPeriod_\" + str(parameterID) + \" WHERE LEFT(Period,6)=LEFT(LegacyPeriod,6) AND CAST(RIGHT(Period,2) AS INT)<>ABS(NumericValueofPeriod)\")\n",
    "\n",
    "        spark.sql(\"DELETE FROM \"+dbschema+\".temp_tblegacymatchPeriod_\" + str(parameterID) + \" WHERE LEFT(Period,6)<>LEFT(LegacyPeriod,6) AND CAST(RIGHT(Period,2) AS INT)<ABS(NumericValueofPeriod)\")\n",
    "\n",
    "        spark.sql(\"INSERT INTO \"+dbschema+\".temp_tblegacymatchPeriod1_\" + str(parameterID) + \" SELECT * FROM \"+dbschema+\".temp_tblegacymatchPeriod_\" + str(parameterID) + \"\")\n",
    "\n",
    "        spark.sql(\"DELETE FROM \"+dbschema+\".temp_tblegacymatchPeriod1_\" + str(parameterID) + \" WHERE LEFT(Period, 6) <> LEFT(LegacyPeriod, 6) AND CAST(RIGHT(Period, 2) AS INT) = ABS(NumericValueofPeriod)\")\n",
    "\n",
    "        spark.sql(\"INSERT INTO \"+dbschema+\".temp_priorityseq1_\" + str(parameterID) + \" SELECT tb.*, ld.FTPPeriod FROM \"+dbschema+\".temp_tballperiod_\" + str(parameterID) + \" tb JOIN (SELECT DISTINCT ls.legacyperiod, ls.periodtype, CONCAT( LEFT( DATE_ADD( MONTH, -CAST(RIGHT(ls.LegacyPeriod, 2) AS INT), to_date( CONCAT( RIGHT(LEFT(ls.LegacyPeriod, 6), 2), '-01-', LEFT(ls.LegacyPeriod, 4) ), 'MM-dd-yyyy' ) ), 4 ),RIGHT(LEFT(DATEADD(MONTH, -CAST(RIGHT(ls.LegacyPeriod, 2) AS INT), to_date( CONCAT( RIGHT(LEFT(ls.LegacyPeriod, 6), 2), '-01-', LEFT(ls.LegacyPeriod, 4) ), 'MM-dd-yyyy' ) ), 7), 2))  AS fromperiod, ls.FTPPeriod FROM \"+dbschema+\".temp_legacysddates_\" + str(parameterID) + \" ls LEFT JOIN \"+dbschema+\".temp_tblegacymatchPeriod1_\" + str(parameterID) + \" mp ON ls.LegacyPeriod = mp.LegacyPeriod AND ls.periodtype = mp.periodtype WHERE mp.LegacyPeriod IS NULL) ld ON LEFT(tb.Period, 6) <= LEFT(ld.LegacyPeriod, 6) AND LEFT(tb.Period, 6) > fromperiod AND tb.periodtype = ld.periodtype WHERE RIGHT(tb.Period, 2) <> RIGHT(ld.LegacyPeriod, 2)\")\n",
    "\n",
    "        spark.sql(\"INSERT INTO \"+dbschema+\".temp_priorityseq_\" + str(parameterID) + \" SELECT   Period,stdperiod,PeriodType,closing,legacyperiod,numericvalueofperiod, Row_number() OVER( partition BY stdperiod, periodtype ORDER BY Period DESC ) priorityseq, CASE WHEN RIGHT(Period, 2)>= '12' THEN 5 WHEN RIGHT(Period, 2)>= '09' THEN 4 WHEN RIGHT(Period, 2)= '06' THEN 3 WHEN RIGHT(Period, 2)= '03' THEN 2 WHEN RIGHT(Period, 2)= '01' THEN 1 ELSE 1 END, FTPPeriod FROM \"+dbschema+\".temp_priorityseq1_\" + str(parameterID) + \" ORDER BY periodtype\")\n",
    "        \n",
    "        spark.sql(\"CREATE TABLE \"+dbschema+\".temp_prioritylevel2temp_01_\" + str(parameterID) + \" as SELECT periodtype, ld.Period, ld.legacyperiod, ftpperiod, concat(pd.closing, CASE ftpperiod WHEN 5 THEN + '-12' WHEN 4 THEN + '-09' WHEN 3 THEN + '-06' WHEN 2 THEN + '-03' WHEN 1 THEN + '-01' END) closing FROM v_PeriodicalDates pd JOIN (SELECT tbm.periodtype, Max(tbm.Period) Period, ld.FinancialYear, ld.LegacyPeriod, CASE ftpperiod WHEN 1 THEN ftpperiod ELSE ftpperiod - 1 END AS ftpperiod, CONCAT( LEFT( DATE_ADD( MONTH, -CAST(RIGHT(ld.LegacyPeriod, 2) AS INT), to_date( CONCAT( RIGHT(LEFT(ld.LegacyPeriod, 6), 2), '-01-', LEFT(ld.LegacyPeriod, 4) ), 'MM-dd-yyyy' ) ), 4 ),RIGHT(LEFT(DATEADD(MONTH, -CAST(RIGHT(ld.LegacyPeriod, 2) AS INT), to_date( CONCAT( RIGHT(LEFT(ld.LegacyPeriod, 6), 2), '-01-', LEFT(ld.LegacyPeriod, 4) ), 'MM-dd-yyyy' ) ), 7), 2)) AS fromperiod FROM \"+dbschema+\".temp_legacysddates_\" + str(parameterID) + \" ld JOIN \"+dbschema+\".temp_tblegacymatchperiod_\" + str(parameterID) + \" tbm ON ld.periodtype = tbm.periodtype AND ld.legacyperiod = tbm.legacyperiod GROUP BY tbm.periodtype, ld.FinancialYear, ld.LegacyPeriod,ftpperiod) ld ON pd.closing > fromperiod AND pd.closing <= LEFT(LegacyPeriod, 6) AND pd.Order = ld.FTPPeriod AND pd.Year = ld.FinancialYear WHERE financialcycleid = \"+str(FiscalPeriodID)+\" AND ld.legacyperiod <> ld.Period\")\n",
    "\n",
    "        spark.sql(\"INSERT INTO \"+dbschema+\".temp_prioritylevel2temp_\" + str(parameterID) + \" SELECT tb.Period, LEFT(tb.Period, 6) AS stdperiod, periodtype, LEFT(tb.Period, 6) AS closing, legacyperiod, ftpperiod FROM \"+dbschema+\".temp_trialbalance_\" + str(parameterID) + \" tb JOIN \"+dbschema+\".temp_prioritylevel2temp_01_\" + str(parameterID) + \" pt ON tb.Period = pt.closing WHERE EntityID_fk = \"+str(EntityID)+\" \")\n",
    "\n",
    "        spark.sql(\"INSERT INTO \" + dbschema + \".temp_prioritylevel2_\" + str(parameterID) + \" SELECT *, Row_number() OVER( partition BY stdperiod, periodtype ORDER BY Period DESC ) priorityseq, CASE WHEN RIGHT(Period, 2)>= '12' THEN 5 WHEN RIGHT(Period, 2)= '09' THEN 4 WHEN RIGHT(Period, 2)= '06' THEN 3 WHEN RIGHT(Period, 2)= '03' THEN 2 WHEN RIGHT(Period, 2)= '01' THEN 1 ELSE 1 END AS order FROM \" + dbschema + \".temp_prioritylevel2temp_\" + str(parameterID) + \" ORDER BY periodtype\")\n",
    "\n",
    "        spark.sql(\"DELETE FROM \"+dbschema+\".temp_priorityseq_\" + str(parameterID) + \" WHERE priorityseq<>1\")\n",
    "\n",
    "        spark.sql(\"DELETE FROM \"+dbschema+\".temp_prioritylevel2_\" + str(parameterID) + \" WHERE priorityseq<>1\")\n",
    "\n",
    "        df_temp_001=spark.sql(\"SELECT DISTINCT PeriodType, max(order) order FROM \"+dbschema+\".temp_priorityseq_\" + str(parameterID) + \" GROUP BY PeriodType EXCEPT SELECT ps.periodtype, ps.order FROM \"+dbschema+\".temp_priorityseq_\" + str(parameterID) + \" ps JOIN \"+dbschema+\".temp_legacysddates_\" + str(parameterID) + \" ld ON ps.periodtype = ld.periodtype AND ps.legacyperiod = ld.legacyperiod AND ps.Order = ld.ftpperiod - 1\")\n",
    "\n",
    "        if df_temp_001.count()>0 :\n",
    "\n",
    "            spark.sql(\"DROP TABLE IF EXISTS \"+dbschema+\".temp_ctePS_\" + str(parameterID) + \"\")\n",
    "\n",
    "            spark.sql(\"CREATE TABLE \"+dbschema+\".temp_ctePS_\" + str(parameterID) + \" AS SELECT PeriodType,LegacyPeriod,max(Order) Order FROM \"+dbschema+\".temp_Priorityseq_\" + str(parameterID) + \" GROUP BY PeriodType,LegacyPeriod\")\n",
    "\n",
    "            spark.sql(\"MERGE INTO \" + dbschema + \".temp_legacysddates_\" + str(parameterID) + \" AS ld USING \" + dbschema + \".temp_ctePS_\" + str(parameterID) + \" AS ps ON ps.periodtype = ld.periodtype AND ps.legacyperiod = ld.legacyperiod AND ps.order <> ld.ftpperiod WHEN MATCHED THEN UPDATE SET ld.ftpperiod = ps.order + 1 \")\n",
    "\n",
    "        spark.sql(\"MERGE INTO \" + dbschema + \".temp_tblegacymatchperiod_\" + str(parameterID) + \" AS tbm USING \" + dbschema + \".temp_prioritylevel2_\" + str(parameterID) + \" AS pl ON tbm.periodtype = pl.periodtype AND tbm.legacyperiod = pl.legacyperiod WHEN MATCHED THEN DELETE\")\n",
    "\n",
    "        spark.sql(\"MERGE INTO \"+dbschema+\".temp_priorityseq_\" + str(parameterID) + \" AS ps USING \"+dbschema+\".temp_legacysddates_\" + str(parameterID) + \" AS ld ON ps.periodtype = ld.periodtype AND ps.legacyperiod = ld.legacyperiod AND ps.Order <> ld.ftpperiod - 1 WHEN MATCHED THEN DELETE\")\n",
    "\n",
    "        spark.sql(\"DROP TABLE IF EXISTS \"+dbschema+\".temp_bspriority_\" + str(parameterID) + \"\")\n",
    "\n",
    "        spark.sql(\"CREATE TABLE \"+dbschema+\".temp_bspriority_\" + str(parameterID) + \" AS SELECT PeriodType, legacyperiod, currentvalue, Max(Period) Period, Max(stdperiod) stdperiod FROM (SELECT ldd.PeriodType, ldd.legacyperiod, ldd.currentvalue, MAX(COALESCE(pseq.Period, pl2.Period, lm.Period)) AS Period, MAX(COALESCE(pseq.stdperiod, pl2.stdperiod, lm.stdperiod)) AS stdperiod FROM \"+dbschema+\".temp_legacysddates_\" + str(parameterID) + \" ldd LEFT JOIN \"+dbschema+\".temp_priorityseq_\" + str(parameterID) + \" pseq ON ldd.periodtype = pseq.PeriodType AND ldd.legacyperiod = pseq.legacyperiod LEFT JOIN \"+dbschema+\".temp_prioritylevel2_\" + str(parameterID) + \" pl2 ON ldd.periodtype = pl2.PeriodType AND ldd.legacyperiod = pl2.legacyperiod LEFT JOIN \"+dbschema+\".temp_tblegacymatchperiod_\" + str(parameterID) + \" lm ON ldd.PeriodType = lm.Periodtype AND ldd.legacyperiod = lm.legacyperiod GROUP BY ldd.PeriodType, ldd.legacyperiod, ldd.currentvalue) as a WHERE a.Period is not null GROUP BY PeriodType,legacyperiod,currentvalue\")\n",
    "\n",
    "        spark.sql(\"MERGE INTO \"+dbschema+\".temp_legacysddates_\" + str(parameterID) + \" AS ld USING \"+dbschema+\".temp_bspriority_\" + str(parameterID) + \" AS ps ON ld.periodtype = ps.PeriodType AND ld.legacyperiod = ps.legacyperiod WHEN MATCHED THEN UPDATE SET ld.LegacyPeriod = ps.Period, ld.CurrentValue = ps.stdperiod\")\n",
    "\n",
    "        spark.sql(\"MERGE INTO \"+dbschema+\".temp_tblegacymatchPeriod_\" + str(parameterID) + \" AS tb  USING ( SELECT * FROM ( SELECT * FROM \"+dbschema+\".temp_tblegacymatchPeriod_\" + str(parameterID) + \" WHERE LEFT(Period, 6) <> LEFT(LegacyPeriod, 6)  AND CAST(RIGHT(Period, 2) AS INT) = ABS(NumericValueofPeriod) ) a  WHERE NOT EXISTS ( SELECT LegacyPeriod  FROM \"+dbschema+\".temp_tblegacymatchPeriod_\" + str(parameterID) + \" b  WHERE a.LegacyPeriod = Period ) ) del  ON tb.LegacyPeriod = del.LegacyPeriod  WHEN MATCHED THEN DELETE\" )\n",
    "\n",
    "        if countofcomp == 0 or countofcomp  == 1 :\n",
    "            spark.sql(\"INSERT INTO \"+dbschema+\".temp_ytddates_\" + str(parameterID) + \" SELECT PeriodTYpe, LEFT(legacyperiod, 6), Period FROM \"+dbschema+\".temp_tblegacymatchperiod_\" + str(parameterID) + \" WHERE ( Periodtype LIKE '%YTD' OR Periodtype LIKE '%Year' ) UNION SELECT PeriodTYpe, LEFT(legacyperiod, 6), Period FROM \"+dbschema+\".temp_priorityseq_\" + str(parameterID) + \" WHERE ( Periodtype LIKE '%YTD' OR Periodtype LIKE '%Year' ) UNION SELECT PeriodTYpe, LEFT(legacyperiod, 6), Period FROM \"+dbschema+\".temp_prioritylevel2_\" + str(parameterID) + \" WHERE ( Periodtype LIKE '%YTD' OR Periodtype LIKE '%Year' )\")\n",
    "        \n",
    "        if countofcomp > 1 :\n",
    "            spark.sql(\"INSERT INTO \"+dbschema+\".temp_ytddates_\" + str(parameterID) + \" SELECT periodtype, pcurrentvalue, legacyperiod FROM \"+dbschema+\".temp_legacysddates_\" + str(parameterID) + \" WHERE periodtype NOT IN ('currentPeriod', 'PreviousPeriod') EXCEPT SELECT periodtype, pcurrentvalue, legacyperiod FROM (  SELECT periodtype, pcurrentvalue, legacyperiod  FROM \"+dbschema+\".temp_legacysddates_\" + str(parameterID) + \"  WHERE left(legacyperiod, 6) <> pcurrentvalue ) a WHERE NOT EXISTS (  SELECT legacyPeriod  FROM \"+dbschema+\".temp_tblegacymatchPeriod_\" + str(parameterID) + \" b  WHERE a.legacyPeriod = Period  AND a.periodtype = PeriodType ) UNION SELECT PeriodTYpe, LEFT(legacyperiod, 6), Period FROM \"+dbschema+\".temp_priorityseq_\" + str(parameterID) + \" UNION SELECT PeriodTYpe, LEFT(legacyperiod, 6), Period FROM \"+dbschema+\".temp_prioritylevel2_\" + str(parameterID) + \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8c61d4a4-fbbc-4454-9543-d04c6877665e",
     "showTitle": true,
     "title": "Extended Financial Cycle Changes"
    }
   },
   "outputs": [],
   "source": [
    "if extflag==1 and Extendedmonthdiff<=12 :\n",
    "\n",
    "    spark.sql(\"MERGE INTO \"+dbschema+\".temp_legacysddates_\" + str(parameterID) + \" AS ld USING v_PeriodicalDatesForExtensionPeriod AS pd ON ld.currentvalue = pd.closing AND pd.Value = ld.Value AND financialyear = '\"+str(Extendedcycleyear)+\"' AND entityid = \"+str(EntityID)+\" WHEN MATCHED THEN UPDATE SET  p = '', ppp = '', py = '', pypp = '', ppy = '', p3y = '', p4y = '' \")\n",
    "\n",
    "    spark.sql(\"MERGE INTO \"+dbschema+\".temp_legacysddates_\" + str(parameterID) + \" AS t USING ( SELECT MIN(currentvalue) AS min_currentvalue FROM \"+dbschema+\".temp_legacysddates_\" + str(parameterID) + \" WHERE p = '' ) AS s ON t.currentvalue < s.min_currentvalue WHEN MATCHED THEN DELETE\")\n",
    "\n",
    "if extflag==1 and Extendedmonthdiff > 12 :\n",
    "    extopen=\"\"\n",
    "    extclose=\"\"\n",
    "    monthdiff=0\n",
    "    NumericalValueofExtPeriodType=0\n",
    "    exttype=\"\"\n",
    "    ExtValue=spark.sql(\"SELECT SUBSTRING('\"+str(PeriodValue)+\"',1,CHARINDEX('-','\"+str(PeriodValue)+\"')-1) as ExtValue\").collect()[0][0]\n",
    "\n",
    "    spark.sql(\"MERGE INTO \"+dbschema+\".temp_legacysddates_\" + str(parameterID) + \" AS ld  USING v_PeriodicalDatesForExtensionPeriod pd  ON pd.closing = ld.Pcurrentvalue AND pd.Value = ld.Value AND financialcycleid = \"+str(FiscalPeriodID)+\" AND entityid = \"+str(EntityID)+\" AND financialyear = '\"+str(Extendedcycleyear)+\"' WHEN MATCHED THEN  UPDATE SET Value = pd.Value, financialyear =  pd.Year, openingperiodvalue = pd.opening, CurrentValue = PcurrentValue \")\n",
    "\n",
    "    ext_df=spark.sql(\"SELECT opening,closing,Year FROM v_PeriodicalDatesForExtensionPeriod WHERE entityid = \"+str(EntityID)+\" and Value = '\"+str(ExtValue)+\"' and financialcycleid=\"+str(FiscalPeriodID)+\" \")\n",
    "\n",
    "    if ext_df.count()>0 : \n",
    "        extopen=ext_df.collect()[0][0]\n",
    "        extclose=ext_df.collect()[0][0]\n",
    "        Extendedcycleyear=ext_df.collect()[0][0]\n",
    "\n",
    "    spark.sql(\"MERGE INTO \"+dbschema+\".temp_legacysddates_\" + str(parameterID) + \" AS ld USING v_PeriodicalDatesForExtensionPeriod AS pd ON ld.currentvalue = pd.closing AND pd.Value = ld.Value and entityid = \"+str(EntityID)+\" and  financialyear='\"+str(Extendedcycleyear)+\"' WHEN MATCHED THEN UPDATE SET openingperiodvalue = '\"+str(extopen)+\"' , CastingPeriod = concat(ld.Value,'-','\"+str(extopen)+\"','-',currentvalue),legacyperiod = CASE WHEN ld.Value IN ('Q1', 'H1', 'Year', '9M') THEN CASE WHEN LENGTH(DATE_DIFF(MONTH,to_date(CONCAT('01-', RIGHT(opening, 2), '-', LEFT(opening, 4)), 'dd-MM-yyyy'),to_date(CONCAT('01-', RIGHT(closing, 2), '-', LEFT(closing, 4)), 'dd-MM-yyyy'))+1) = 1 THEN CONCAT(pcurrentvalue , '-0',DATE_DIFF(MONTH,to_date(concat('01-',RIGHT(opening, 2),'-',LEFT(opening, 4)),'dd-MM-yyyy'),to_date(concat('01-',RIGHT(closing, 2),'-',LEFT(closing, 4)),'dd-MM-yyyy'))+1)  ELSE CONCAT(pcurrentvalue , '-',DATE_DIFF(MONTH,to_date(concat('01-',RIGHT(opening, 2),'-',LEFT(opening, 4)),'dd-MM-yyyy'),to_date(concat('01-',RIGHT(closing, 2),'-',LEFT(closing, 4)),'dd-MM-yyyy'))+1) END WHEN ld.Value IN ('H2') THEN concat(currentvalue , '-' ,'06') WHEN ld.Value IN ('Q2', 'Q3', 'Q4') THEN concat(currentvalue, '-' ,'03') ELSE legacyperiod END , p = '', ppp = '', py = '', pypp = '', ppy = '', p3y = '', p4y = '' \")\n",
    "\n",
    "    spark.sql(\"DELETE FROM \" + dbschema + \".temp_legacysddates_\" + str(parameterID) + \" WHERE currentvalue < (SELECT MIN(currentvalue) FROM \" + dbschema + \".temp_legacysddates_\" + str(parameterID) + \" WHERE p = '')\")\n",
    "\n",
    "    spark.sql(\"UPDATE \"+dbschema+\".temp_legacysddates_\" + str(parameterID) + \" SET numericvalueofperiod = -CAST(RIGHT(LegacyPeriod, 2) AS INT)\")\n",
    "\n",
    "    ################## Handle YTD calc for ext financial cycle ##################\n",
    "\n",
    "    queryString=\"DROP TABLE IF EXISTS {vSQLDB}.temp_legacyMatchPeriodFOrLargerFTP;DROP TABLE IF EXISTS {vSQLDB}.temp_legacyMatchPeriodFOrEqualFTP;DROP TABLE IF EXISTS {vSQLDB}.temp_numperiod;DROP TABLE IF EXISTS {vSQLDB}.temp_cteorder;\" \n",
    "    \n",
    "    queryString=queryString.replace(\"{vSQLDB}\",f\"{dbschema}\")\n",
    "    \n",
    "    sqlwarehouse_queryExecutor(queryString)\n",
    "\n",
    "    spark.sql(\"CREATE TABLE \"+dbschema+\".temp_numperiod_\" + str(parameterID) + \" ( PeriodType VARCHAR(50), LegacyPeriod VARCHAR(20), numericvalueofperiod INT )\")\n",
    "\n",
    "    if Extendedcycleyear == FinancialYear :\n",
    "\n",
    "        queryString=\"TRUNCATE TABLE {vSQLDB}.temp_priorityseq_\" + str(parameterID) + \";TRUNCATE TABLE {vSQLDB}.temp_prioritylevel2_\" + str(parameterID) + \";TRUNCATE TABLE {vSQLDB}.temp_priorityseq1_\" + str(parameterID) + \";TRUNCATE TABLE {vSQLDB}.temp_tballperiod_\" + str(parameterID) + \";TRUNCATE TABLE {vSQLDB}.temp_tblegacymatchperiod_\" + str(parameterID) + \";TRUNCATE TABLE  {vSQLDB}.temp_tblegacymatchPeriod1_\" + str(parameterID) + \";\"\n",
    "        \n",
    "        queryString=queryString.replace(\"{vSQLDB}\",f\"{dbschema}\")\n",
    "        \n",
    "        sqlwarehouse_queryExecutor(queryString)\n",
    "\n",
    "        spark.sql(\"CREATE TABLE  \"+dbschema+\".temp_legacyMatchPeriodFOrLargerFTP_\" + str(parameterID) + \" AS SELECT DISTINCT PeriodType, closing, LegacyPeriod, numericvalueofperiod FROM \"+dbschema+\".temp_legacysddates_\" + str(parameterID) + \" ld JOIN v_PeriodicalDatesForExtensionPeriod mp ON mp.closing <= ld.currentvalue AND mp.opening >= ld.openingperiodvalue AND ld.FTPPeriod >= mp.Order WHERE entityid = \"+str(EntityID)+\"  and financialcycleid=\"+str(FiscalPeriodID)+\" \")\n",
    "\n",
    "        spark.sql(\"CREATE TABLE \"+dbschema+\".temp_legacyMatchPeriodFOrEqualFTP_\" + str(parameterID) + \" AS SELECT DISTINCT PeriodType, closing, LegacyPeriod, numericvalueofperiod FROM \"+dbschema+\".temp_legacysddates_\" + str(parameterID) + \" ld JOIN v_PeriodicalDatesForExtensionPeriod mp ON mp.closing <= ld.currentvalue AND mp.opening >= ld.openingperiodvalue AND ld.FTPPeriod = mp.Order WHERE entityid = \"+str(EntityID)+\"  and financialcycleid=\"+str(FiscalPeriodID)+\" \")\n",
    "\n",
    "        spark.sql(\"INSERT INTO \"+dbschema+\".temp_tballperiod_\" + str(parameterID) + \" SELECT tb.Period, LEFT(tb.Period, 6) AS stdperiod,ld.*,\"+alldimcols+\"  FROM \"+dbschema+\".temp_trialbalance_\" + str(parameterID) + \" tb JOIN \"+dbschema+\".temp_legacyMatchPeriodFOrLargerFTP_\" + str(parameterID) + \" ld ON LEFT(tb.Period, 6)= ld.closing WHERE EntityID_fk = \"+str(EntityID)+\" \")\n",
    "\n",
    "        spark.sql(\"INSERT INTO \"+dbschema+\".temp_tblegacymatchperiod_\" + str(parameterID) + \" SELECT tb.Period, LEFT(tb.Period, 6) AS stdperiod,ld.*,\"+alldimcols+\"  FROM \"+dbschema+\".temp_trialbalance_\" + str(parameterID) + \" tb JOIN \"+dbschema+\".temp_legacyMatchPeriodFOrEqualFTP_\" + str(parameterID) + \" ld ON LEFT(tb.Period, 6)= ld.closing WHERE EntityID_fk = \"+str(EntityID)+\" \")\n",
    "\n",
    "        spark.sql(\"INSERT INTO \"+dbschema+\".temp_numperiod_\" + str(parameterID) + \" SELECT periodtype, LegacyPeriod, MAX(numericvalueofperiod) AS numericvalueofperiod FROM ( SELECT ld.periodtype, md.LegacyPeriod, CASE LEFT(Value, 1) WHEN 'Y' THEN INT(\"+str(Extendedmonthdiff)+\") WHEN '9' THEN INT(\"+str(Extendedmonthdiff)+\") - 3 WHEN 'H' THEN INT(\"+str(Extendedmonthdiff)+\") - 6 WHEN 'Q' THEN INT(\"+str(Extendedmonthdiff)+\") - 9 ELSE 1 END AS numericvalueofperiod FROM \"+dbschema+\".temp_legacysddates_\" + str(parameterID) + \" ld JOIN \"+dbschema+\".temp_tblegacymatchPeriod_\" + str(parameterID) + \" md ON ld.periodtype = md.PeriodType AND ld.legacyperiod = md.LegacyPeriod ) a GROUP BY periodtype, LegacyPeriod\")\n",
    "\n",
    "        spark.sql(\"DELETE FROM \"+dbschema+\".temp_tblegacymatchPeriod_\" + str(parameterID) + \" WHERE LEFT(Period,6) = LEFT(LegacyPeriod,6) AND CAST(RIGHT(Period,2) AS INT) <> ABS(NumericValueofPeriod)\")\n",
    "\n",
    "        spark.sql(\"DELETE FROM \"+dbschema+\".temp_tblegacymatchPeriod_\" + str(parameterID) + \" WHERE LEFT(Period,6)<>LEFT(LegacyPeriod,6) AND CAST(RIGHT(Period,2) AS INT) < ABS(NumericValueofPeriod)\")\n",
    "\n",
    "        spark.sql(\"MERGE INTO \"+dbschema+\".temp_tblegacymatchPeriod_\" + str(parameterID) + \" tb1 USING \"+dbschema+\".temp_numperiod_\" + str(parameterID) + \" nm ON tb1.PeriodType = nm.periodtype AND tb1.LegacyPeriod = nm.LegacyPeriod and LEFT(tb1.Period, 6) <> LEFT(tb1.LegacyPeriod, 6) AND CAST(RIGHT(tb1.Period, 2) AS INT) > ABS(nm.numericvalueofperiod) WHEN MATCHED THEN DELETE\")\n",
    "\n",
    "        spark.sql(\"INSERT INTO \"+dbschema+\".temp_priorityseq1_\" + str(parameterID) + \" SELECT tb.*,ld.ftpperiod FROM \"+dbschema+\".temp_tballperiod_\" + str(parameterID) + \" tb JOIN ( SELECT DISTINCT ls.legacyperiod, ls.periodtype, CONCAT( LEFT( DATE_ADD( MONTH, -CAST(RIGHT(ls.LegacyPeriod, 2) AS INT), to_date( CONCAT( RIGHT(LEFT(ls.LegacyPeriod, 6), 2), '-01-', LEFT(ls.LegacyPeriod, 4) ), 'MM-dd-yyyy' ) ), 4 ),RIGHT(LEFT(DATEADD(MONTH, -CAST(RIGHT(ls.LegacyPeriod, 2) AS INT), to_date( CONCAT( RIGHT(LEFT(ls.LegacyPeriod, 6), 2), '-01-', LEFT(ls.LegacyPeriod, 4) ), 'MM-dd-yyyy' ) ), 7), 2)) AS fromperiod ,ls.FTPPeriod FROM \"+dbschema+\".temp_legacysddates_\" + str(parameterID) + \" ls LEFT JOIN \"+dbschema+\".temp_tblegacymatchperiod_\" + str(parameterID) + \" mp ON ls.LegacyPeriod = mp.LegacyPeriod AND ls.periodtype = mp.periodtype WHERE mp.LegacyPeriod IS NULL  ) ld ON LEFT(tb.Period, 6)<= LEFT(ld.LegacyPeriod, 6) AND LEFT(tb.Period, 6)> fromperiod AND tb.periodtype = ld.periodtype WHERE RIGHT(tb.Period, 2)<> RIGHT(ld.LegacyPeriod, 2) and CAST(RIGHT(tb.Period, 2) AS INT)<=ABS(NumericValueofPeriod)\")\n",
    "\n",
    "        spark.sql(\"INSERT INTO \"+dbschema+\".temp_priorityseq_\" + str(parameterID) + \" SELECT Period,stdperiod,PeriodType,closing,legacyperiod,numericvalueofperiod, Row_number() OVER( partition BY stdperiod, periodtype ORDER BY Period DESC ) priorityseq, 0, ftpperiod FROM \"+dbschema+\".temp_priorityseq1_\" + str(parameterID) + \" ORDER BY periodtype\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cd712d09-c370-47ed-a241-d2636226834b",
     "showTitle": true,
     "title": "Populate Order for ext cycle periods"
    }
   },
   "outputs": [],
   "source": [
    "if extflag==1 and Extendedmonthdiff > 12 :\n",
    "    if Extendedcycleyear == FinancialYear :\n",
    "        queryString=\"DROP TABLE IF EXISTS {vSQLDB}.temp_cteorder_\" + str(parameterID) + \";DROP TABLE IF EXISTS {vSQLDB}.temp_prioritylevel2temp_01_\" + str(parameterID) + \";\" \n",
    "        \n",
    "        queryString=queryString.replace(\"{vSQLDB}\",f\"{dbschema}\")\n",
    "        \n",
    "        sqlwarehouse_queryExecutor(queryString)\n",
    "\n",
    "        spark.sql(\"CREATE TABLE  \"+dbschema+\".temp_cteorder_\" + str(parameterID) + \" AS SELECT ps.Period, ps.PeriodType, mp.closing, ps.legacyperiod, priorityseq, max(mp.order) AS Order FROM (SELECT *, right(left(to_date(concat('01-',substring ( Period,charindex(' ',Period)+1, case when charindex('-',Period)=0 then len(Period)-charindex(' ',Period) else charindex('-',Period)-charindex(' ',Period)-1 end ),'-',left(closing,4)), 'dd-MM-yyyy'),7),2) FromPeriod FROM v_periodicaldatesforextensionperiod WHERE entityid=\"+str(EntityID)+\" ) mp JOIN (SELECT *, CONCAT( LEFT( DATE_ADD( MONTH, -CAST(RIGHT(Period, 2)AS INT)+1, to_date( CONCAT( RIGHT(LEFT(Period, 6), 2), '-01-', LEFT(Period, 4) ), 'MM-dd-yyyy' ) ), 4 ),RIGHT(LEFT(DATEADD(MONTH, -CAST(RIGHT(Period, 2) AS INT)+1, to_date( CONCAT( RIGHT(LEFT(Period, 6), 2), '-01-', LEFT(Period, 4) ), 'MM-dd-yyyy' ) ), 7), 2))  AS fromperiod FROM \"+dbschema+\".temp_priorityseq_\" + str(parameterID) + \" ) ps ON ps.closing = mp.closing AND ps.FTPPeriod > mp.order AND ps.fromperiod = left(ps.FromPeriod,4) + mp.FromPeriod WHERE financialcycleid =\"+str(FiscalPeriodID)+\" AND entityid = \"+str(EntityID)+\" GROUP BY ps.Period, ps.PeriodType, mp.closing, ps.legacyperiod, priorityseq\")\n",
    "        \n",
    "        spark.sql(\"MERGE INTO \"+dbschema+\".temp_priorityseq_\" + str(parameterID) + \" AS ps USING \"+dbschema+\".temp_cteorder_\" + str(parameterID) + \" AS cte ON ps.Period = cte.Period WHEN MATCHED THEN UPDATE SET ps.order = cte.order\")\n",
    "        \n",
    "        spark.sql(\"DROP TABLE IF EXISTS \"+dbschema+\".temp_prioritylevel2temp_01_\" + str(parameterID) + \"\")\n",
    "        \n",
    "        spark.sql(\"CREATE TABLE \"+dbschema+\".temp_prioritylevel2temp_01_\" + str(parameterID) + \" AS SELECT DISTINCT periodtype, ld.Period, ld.legacyperiod, ftpperiod, ld.closing, CASE WHEN ld.Value IN ('H1', 'Q1', 'Year', '9M') THEN Datediff(month, TO_DATE(CONCAT('01', '-', RIGHT(FromOpeningPeriod, 2), '-', LEFT(FromOpeningPeriod, 4)), 'dd-MM-yyyy'), TO_DATE(CONCAT('01', '-', RIGHT(closing, 2), '-', LEFT(closing, 4)), 'dd-MM-yyyy')) + 1 WHEN ld.Value NOT IN ('H1', 'Q1', 'Year', '9M') THEN CASE WHEN LEFT(ld.Value, 1)='9' THEN '09' WHEN LEFT(ld.Value, 1)='H' THEN '06' WHEN LEFT(ld.Value, 1)='Q' THEN '03' WHEN LEFT(ld.Value, 1)='M' THEN '01' END END as monthdiff FROM ( SELECT ld.PeriodType, ld.Period, ld.FinancialYear, ld.LegacyPeriod, ld.ftpperiod, pd.Value, pd.closing, CONCAT(left(FromPeriod, 4),right(left(to_date(concat('01-',substring ( pd.Period,charindex(' ',pd.Period)+1, case when charindex('-',pd.Period)=0 then len(pd.Period)-charindex(' ',pd.Period) else charindex('-',pd.Period)-charindex(' ',pd.Period)-1 end ),'-',left(closing,4)), 'dd-MM-yyyy'),7),2)) as FromOpeningPeriod FROM v_PeriodicalDatesForExtensionPeriod as pd JOIN ( SELECT tbm.periodtype, MAX(tbm.Period) as Period, ld.FinancialYear, ld.LegacyPeriod, CASE WHEN ftpperiod = 1 THEN ftpperiod ELSE ftpperiod - 1 END as ftpperiod, CONCAT( LEFT( DATE_ADD( MONTH, -CAST(RIGHT(ld.LegacyPeriod, 2)AS INT), to_date( CONCAT( RIGHT(LEFT(ld.LegacyPeriod, 6), 2), '-01-', LEFT(ld.LegacyPeriod, 4) ), 'MM-dd-yyyy' ) ), 4 ),RIGHT(LEFT(DATEADD(MONTH, -CAST(RIGHT(ld.LegacyPeriod, 2) AS INT), to_date( CONCAT( RIGHT(LEFT(ld.LegacyPeriod, 6), 2), '-01-', LEFT(ld.LegacyPeriod, 4) ), 'MM-dd-yyyy' ) ), 7), 2))  AS fromperiod FROM \"+dbschema+\".temp_legacysddates_\" + str(parameterID) + \" as ld JOIN \"+dbschema+\".temp_tblegacymatchperiod_\" + str(parameterID) + \" as tbm ON ld.periodtype = tbm.periodtype AND ld.legacyperiod = tbm.legacyperiod GROUP BY tbm.periodtype, ld.FinancialYear, ld.LegacyPeriod, ftpperiod ) as ld ON pd.closing > fromperiod AND pd.closing <= LEFT(LegacyPeriod, 6) AND pd.Order = ld.ftpperiod AND pd.Year = ld.FinancialYear WHERE financialcycleid = \"+str(FiscalPeriodID)+\" and Entityid = \"+str(EntityID)+\"  ) as ld \")\n",
    "\n",
    "        spark.sql(\"INSERT INTO \"+dbschema+\".temp_prioritylevel2temp_\" + str(parameterID) + \" SELECT tb.Period, LEFT(tb.Period, 6) AS stdperiod, periodtype, LEFT(tb.Period, 6) AS closing, legacyperiod, ftpperiod,\"+alldimcols+\" FROM \"+dbschema+\".temp_trialbalance_\" + str(parameterID) + \" tb JOIN \"+dbschema+\".temp_prioritylevel2temp_01_\" + str(parameterID) + \" pt ON LEFT(tb.Period,6) = pt.closing  and ABS(RIGHT(tb.Period, 2) )= monthdiff WHERE EntityID_fk = \"+str(EntityID)+\" \")\n",
    "        \n",
    "        spark.sql(\"INSERT INTO \" + dbschema + \".temp_prioritylevel2_\" + str(parameterID) + \" SELECT *, Row_number() OVER( partition BY stdperiod, periodtype ORDER BY Period DESC ) priorityseq, 0 as order FROM \" + dbschema + \".temp_prioritylevel2temp_\" + str(parameterID) + \" ORDER BY periodtype\")\n",
    "\n",
    "        spark.sql(\"DELETE FROM \"+dbschema+\".temp_priorityseq_\" + str(parameterID) + \" WHERE priorityseq<>1\")\n",
    "\n",
    "        spark.sql(\"DELETE FROM \"+dbschema+\".temp_prioritylevel2_\" + str(parameterID) + \" WHERE priorityseq<>1\")\n",
    "\n",
    "        df_temp_002=spark.sql(\"SELECT DISTINCT PeriodType, max(order) order FROM \"+dbschema+\".temp_priorityseq_\" + str(parameterID) + \" GROUP BY PeriodType EXCEPT SELECT ps.periodtype, ps.order FROM \"+dbschema+\".temp_priorityseq_\" + str(parameterID) + \" ps JOIN \"+dbschema+\".temp_legacysddates_\" + str(parameterID) + \" ld ON ps.periodtype = ld.periodtype AND ps.legacyperiod = ld.legacyperiod AND ps.Order = ld.ftpperiod - 1\")\n",
    "\n",
    "        if df_temp_002.count()>0 :\n",
    "\n",
    "            spark.sql(\"DROP TABLE IF EXISTS \"+dbschema+\".temp_ctePS_\" + str(parameterID) + \"\")\n",
    "\n",
    "            spark.sql(\"CREATE TABLE \"+dbschema+\".temp_ctePS_\" + str(parameterID) + \" AS SELECT PeriodType,LegacyPeriod,max(Order) Order FROM \"+dbschema+\".temp_Priorityseq_\" + str(parameterID) + \" GROUP BY PeriodType,LegacyPeriod\")\n",
    "            \n",
    "            spark.sql(\"MERGE INTO \" + dbschema + \".temp_legacysddates_\" + str(parameterID) + \" AS ld USING \" + dbschema + \".temp_ctePS_\" + str(parameterID) + \" AS ps ON ps.periodtype = ld.periodtype AND ps.legacyperiod = ld.legacyperiod AND ps.order <> ld.ftpperiod WHEN MATCHED THEN UPDATE SET ld.ftpperiod = ps.order + 1 \")\n",
    "\n",
    "        spark.sql(\"DROP TABLE IF EXISTS \"+dbschema+\".temp_deleteplev2_\" + str(parameterID) + \"\")\n",
    "        \n",
    "        spark.sql(\"CREATE TABLE  \"+dbschema+\".temp_deleteplev2_\" + str(parameterID) + \" AS SELECT LegacyPeriod,PeriodType from \"+dbschema+\".temp_tblegacymatchPeriod_\" + str(parameterID) + \" where Period=LegacyPeriod\")\n",
    "\n",
    "        spark.sql(\"MERGE INTO \"+dbschema+\".temp_prioritylevel2_\" + str(parameterID) + \" AS pl USING \"+dbschema+\".temp_deleteplev2_\" + str(parameterID) + \" AS tbm ON tbm.periodtype = pl.periodtype AND tbm.legacyperiod = pl.legacyperiod WHEN MATCHED THEN DELETE\")\n",
    "        \n",
    "        spark.sql(\"MERGE INTO \"+dbschema+\".temp_tblegacymatchperiod_\" + str(parameterID) + \" AS tbm USING \"+dbschema+\".temp_prioritylevel2_\" + str(parameterID) + \" AS pl ON tbm.periodtype = pl.periodtype AND tbm.legacyperiod = pl.legacyperiod WHEN MATCHED THEN DELETE\")\n",
    "        \n",
    "        spark.sql(\"MERGE INTO \"+dbschema+\".temp_priorityseq_\" + str(parameterID) + \" AS ps USING \"+dbschema+\".temp_legacysddates_\" + str(parameterID) + \" AS ld ON ps.periodtype = ld.periodtype AND ps.legacyperiod = ld.legacyperiod AND ps.order <> ld.ftpperiod - 1 WHEN MATCHED THEN DELETE\")\n",
    "\n",
    "        spark.sql(\"DROP TABLE IF EXISTS \"+dbschema+\".temp_bspriority_\" + str(parameterID) + \"\")\n",
    "        \n",
    "        spark.sql(\"CREATE TABLE \"+dbschema+\".temp_bspriority_\" + str(parameterID) + \" AS SELECT PeriodType, legacyperiod, currentvalue, MAX(Period) AS Period, MAX(stdperiod) AS stdperiod FROM ( SELECT ld.PeriodType, ld.legacyperiod, ld.currentvalue, MAX(ps.Period) AS Period, MAX(ps.stdperiod) AS stdperiod FROM \"+dbschema+\".temp_legacysddates_\" + str(parameterID) + \" ld JOIN \"+dbschema+\".temp_priorityseq_\" + str(parameterID) + \" ps ON ld.periodtype = ps.PeriodType AND ld.legacyperiod = ps.legacyperiod GROUP BY ld.PeriodType, ld.legacyperiod, ld.currentvalue UNION SELECT ld.PeriodType, ld.legacyperiod, ld.currentvalue, MAX(ps.Period) AS Period, MAX(ps.stdperiod) AS stdperiod FROM \"+dbschema+\".temp_legacysddates_\" + str(parameterID) + \" ld JOIN \"+dbschema+\".temp_prioritylevel2_\" + str(parameterID) + \" ps ON ld.periodtype = ps.PeriodType AND ld.legacyperiod = ps.legacyperiod GROUP BY ld.PeriodType, ld.legacyperiod, ld.currentvalue UNION SELECT PeriodType, legacyperiod, LEFT(legacyperiod, 6), MAX(Period), MAX(stdperiod) FROM \"+dbschema+\".temp_tblegacymatchperiod_\" + str(parameterID) + \" GROUP BY PeriodType, legacyperiod, legacyperiod ) a GROUP BY PeriodType, legacyperiod, currentvalue \")\n",
    "\n",
    "        spark.sql(\"MERGE INTO \"+dbschema+\".temp_legacysddates_\" + str(parameterID) + \" AS ld USING \"+dbschema+\".temp_bspriority_\" + str(parameterID) + \" AS ps ON ld.periodtype = ps.PeriodType AND ld.legacyperiod = ps.legacyperiod WHEN MATCHED THEN UPDATE SET ld.LegacyPeriod = ps.Period, ld.CurrentValue = ps.stdperiod\")\n",
    "\n",
    "        spark.sql(\"MERGE INTO \"+dbschema+\".temp_tblegacymatchPeriod_\" + str(parameterID) + \" AS tb  USING ( SELECT * FROM ( SELECT * FROM \"+dbschema+\".temp_tblegacymatchPeriod_\" + str(parameterID) + \" WHERE LEFT(Period, 6) <> LEFT(LegacyPeriod, 6) ) a  WHERE NOT EXISTS ( SELECT LegacyPeriod  FROM \"+dbschema+\".temp_tblegacymatchPeriod_\" + str(parameterID) + \" b  WHERE a.LegacyPeriod = Period ) ) del  ON tb.LegacyPeriod = del.LegacyPeriod  WHEN MATCHED THEN DELETE\" )\n",
    "\n",
    "        spark.sql(\"TRUNCATE TABLE \"+dbschema+\".temp_ytddates_\" + str(parameterID) + \"\")\n",
    "\n",
    "        if countofcomp == 0 or countofcomp  == 1 :\n",
    "            spark.sql(\"INSERT INTO \"+dbschema+\".temp_ytddates_\" + str(parameterID) + \" SELECT PeriodTYpe, LEFT(legacyperiod, 6), Period FROM \"+dbschema+\".temp_tblegacymatchperiod_\" + str(parameterID) + \" WHERE ( Periodtype LIKE '%YTD' OR Periodtype LIKE '%Year' ) UNION SELECT PeriodTYpe, LEFT(legacyperiod, 6), Period FROM \"+dbschema+\".temp_priorityseq_\" + str(parameterID) + \" WHERE ( Periodtype LIKE '%YTD' OR Periodtype LIKE '%Year' ) UNION SELECT PeriodTYpe, LEFT(legacyperiod, 6), Period FROM \"+dbschema+\".temp_prioritylevel2_\" + str(parameterID) + \" WHERE ( Periodtype LIKE '%YTD' OR Periodtype LIKE '%Year' )\")\n",
    "        \n",
    "        if countofcomp > 1 :\n",
    "            spark.sql(\"INSERT INTO \"+dbschema+\".temp_ytddates_\" + str(parameterID) + \" SELECT periodtype, pcurrentvalue, legacyperiod FROM \"+dbschema+\".temp_legacysddates_\" + str(parameterID) + \" WHERE periodtype NOT IN ('currentPeriod', 'PreviousPeriod') EXCEPT SELECT periodtype, pcurrentvalue, legacyperiod FROM (  SELECT periodtype, pcurrentvalue, legacyperiod  FROM \"+dbschema+\".temp_legacysddates_\" + str(parameterID) + \"  WHERE left(legacyperiod, 6) <> pcurrentvalue ) a WHERE NOT EXISTS (  SELECT legacyPeriod  FROM \"+dbschema+\".temp_tblegacymatchPeriod_\" + str(parameterID) + \" b  WHERE a.legacyPeriod = Period  AND a.periodtype = PeriodType ) UNION SELECT PeriodTYpe, LEFT(legacyperiod, 6), Period FROM \"+dbschema+\".temp_priorityseq_\" + str(parameterID) + \" UNION SELECT PeriodTYpe, LEFT(legacyperiod, 6), Period FROM \"+dbschema+\".temp_prioritylevel2_\" + str(parameterID) + \" UNION select periodtype,pcurrentvalue,period from ( select periodtype, LEFT(legacyperiod, 6) as pcurrentvalue,period  from  \"+dbschema+\".temp_tblegacymatchperiod_\" + str(parameterID) + \" except select  periodtype, pcurrentvalue,legacyperiod  from \"+dbschema+\".temp_legacysddates_\" + str(parameterID) + \") b \")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "58c2ca11-467f-47fb-b328-10d34fa5ee5f",
     "showTitle": true,
     "title": "Not extended year"
    }
   },
   "outputs": [],
   "source": [
    "if extflag==1 and Extendedmonthdiff > 12 :\n",
    "    if Extendedcycleyear != FinancialYear :\n",
    "        \n",
    "        queryString=\"TRUNCATE TABLE {vSQLDB}.temp_priorityseq1_\" + str(parameterID) + \";TRUNCATE TABLE {vSQLDB}.temp_tballperiod_\" + str(parameterID) + \";TRUNCATE TABLE {vSQLDB}.temp_tblegacymatchperiod_\" + str(parameterID) + \";TRUNCATE TABLE {vSQLDB}.temp_prioritylevel2temp_\" + str(parameterID) + \";\"\n",
    "        \n",
    "        queryString=queryString.replace(\"{vSQLDB}\",f\"{dbschema}\")\n",
    "        \n",
    "        sqlwarehouse_queryExecutor(queryString)\n",
    "        \n",
    "        spark.sql(\"MERGE INTO \"+dbschema+\".temp_priorityseq_\" + str(parameterID) + \" AS ps USING \"+dbschema+\".temp_legacysddates_\" + str(parameterID) + \" AS ld ON ps.periodtype = ld.periodtype AND financialyear='\"+str(Extendedcycleyear)+\"'  WHEN MATCHED THEN DELETE\")\n",
    "\n",
    "        spark.sql(\"MERGE INTO \"+dbschema+\".temp_prioritylevel2_\" + str(parameterID) + \" AS ps USING \"+dbschema+\".temp_legacysddates_\" + str(parameterID) + \" AS ld ON ps.periodtype = ld.periodtype AND financialyear='\"+str(Extendedcycleyear)+\"'  WHEN MATCHED THEN DELETE\")\n",
    "\n",
    "        spark.sql(\"SELECT DISTINCT Period, LEFT(Period, 6) AS stdperiod, ld.* FROM \"+dbschema+\".temp_trialbalance_\" + str(parameterID) + \" tb JOIN ( SELECT DISTINCT PeriodType, closing, LegacyPeriod, numericvalueofperiod FROM \"+dbschema+\".temp_legacysddates_\" + str(parameterID) + \" ld JOIN v_PeriodicalDatesForExtensionPeriod mp ON mp.closing <= ld.currentvalue AND mp.opening >= ld.openingperiodvalue AND ld.FTPPeriod >= mp.Order WHERE financialcycleid = \"+str(FiscalPeriodID)+\" AND Entityid=\"+str(EntityID)+\") ld ON LEFT(tb.Period, 6) = ld.closing  WHERE EntityID_fk = \"+str(EntityID)+\" \")\n",
    "\n",
    "        spark.sql(\"INSERT INTO \"+dbschema+\".temp_tblegacymatchperiod_\" + str(parameterID) + \" SELECT Period, LEFT(Period, 6) AS stdperiod, ld.*,\"+alldimcols+\" FROM \"+dbschema+\".temp_trialbalance_\" + str(parameterID) + \" tb JOIN ( SELECT DISTINCT PeriodType, closing, LegacyPeriod, numericvalueofperiod FROM \"+dbschema+\".temp_legacysddates_\" + str(parameterID) + \" ld JOIN v_PeriodicalDatesForExtensionPeriod mp ON mp.closing <= ld.currentvalue AND mp.opening >= ld.openingperiodvalue AND ld.FTPPeriod = mp.Order WHERE financialcycleid = \"+str(FiscalPeriodID)+\"  AND Entityid = \"+str(EntityID)+\" ) ld ON LEFT(tb.Period, 6) = closing WHERE EntityID_fk = \"+str(EntityID)+\" ORDER BY PeriodType\")\n",
    "\n",
    "        spark.sql(\"INSERT INTO \"+dbschema+\".temp_numperiod_\" + str(parameterID) + \" SELECT periodtype, LegacyPeriod, MAX(numericvalueofperiod) AS numericvalueofperiod FROM ( SELECT ld.periodtype, md.LegacyPeriod, CASE LEFT(Value, 1) WHEN 'Y' THEN INT(\"+str(Extendedmonthdiff)+\") WHEN '9' THEN INT(\"+str(Extendedmonthdiff)+\") - 3 WHEN 'H' THEN INT(\"+str(Extendedmonthdiff)+\") - 6 WHEN 'Q' THEN INT(\"+str(Extendedmonthdiff)+\") - 9 ELSE 1 END AS numericvalueofperiod FROM \"+dbschema+\".temp_legacysddates_\" + str(parameterID) + \" ld JOIN \"+dbschema+\".temp_tblegacymatchPeriod_\" + str(parameterID) + \" md ON ld.periodtype = md.PeriodType AND ld.legacyperiod = md.LegacyPeriod ) a GROUP BY periodtype, LegacyPeriod\")\n",
    "\n",
    "        spark.sql(\"DELETE FROM \"+dbschema+\".temp_tblegacymatchPeriod_\" + str(parameterID) + \" WHERE LEFT(Period,6) = LEFT(LegacyPeriod,6) AND CAST(RIGHT(Period,2) AS INT) <> ABS(NumericValueofPeriod)\")\n",
    "        \n",
    "        spark.sql(\"DELETE FROM \"+dbschema+\".temp_tblegacymatchPeriod_\" + str(parameterID) + \" WHERE LEFT(Period,6)<>LEFT(LegacyPeriod,6) AND CAST(RIGHT(Period,2) AS INT) < ABS(NumericValueofPeriod)\")\n",
    "        \n",
    "        spark.sql(\"MERGE INTO \"+dbschema+\".temp_tblegacymatchPeriod_\" + str(parameterID) + \" tb1 USING \"+dbschema+\".temp_numperiod_\" + str(parameterID) + \" nm ON tb1.PeriodType = nm.periodtype AND tb1.LegacyPeriod = nm.LegacyPeriod and LEFT(tb1.Period, 6) <> LEFT(tb1.LegacyPeriod, 6) AND CAST(RIGHT(tb1.Period, 2) AS INT) > ABS(nm.numericvalueofperiod) WHEN MATCHED THEN DELETE\")\n",
    "\n",
    "        spark.sql(\"INSERT INTO \"+dbschema+\".temp_priorityseq1_\" + str(parameterID) + \" SELECT tb.*,ld.ftpperiod FROM \"+dbschema+\".temp_tballperiod_\" + str(parameterID) + \" tb JOIN ( SELECT DISTINCT ls.legacyperiod, ls.periodtype, CONCAT( LEFT( DATE_ADD( MONTH, -CAST(RIGHT(ls.LegacyPeriod, 2) AS INT), to_date( CONCAT( RIGHT(LEFT(ls.LegacyPeriod, 6), 2), '-01-', LEFT(ls.LegacyPeriod, 4) ), 'MM-dd-yyyy' ) ), 4 ),RIGHT(LEFT(DATEADD(MONTH, -CAST(RIGHT(ls.LegacyPeriod, 2) AS INT), to_date( CONCAT( RIGHT(LEFT(ls.LegacyPeriod, 6), 2), '-01-', LEFT(ls.LegacyPeriod, 4) ), 'MM-dd-yyyy' ) ), 7), 2)) AS fromperiod ,ls.FTPPeriod FROM \"+dbschema+\".temp_legacysddates_\" + str(parameterID) + \" ls LEFT JOIN \"+dbschema+\".temp_tblegacymatchperiod_\" + str(parameterID) + \" mp ON ls.LegacyPeriod = mp.LegacyPeriod AND ls.periodtype = mp.periodtype WHERE mp.LegacyPeriod IS NULL  ) ld ON LEFT(tb.Period, 6)<= LEFT(ld.LegacyPeriod, 6) AND LEFT(tb.Period, 6)> fromperiod AND tb.periodtype = ld.periodtype WHERE RIGHT(tb.Period, 2)<> RIGHT(ld.LegacyPeriod, 2) and CAST(RIGHT(tb.Period, 2) AS INT)<=ABS(NumericValueofPeriod)\")\n",
    "\n",
    "        spark.sql(\"INSERT INTO \"+dbschema+\".temp_priorityseq_\" + str(parameterID) + \" SELECT Period,stdperiod,PeriodType,closing,legacyperiod,numericvalueofperiod, Row_number() OVER( partition BY stdperiod, periodtype ORDER BY Period DESC ) priorityseq, 0, ftpperiod FROM \"+dbschema+\".temp_priorityseq1_\" + str(parameterID) + \" ORDER BY periodtype\")\n",
    "\n",
    "        queryString=\"DROP TABLE IF EXISTS {vSQLDB}.temp_cteorder_\" + str(parameterID) + \";DROP TABLE IF EXISTS {vSQLDB}.temp_prioritylevel2temp_01_\" + str(parameterID) + \";\" \n",
    "        \n",
    "        queryString=queryString.replace(\"{vSQLDB}\",f\"{dbschema}\")\n",
    "        \n",
    "        sqlwarehouse_queryExecutor(queryString)\n",
    "\n",
    "        spark.sql(\"CREATE TABLE  \"+dbschema+\".temp_cteorder_\" + str(parameterID) + \" AS SELECT ps.Period, ps.PeriodType, mp.closing, ps.legacyperiod, priorityseq, max(mp.order) AS Order FROM (SELECT *, right(left(to_date(concat('01-',substring ( Period,charindex(' ',Period)+1, case when charindex('-',Period)=0 then len(Period)-charindex(' ',Period) else charindex('-',Period)-charindex(' ',Period)-1 end ),'-',left(closing,4)), 'dd-MM-yyyy'),7),2) FromPeriod FROM v_periodicaldatesforextensionperiod WHERE entityid=\"+str(EntityID)+\" ) mp JOIN (SELECT *, CONCAT( LEFT( DATE_ADD( MONTH, -CAST(RIGHT(Period, 2)AS INT)+1, to_date( CONCAT( RIGHT(LEFT(Period, 6), 2), '-01-', LEFT(Period, 4) ), 'MM-dd-yyyy' ) ), 4 ),RIGHT(LEFT(DATEADD(MONTH, -CAST(RIGHT(Period, 2) AS INT)+1, to_date( CONCAT( RIGHT(LEFT(Period, 6), 2), '-01-', LEFT(Period, 4) ), 'MM-dd-yyyy' ) ), 7), 2))  AS fromperiod FROM \"+dbschema+\".temp_priorityseq_\" + str(parameterID) + \" ) ps ON ps.closing = mp.closing AND ps.FTPPeriod > mp.order AND ps.fromperiod = left(ps.FromPeriod,4) + mp.FromPeriod WHERE financialcycleid =\"+str(FiscalPeriodID)+\" AND entityid = \"+str(EntityID)+\" GROUP BY ps.Period, ps.PeriodType, mp.closing, ps.legacyperiod, priorityseq\")\n",
    "\n",
    "        spark.sql(\"MERGE INTO \"+dbschema+\".temp_priorityseq_\" + str(parameterID) + \" AS ps USING \"+dbschema+\".temp_cteorder_\" + str(parameterID) + \" AS cte ON ps.Period = cte.Period WHEN MATCHED THEN UPDATE SET ps.order = cte.order\")\n",
    "\n",
    "        spark.sql(\"DROP TABLE IF EXISTS \"+dbschema+\".temp_prioritylevel2temp_01_\" + str(parameterID) + \"\")\n",
    "\n",
    "        spark.sql(\"CREATE TABLE \"+dbschema+\".temp_prioritylevel2temp_01_\" + str(parameterID) + \" AS SELECT DISTINCT periodtype, ld.Period, ld.legacyperiod, ftpperiod, ld.closing, CASE WHEN ld.Value IN ('H1', 'Q1', 'Year', '9M') THEN Datediff(month, TO_DATE(CONCAT('01', '-', RIGHT(FromOpeningPeriod, 2), '-', LEFT(FromOpeningPeriod, 4)), 'dd-MM-yyyy'), TO_DATE(CONCAT('01', '-', RIGHT(closing, 2), '-', LEFT(closing, 4)), 'dd-MM-yyyy')) + 1 WHEN ld.Value NOT IN ('H1', 'Q1', 'Year', '9M') THEN CASE WHEN LEFT(ld.Value, 1)='9' THEN '09' WHEN LEFT(ld.Value, 1)='H' THEN '06' WHEN LEFT(ld.Value, 1)='Q' THEN '03' WHEN LEFT(ld.Value, 1)='M' THEN '01' END END as monthdiff FROM ( SELECT ld.PeriodType, ld.Period, ld.FinancialYear, ld.LegacyPeriod, ld.ftpperiod, pd.Value, pd.closing, CONCAT(left(FromPeriod, 4),right(left(to_date(concat('01-',substring ( pd.Period,charindex(' ',pd.Period)+1, case when charindex('-',pd.Period)=0 then len(pd.Period)-charindex(' ',pd.Period) else charindex('-',pd.Period)-charindex(' ',pd.Period)-1 end ),'-',left(closing,4)), 'dd-MM-yyyy'),7),2)) as FromOpeningPeriod FROM v_PeriodicalDatesForExtensionPeriod as pd JOIN ( SELECT tbm.periodtype, MAX(tbm.Period) as Period, ld.FinancialYear, ld.LegacyPeriod, CASE WHEN ftpperiod = 1 THEN ftpperiod ELSE ftpperiod - 1 END as ftpperiod, CONCAT( LEFT( DATE_ADD( MONTH, -CAST(RIGHT(ld.LegacyPeriod, 2)AS INT), to_date( CONCAT( RIGHT(LEFT(ld.LegacyPeriod, 6), 2), '-01-', LEFT(ld.LegacyPeriod, 4) ), 'MM-dd-yyyy' ) ), 4 ),RIGHT(LEFT(DATEADD(MONTH, -CAST(RIGHT(ld.LegacyPeriod, 2) AS INT), to_date( CONCAT( RIGHT(LEFT(ld.LegacyPeriod, 6), 2), '-01-', LEFT(ld.LegacyPeriod, 4) ), 'MM-dd-yyyy' ) ), 7), 2))  AS fromperiod FROM \"+dbschema+\".temp_legacysddates_\" + str(parameterID) + \" as ld JOIN \"+dbschema+\".temp_tblegacymatchperiod_\" + str(parameterID) + \" as tbm ON ld.periodtype = tbm.periodtype AND ld.legacyperiod = tbm.legacyperiod GROUP BY tbm.periodtype, ld.FinancialYear, ld.LegacyPeriod, ftpperiod ) as ld ON pd.closing > fromperiod AND pd.closing <= LEFT(LegacyPeriod, 6) AND pd.Order = ld.ftpperiod AND pd.Year = ld.FinancialYear WHERE financialcycleid = \"+str(FiscalPeriodID)+\" and Entityid = \"+str(EntityID)+\"  ) as ld \")\n",
    "\n",
    "        spark.sql(\"INSERT INTO \" + dbschema + \".temp_prioritylevel2_\" + str(parameterID) + \" SELECT *, Row_number() OVER( partition BY stdperiod, periodtype ORDER BY Period DESC ) priorityseq, 0 as order FROM \" + dbschema + \".temp_prioritylevel2temp_\" + str(parameterID) + \" ORDER BY periodtype\")\n",
    "\n",
    "        spark.sql(\"DELETE FROM \"+dbschema+\".temp_priorityseq_\" + str(parameterID) + \" WHERE priorityseq<>1\")\n",
    "\n",
    "        spark.sql(\"DELETE FROM \"+dbschema+\".temp_prioritylevel2_\" + str(parameterID) + \" WHERE priorityseq<>1\")\n",
    "\n",
    "        df_temp_003=spark.sql(\"SELECT DISTINCT PeriodType, max(order) order FROM \"+dbschema+\".temp_priorityseq_\" + str(parameterID) + \" GROUP BY PeriodType EXCEPT SELECT ps.periodtype, ps.order FROM \"+dbschema+\".temp_priorityseq_\" + str(parameterID) + \" ps JOIN \"+dbschema+\".temp_legacysddates_\" + str(parameterID) + \" ld ON ps.periodtype = ld.periodtype AND ps.legacyperiod = ld.legacyperiod AND ps.Order = ld.ftpperiod - 1\")\n",
    "\n",
    "        if df_temp_003.count()>0 :\n",
    "            spark.sql(\"DROP TABLE IF EXISTS \"+dbschema+\".temp_ctePS_\" + str(parameterID) + \"\")\n",
    "            \n",
    "            spark.sql(\"CREATE TABLE \"+dbschema+\".temp_ctePS_\" + str(parameterID) + \" AS SELECT PeriodType,LegacyPeriod,max(Order) Order FROM \"+dbschema+\".temp_Priorityseq_\" + str(parameterID) + \" GROUP BY PeriodType,LegacyPeriod\")\n",
    "            \n",
    "            spark.sql(\"MERGE INTO \" + dbschema + \".temp_legacysddates_\" + str(parameterID) + \" AS ld USING \" + dbschema + \".temp_ctePS_\" + str(parameterID) + \" AS ps ON ps.periodtype = ld.periodtype AND ps.legacyperiod = ld.legacyperiod AND ps.order <> ld.ftpperiod WHEN MATCHED THEN UPDATE SET ld.ftpperiod = ps.order + 1 \")\n",
    "        \n",
    "        spark.sql(\"DROP TABLE IF EXISTS \"+dbschema+\".temp_deleteplev2_\" + str(parameterID) + \"\")\n",
    "        \n",
    "        spark.sql(\"CREATE TABLE  \"+dbschema+\".temp_deleteplev2_\" + str(parameterID) + \" AS SELECT LegacyPeriod,PeriodType from \"+dbschema+\".temp_tblegacymatchPeriod_\" + str(parameterID) + \" where Period=LegacyPeriod\")\n",
    "\n",
    "        spark.sql(\"MERGE INTO \"+dbschema+\".temp_prioritylevel2_\" + str(parameterID) + \" AS pl USING \"+dbschema+\".temp_deleteplev2_\" + str(parameterID) + \" AS tbm ON tbm.periodtype = pl.periodtype AND tbm.legacyperiod = pl.legacyperiod WHEN MATCHED THEN DELETE\")\n",
    "        \n",
    "        spark.sql(\"MERGE INTO \"+dbschema+\".temp_tblegacymatchperiod_\" + str(parameterID) + \" AS tbm USING \"+dbschema+\".temp_prioritylevel2_\" + str(parameterID) + \" AS pl ON tbm.periodtype = pl.periodtype AND tbm.legacyperiod = pl.legacyperiod WHEN MATCHED THEN DELETE\")\n",
    "        \n",
    "        spark.sql(\"MERGE INTO \"+dbschema+\".temp_priorityseq_\" + str(parameterID) + \" AS ps USING \"+dbschema+\".temp_legacysddates_\" + str(parameterID) + \" AS ld ON ps.periodtype = ld.periodtype AND ps.legacyperiod = ld.legacyperiod AND ps.order <> ld.ftpperiod - 1 WHEN MATCHED THEN DELETE\")\n",
    "\n",
    "        spark.sql(\"DROP TABLE IF EXISTS \"+dbschema+\".temp_bspriority_\" + str(parameterID) + \"\")\n",
    "\n",
    "        spark.sql(\"CREATE TABLE \"+dbschema+\".temp_bspriority_\" + str(parameterID) + \" AS SELECT PeriodType, legacyperiod, currentvalue, MAX(Period) AS Period, MAX(stdperiod) AS stdperiod FROM ( SELECT ld.PeriodType, ld.legacyperiod, ld.currentvalue, MAX(ps.Period) AS Period, MAX(ps.stdperiod) AS stdperiod FROM \"+dbschema+\".temp_legacysddates_\" + str(parameterID) + \" ld JOIN \"+dbschema+\".temp_priorityseq_\" + str(parameterID) + \" ps ON ld.periodtype = ps.PeriodType AND ld.legacyperiod = ps.legacyperiod GROUP BY ld.PeriodType, ld.legacyperiod, ld.currentvalue UNION SELECT ld.PeriodType, ld.legacyperiod, ld.currentvalue, MAX(ps.Period) AS Period, MAX(ps.stdperiod) AS stdperiod FROM \"+dbschema+\".temp_legacysddates_\" + str(parameterID) + \" ld JOIN \"+dbschema+\".temp_prioritylevel2_\" + str(parameterID) + \" ps ON ld.periodtype = ps.PeriodType AND ld.legacyperiod = ps.legacyperiod GROUP BY ld.PeriodType, ld.legacyperiod, ld.currentvalue UNION SELECT PeriodType, legacyperiod, LEFT(legacyperiod, 6), MAX(Period), MAX(stdperiod) FROM \"+dbschema+\".temp_tblegacymatchperiod_\" + str(parameterID) + \" GROUP BY PeriodType, legacyperiod, legacyperiod ) a GROUP BY PeriodType, legacyperiod, currentvalue \")\n",
    "\n",
    "        spark.sql(\"MERGE INTO \"+dbschema+\".temp_legacysddates_\" + str(parameterID) + \" AS ld USING \"+dbschema+\".temp_bspriority_\" + str(parameterID) + \" AS ps ON ld.periodtype = ps.PeriodType AND ld.legacyperiod = ps.legacyperiod WHEN MATCHED THEN UPDATE SET ld.LegacyPeriod = ps.Period, ld.CurrentValue = ps.stdperiod\")\n",
    "\n",
    "        spark.sql(\"MERGE INTO \"+dbschema+\".temp_ytddates_\" + str(parameterID) + \" as Ys USING \"+dbschema+\".temp_legacysddates_\" + str(parameterID) + \" as ld ON ys.periodtype = ld.periodtype and financialyear = '\"+str(Extendedcycleyear)+\"' WHEN MATCHED THEN DELETE\")\n",
    "\n",
    "        spark.sql(\"MERGE INTO \"+dbschema+\".temp_tblegacymatchPeriod_\" + str(parameterID) + \" AS tb  USING ( SELECT * FROM ( SELECT * FROM \"+dbschema+\".temp_tblegacymatchPeriod_\" + str(parameterID) + \" WHERE LEFT(Period, 6) <> LEFT(LegacyPeriod, 6) ) a  WHERE NOT EXISTS ( SELECT LegacyPeriod  FROM \"+dbschema+\".temp_tblegacymatchPeriod_\" + str(parameterID) + \" b  WHERE a.LegacyPeriod = Period ) ) del  ON tb.LegacyPeriod = del.LegacyPeriod  WHEN MATCHED THEN DELETE\" )\n",
    "\n",
    "        if countofcomp == 0 or countofcomp  == 1 :\n",
    "            spark.sql(\"INSERT INTO \"+dbschema+\".temp_ytddates_\" + str(parameterID) + \" SELECT PeriodTYpe, LEFT(legacyperiod, 6) stdperiod, Period FROM \"+dbschema+\".temp_tblegacymatchperiod_\" + str(parameterID) + \" WHERE Periodtype LIKE '%YTD' OR Periodtype LIKE '%Year' UNION SELECT ld.PeriodTYpe, LEFT(ld.legacyperiod, 6), Period FROM \"+dbschema+\".temp_priorityseq_\" + str(parameterID) + \" ps JOIN \"+dbschema+\".temp_legacysddates_\" + str(parameterID) + \" ld ON ps.periodtype = ld.periodtype WHERE (ld.Periodtype LIKE '%YTD' OR ld.Periodtype LIKE '%Year') AND financialyear = '\"+str(Extendedcycleyear)+\"' UNION SELECT ld.PeriodTYpe, LEFT(pl.legacyperiod, 6), Period FROM \"+dbschema+\".temp_prioritylevel2_\" + str(parameterID) + \" pl JOIN \"+dbschema+\".temp_legacysddates_\" + str(parameterID) + \" ld ON pl.periodtype = ld.periodtype WHERE (ld.Periodtype LIKE '%YTD' OR ld.Periodtype LIKE '%Year') AND financialyear = '\"+str(Extendedcycleyear)+\"'\")\n",
    "        \n",
    "        if countofcomp > 1 :\n",
    "            spark.sql(\"INSERT INTO \"+dbschema+\".temp_ytddates_\" + str(parameterID) + \" SELECT periodtype, pcurrentvalue, legacyperiod FROM \"+dbschema+\".temp_legacysddates_\" + str(parameterID) + \" WHERE periodtype NOT IN ('currentPeriod', 'PreviousPeriod') AND financialyear = '\"+str(Extendedcycleyear)+\"' EXCEPT SELECT periodtype, pcurrentvalue, legacyperiod FROM ( SELECT periodtype, pcurrentvalue, legacyperiod, financialyear FROM \"+dbschema+\".temp_legacysddates_\" + str(parameterID) + \" WHERE LEFT(legacyperiod, 6) <> pcurrentvalue ) a WHERE NOT EXISTS ( SELECT legacyPeriod FROM \"+dbschema+\".temp_tblegacymatchPeriod_\" + str(parameterID) + \" b WHERE a.legacyPeriod = Period AND a.periodtype = PeriodType ) AND financialyear = '\"+str(Extendedcycleyear)+\"' UNION SELECT PeriodTYpe, LEFT(legacyperiod, 6), Period FROM \"+dbschema+\".temp_priorityseq_\" + str(parameterID) + \" UNION SELECT PeriodTYpe, LEFT(legacyperiod, 6), Period FROM \"+dbschema+\".temp_prioritylevel2_\" + str(parameterID) + \" UNION SELECT periodtype, pcurrentvalue, Period FROM ( SELECT periodtype, LEFT(legacyperiod, 6) AS pcurrentvalue, Period FROM \"+dbschema+\".temp_tblegacymatchperiod_\" + str(parameterID) + \" EXCEPT SELECT periodtype, pcurrentvalue, legacyperiod FROM \"+dbschema+\".temp_legacysddates_\" + str(parameterID) + \" WHERE financialyear = '\"+str(Extendedcycleyear)+\"' ) a \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "30249b66-36f3-424a-9024-4e4ee88ff981",
     "showTitle": true,
     "title": "Define GetLegacyAdjustmentByCategoryDrilldown Function"
    }
   },
   "outputs": [],
   "source": [
    "def GetLegacyAdjustmentByCategoryDrilldown(AccountingStandard,EntityID,PeriodValue,Currency,Category):\n",
    "    \n",
    "    queryString=\"DROP TABLE IF EXISTS {vSQLDB}.temp_templegacy_dd_\" + str(parameterID) + \";DROP TABLE IF EXISTS {vSQLDB}.temp_tmpLegacywithoutendperiod_dd_\" + str(parameterID) + \";DROP TABLE IF EXISTS {vSQLDB}.temp_tmplegacywithoutNA_dd_\" + str(parameterID) + \";\" \n",
    "    \n",
    "    queryString=queryString.replace(\"{vSQLDB}\",f\"{dbschema}\")\n",
    "    \n",
    "    sqlwarehouse_queryExecutor(queryString)\n",
    "\n",
    "    CategoryName=\"\"\n",
    "    GivenYear=\"\"\n",
    "\n",
    "    spark.sql(\"CREATE TABLE \"+dbschema+\".temp_tmpLegacywithoutendperiod_dd_\" + str(parameterID) + \" TBLPROPERTIES ('delta.feature.allowColumnDefaults' = 'supported')  SELECT  LegacyAdjustmentID, AccountingStdID_fk, Period, EntityID_fk,  JournalNumber, JournalDate, UserGLCode, DebitCredit, CurrencyID_fk, Amount, JournalType, Narration, CreatedDate, CreatedBy, ModifiedDate, ModifiedBy, BUID_fk, IsImported, CategoryID, IsMappedGLCode, EndPeriod, NotApplicable,\"+alldimcols+\" FROM  \"+dbschema+\".temp_legacyadjustment_\" + str(parameterID) + \" WHERE  BUID_fk=0 AND CategoryID =\"+str(Category)+\" AND EntityID_fk = \"+str(EntityID)+\" AND AccountingStdID_fk = \"+str(AccountingStandard )+\" AND Period <= \"+str(PeriodValue )+\" AND IFNULL(EndPeriod, 999999-99) > \"+str(PeriodValue )+\" \")\n",
    "\n",
    "    spark.sql(\"CREATE TABLE \"+dbschema+\".temp_tmplegacywithoutNA_dd_\" + str(parameterID) + \" TBLPROPERTIES ('delta.feature.allowColumnDefaults' = 'supported') SELECT *,case when Period = value then 0 when (NotApplicable is not null and NotApplicable !='' and value='') then 0 else 1 end IsmappedGLUpdatedCode FROM (SELECT *,explode(split(NVL(NotApplicable,''), ',')) as value  FROM \"+dbschema+\".temp_tmpLegacywithoutendperiod_dd_\" + str(parameterID) + \") as t\")\n",
    "\n",
    "    spark.sql(\"CREATE TABLE \"+dbschema+\".temp_tempLegacy_dd_\" + str(parameterID) + \" TBLPROPERTIES ('delta.feature.allowColumnDefaults' = 'supported') SELECT DISTINCT LegacyAdjustmentID, AccountingStdID_fk, Period, EntityID_fk,  JournalNumber, JournalDate, UserGLCode, DebitCredit, CurrencyID_fk, Amount, JournalType, Narration, CreatedDate, CreatedBy, ModifiedDate, ModifiedBy, BUID_fk, IsImported, CategoryID, IsMappedGLCode, EndPeriod, NotApplicable,\"+alldimcols+\" FROM \"+dbschema+\".temp_tmplegacywithoutNA_dd_\" + str(parameterID) + \"  where IsmappedGLUpdatedCode=1 and period not in (select distinct value from \"+dbschema+\".temp_tmplegacywithoutNA_dd_\" + str(parameterID) + \" )\")\n",
    "\n",
    "    spark.sql(\"INSERT INTO \"+dbschema+\".legacyadjustmentsdrilldown( AccountingStdID_fk, Period, EntityID_fk,  JournalNumber, JournalDate, UserGLCode, DebitCredit, CurrencyID_fk, Amount, JournalType, Narration, CreatedDate, CreatedBy, ModifiedDate, ModifiedBy, BUID_fk, IsImported, CategoryID, IsMappedGLCode, EndPeriod, NotApplicable, \"+alldimcols+\" ) SELECT  AccountingStdID_fk, Period, EntityID_fk,  JournalNumber, JournalDate, UserGLCode, DebitCredit, CurrencyID_fk, Amount, JournalType, Narration, CreatedDate, CreatedBy, ModifiedDate, ModifiedBy, BUID_fk, IsImported, CategoryID, IsMappedGLCode, EndPeriod, NotApplicable,\"+alldimcols+\" FROM \"+dbschema+\".temp_tempLegacy_dd_\" + str(parameterID) + \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b05d5d2b-cf93-400a-a270-2ff79fdd3058",
     "showTitle": true,
     "title": "Casting and Legacy adj Data fetch"
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"MERGE INTO \"+dbschema+\".temp_ytddates_\" + str(parameterID) + \" t USING (SELECT row_number() over(partition by  PeriodType,CurrentValue,FTPValue order by CurrentValue )seq,* FROM \"+dbschema+\".temp_ytddates_\" + str(parameterID) + \" ) s ON t.PeriodType=s.PeriodType and t.CurrentValue=s.CurrentValue and  t.FTPValue=s.FTPValue and s.seq=2 WHEN MATCHED THEN DELETE\")\n",
    "\n",
    "spark.sql(\"CREATE TABLE \"+dbschema+\".temp_ladjjfinal_\" + str(parameterID) + \" TBLPROPERTIES ('delta.feature.allowColumnDefaults' = 'supported') AS  SELECT *, CAST('' AS VARCHAR(50)) AS convertedperiod, CAST('' AS VARCHAR(100)) AS periodtype FROM \"+dbschema+\".legacyadjustmentsdrilldown WHERE 1=0\")\n",
    "\n",
    "spark.sql(\"CREATE TABLE \"+dbschema+\".temp_castingsdadjustment1_\" + str(parameterID) + \" TBLPROPERTIES ('delta.feature.allowColumnDefaults' = 'supported') AS  SELECT * FROM \"+dbschema+\".CastingAdjustmentTable WHERE 1=0\")\n",
    "\n",
    "spark.sql(\"TRUNCATE TABLE \"+dbschema+\".legacyadjustmentsdrilldown\")\n",
    "\n",
    "legacyadj_df = spark.sql(\"SELECT id,legacyperiod,financialyear FROM \"+dbschema+\".temp_legacysddates_\" + str(parameterID) + \"\").collect()\n",
    "\n",
    "if len(legacyadj_df) > 0 :\n",
    "\n",
    "    for row in legacyadj_df:\n",
    "\n",
    "        spark.sql(\"MERGE INTO \"+dbschema+\".temp_legacysddates_\" + str(parameterID) + \" USING ( SELECT categoryid FROM v_categorydropdown WHERE categoryname = '\"+str(CategorySDName)+\"' AND accountingstandardid = \"+str(AccountingStandardID)+\" AND entityid = \"+str(EntityID)+\" AND Year = '\"+str(row[\"financialyear\"])+\"' ) AS src ON (1=1) and id=\"+str(row[\"id\"])+\" WHEN MATCHED THEN UPDATE SET categoryid = COALESCE(src.categoryid, 0)\")\n",
    "\n",
    "        temp_legacysddates_df=spark.sql(\"SELECT legacyperiod,categoryid,castingperiod FROM \"+dbschema+\".temp_legacysddates_\" + str(parameterID) + \" WHERE id=\"+str(row[\"id\"])+\" \").collect()\n",
    "\n",
    "        if len(temp_legacysddates_df)>0 :\n",
    "\n",
    "            for row_1 in temp_legacysddates_df : \n",
    "                GetLegacyAdjustmentByCategoryDrilldown(AccountingStandardID,EntityID,row_1[\"legacyperiod\"],SDCurrencyid,row_1[\"categoryid\"])\n",
    "                \n",
    "                spark.sql(\"INSERT INTO \"+dbschema+\".temp_castingsdadjustment1_\" + str(parameterID) + \" SELECT * FROM \"+dbschema+\".temp_castingsdadjustment_\" + str(parameterID) + \" WHERE categoryid = '\"+str(row_1[\"categoryid\"])+\"' AND fk_entityid = \"+str(EntityID)+\" AND fk_accountingstandardid = \"+str(AccountingStandardID)+\" AND roundoff = '\"+str(Roundoff)+\"' AND amountsin = '\"+str(AmountsIn)+\"' AND periodvalue = '\"+str(row_1[\"castingperiod\"])+\"' AND roundtypelevel = '\"+str(RoundTypeLevel)+\"'\")\n",
    "                \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "69acd8e3-d160-4dba-b727-bc0fd1d8376b",
     "showTitle": true,
     "title": "Fetch combined Data for TB and Adjustments"
    }
   },
   "outputs": [],
   "source": [
    "queryString=\"DROP TABLE IF EXISTS  {vSQLDB}.temp_tbfinal_\" + str(parameterID) + \";DROP TABLE IF EXISTS  {vSQLDB}.temp_tbfinalytd_\" + str(parameterID) + \";DROP TABLE IF EXISTS  {vSQLDB}.temp_disclosures_\" + str(parameterID) + \";DROP TABLE IF EXISTS  {vSQLDB}.temp_disclosuresytd_\" + str(parameterID) + \";DROP TABLE IF EXISTS  {vSQLDB}.temp_tbfinaldd_\" + str(parameterID) + \";DROP TABLE IF EXISTS  {vSQLDB}.temp_tbfinalddytd_\" + str(parameterID) + \";\" \n",
    "\n",
    "queryString=queryString.replace(\"{vSQLDB}\",f\"{dbschema}\")\n",
    "\n",
    "sqlwarehouse_queryExecutor(queryString)\n",
    "\n",
    "queryString=\"CREATE TABLE {vSQLDB}.temp_tbfinal_\" + str(parameterID) + \" ( PeriodType VARCHAR(50), Period VARCHAR(50), userglcode VARCHAR(50), currency INT, closingbalance DECIMAL(28, 8),\"+alldimcols_dt+\");CREATE TABLE {vSQLDB}.temp_tbfinalytd_\" + str(parameterID) + \" ( PeriodType VARCHAR(50), Period VARCHAR(50), userglcode VARCHAR(50), currency INT, closingbalance DECIMAL(28, 8), \"+alldimcols_dt+\" );CREATE TABLE {vSQLDB}.temp_tbfinaldd_\" + str(parameterID) + \" ( PeriodType VARCHAR(50), Period VARCHAR(50), userglcode VARCHAR(50), usergldescription VARCHAR(200), currency INT, closingbalance DECIMAL(28, 8), Source VARCHAR(50), \"+alldimcols_dt+\" );CREATE TABLE {vSQLDB}.temp_tbfinalddytd_\" + str(parameterID) + \" ( PeriodType VARCHAR(50), Period VARCHAR(50), userglcode VARCHAR(50), usergldescription VARCHAR(200), currency INT, closingbalance DECIMAL(28, 8), Source VARCHAR(50), \"+alldimcols_dt+\" );CREATE TABLE {vSQLDB}.temp_disclosures_\" + str(parameterID) + \" ( PeriodType VARCHAR(50), Period VARCHAR(50), entityid INT, ultimateparent VARCHAR(50), categoryid INT, member VARCHAR(200), accounttype VARCHAR(50), amount DECIMAL(28, 8), currencyid INT, column27 VARCHAR(100), AccountSubType VARCHAR(500), \"+alldimcols_dt+\" );CREATE TABLE {vSQLDB}.temp_disclosuresytd_\" + str(parameterID) + \" ( PeriodType VARCHAR(50), Period VARCHAR(50), entityid INT, ultimateparent VARCHAR(50), categoryid INT, member VARCHAR(200), accounttype VARCHAR(50), amount DECIMAL(28, 8), currencyid INT, column27 VARCHAR(100), AccountSubType VARCHAR(500), \"+alldimcols_dt+\" );DROP TABLE IF EXISTS {vSQLDB}.temp_legacyDatesLite_\" + str(parameterID) + \";CREATE TABLE {vSQLDB}.temp_legacyDatesLite_\" + str(parameterID) + \" ( LegacyPeriod VARCHAR(20), PeriodType VARCHAR(500) );\"\n",
    "\n",
    "queryString=queryString.replace(\"{vSQLDB}\",f\"{dbschema}\")\n",
    "\n",
    "sqlwarehouse_queryExecutor(queryString)\n",
    "\n",
    "spark.sql(\"INSERT INTO \"+dbschema+\".temp_legacyDatesLite_\" + str(parameterID) + \" SELECT DISTINCT LegacyPeriod,PeriodType FROM  \"+dbschema+\".temp_legacysddates_\" + str(parameterID) + \" \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f66858d8-81f4-458d-beab-6584a4405bb7",
     "showTitle": true,
     "title": "Standard Period data fetch"
    }
   },
   "outputs": [],
   "source": [
    "########## Below Need To Cross Verify with Current System Changes ################\n",
    "temp_tbfinal_df=spark.sql(\"SELECT * FROM \"+dbschema+\".temp_tbfinal_\" + str(parameterID) + \" LIMIT 1 \").collect()\n",
    "\n",
    "if len(temp_tbfinal_df)<=0 :\n",
    "    ########## Below Need To Cross Verify with Current System Changes ################\n",
    "    spark.sql(\"INSERT INTO \"+dbschema+\".temp_tbfinal_\" + str(parameterID) + \" SELECT a.PeriodType,a.Period, a.userglcode, currency, Sum(closingbalance) as closingbalance, \"+alldimcols+\" FROM ( SELECT PeriodType,LEFT(Period,6) as Period, userglcode, closingbalance, CurrencyID_fk AS currency, \"+alldimcols+\" FROM \"+dbschema+\".temp_trialbalance_\" + str(parameterID) + \" vt INNER JOIN \"+dbschema+\".temp_legacyDatesLite_\" + str(parameterID) + \" ld ON vt.Period = ld.legacyperiod AND EntityID_fk = \"+str(EntityID)+\" AND AccountingStdID_fk = \"+str(AccountingStandardID)+\" UNION ALL SELECT PeriodType,LEFT(Period,6) as Period, userglcode, amount, CurrencyID_fk, \"+alldimcols+\" FROM \"+dbschema+\".adjustments vt INNER JOIN \"+dbschema+\".temp_legacyDatesLite_\" + str(parameterID) + \" ld ON vt.Period = ld.legacyperiod AND EntityID_fk = \"+str(EntityID)+\" AND AccountingStdID_fk = \"+str(AccountingStandardID)+\" AND IsUserGLCodeAdjustment=1 INNER JOIN \"+dbschema+\".temp_tempcategory_\" + str(parameterID) + \" tc ON tc.categoryid = vt.CategoryID_fk UNION ALL SELECT PeriodType,LEFT(convertedperiod, 6) AS Period, userglcode, amount, CurrencyID_fk, \"+alldimcols+\" FROM \"+dbschema+\".temp_ladjjfinal_\" + str(parameterID) + \" WHERE ismappedglcode = 1 ) A GROUP BY PeriodType,Period, userglcode, currency, \"+alldimcols+\"\")\n",
    "    \n",
    "    spark.sql(\"INSERT INTO \"+dbschema+\".temp_tbfinalytd_\" + str(parameterID) + \" SELECT a.PeriodType,a.Period, a.userglcode, currency, Sum(closingbalance) as closingbalance, \"+alldimcols+\" FROM ( SELECT PeriodType,LEFT(Period,6) as Period, userglcode, Sum(closingbalance) as closingbalance, CurrencyID_fk AS currency,\"+alldimcols+\" FROM \"+dbschema+\".temp_trialbalance_\" + str(parameterID) + \" vt INNER JOIN \"+dbschema+\".temp_ytddates_\" + str(parameterID) + \" yt ON vt.Period = yt.FTPValue AND EntityID_fk = \"+str(EntityID)+\" AND AccountingStdID_fk = \"+str(AccountingStandardID)+\" GROUP BY PeriodType,Period,userglcode,CurrencyID_fk, \"+alldimcols+\"  UNION  SELECT PeriodType,LEFT(Period,6) as Period, userglcode, sum(amount) as amount, CurrencyID_fk, \"+alldimcols+\" FROM \"+dbschema+\".adjustments vt INNER JOIN \"+dbschema+\".temp_ytddates_\" + str(parameterID) + \" yt ON vt.Period = yt.FTPValue AND EntityID_fk = \"+str(EntityID)+\" AND AccountingStdID_fk = \"+str(AccountingStandardID)+\" AND IsUserGLCodeAdjustment=1 INNER JOIN \"+dbschema+\".temp_tempcategory_\" + str(parameterID) + \" tc ON tc.categoryid = vt.CategoryID_fk GROUP BY PeriodType,Period,userglcode,CurrencyID_fk, \"+alldimcols+\" UNION  SELECT PeriodType,LEFT(convertedperiod, 6) AS Period, userglcode, amount, CurrencyID_fk, \"+alldimcols+\" FROM \"+dbschema+\".temp_ladjjfinal_\" + str(parameterID) + \" WHERE ismappedglcode = 1 ) A GROUP BY PeriodType,Period, userglcode, currency, \"+alldimcols+\"\")\n",
    "\n",
    "\n",
    "    #Select bulk upload disclsore data  if category not there else categorywise data\n",
    "\n",
    "\n",
    "    spark.sql(\"INSERT INTO \"+dbschema+\".temp_disclosures_\" + str(parameterID) + \" ( PeriodType,Period, entityid, ultimateparent, categoryid, member, accounttype, amount, currencyid, column27, AccountSubType, \"+alldimcols+\" ) SELECT PeriodType,LEFT(Period,6) as Period, EntityID_fk, vt.ultimateparent, CategoryID_fk, vt.member, accounttype, amount, CurrencyID_fk, '' column27, vt.AccountSubType, \"+alldimcols+\" FROM \"+dbschema+\".adjustments vt INNER JOIN \"+dbschema+\".temp_allperiodFTPscoa_\" + str(parameterID) + \" sc on vt.Member=sc.Member and vt.AccountSubType = sc.AccountSubType INNER JOIN \"+dbschema+\".temp_legacyDatesLite_\" + str(parameterID) + \" ld ON vt.Period = ld.LegacyPeriod AND EntityID_fk =\"+str(EntityID)+\" AND AccountingStdID_fk = \"+str(AccountingStandardID)+\" WHERE lower(AdjustmentDisclosureType)='disclosures' AND IsActive = 1\")\n",
    "\n",
    "    spark.sql(\"INSERT INTO \"+dbschema+\".temp_disclosuresytd_\" + str(parameterID) + \" ( PeriodType,Period, entityid, ultimateparent, categoryid, member, accounttype, amount, currencyid, column27, AccountSubType, \"+alldimcols+\" ) SELECT PeriodType,LEFT(Period,6) as Period, EntityID_fk, vt.ultimateparent, CategoryID_fk, vt.member, accounttype, sum(amount) as amount, CurrencyID_fk, '' column27, vt.AccountSubType, \"+alldimcols+\" FROM \"+dbschema+\".adjustments vt INNER JOIN \"+dbschema+\".temp_allperiodFTPscoa_\" + str(parameterID) + \" sc on vt.Member=sc.Member and vt.AccountSubType = sc.AccountSubType INNER JOIN \"+dbschema+\".temp_ytddates_\" + str(parameterID) + \" yt ON vt.Period = yt.FTPValue AND EntityID_fk =\"+str(EntityID)+\" AND AccountingStdID_fk = \"+str(AccountingStandardID)+\" WHERE lower(AdjustmentDisclosureType)='disclosures' AND IsActive = 1 GROUP BY PeriodType,Period,EntityID_fk, vt.ultimateparent, CategoryID_fk, vt.member, accounttype, CurrencyID_fk, vt.AccountSubType, \"+alldimcols+\"\")\n",
    "\n",
    "    spark.sql(\"DROP TABLE IF EXISTS \"+dbschema+\".temp_updatechildftp_\" + str(parameterID) + \"\")\n",
    "    \n",
    "    spark.sql(\"CREATE TABLE \"+dbschema+\".temp_updatechildftp_\" + str(parameterID) + \" as SELECT sc.member,sc.path FROM \"+dbschema+\".temp_disclosures_\" + str(parameterID) + \" d INNER JOIN \"+dbschema+\".temp_allperiodftpscoa_\" + str(parameterID) + \" sc ON d.member=sc.member  and d.AccountSubType = sc.AccountSubType WHERE sc.column27 = 'FTP' \")\n",
    "    \n",
    "    spark.sql(\"DROP TABLE IF EXISTS \"+dbschema+\".temp_updatechildftp_01_\" + str(parameterID) + \"\")\n",
    "    spark.sql(\"DROP TABLE IF EXISTS \"+dbschema+\".temp_updatechildftp_\" + str(parameterID) + \"\")\n",
    "    \n",
    "    spark.sql(\"CREATE TABLE \"+dbschema+\".temp_updatechildftp_01_\" + str(parameterID) + \" as SELECT sc.member,sc.path FROM \"+dbschema+\".temp_disclosures_\" + str(parameterID) + \" d INNER JOIN \"+dbschema+\".temp_allperiodftpscoa_\" + str(parameterID) + \" sc ON d.member=sc.member  and d.AccountSubType = sc.AccountSubType WHERE sc.column27 = 'FTP' \")\n",
    "    \n",
    "    spark.sql(\"CREATE TABLE \"+dbschema+\".temp_updatechildftp_\" + str(parameterID) + \" SELECT sc.member,sc.Path,'FTP' column27 FROM \"+dbschema+\".temp_updatechildftp_01_\" + str(parameterID) + \" up INNER JOIN  \"+dbschema+\".temp_allperiodftpscoa_\" + str(parameterID) + \" sc ON Charindex(up.path,sc.path)>0\")\n",
    "    \n",
    "    spark.sql(\"MERGE INTO \"+dbschema+\".temp_disclosures_\" + str(parameterID) + \" d USING \"+dbschema+\".temp_updatechildftp_\" + str(parameterID) + \" sc ON d.member=sc.member WHEN MATCHED THEN UPDATE SET d.column27=COALESCE(sc.column27,'')\")\n",
    "    \n",
    "    spark.sql(\"MERGE INTO \"+dbschema+\".temp_disclosuresytd_\" + str(parameterID) + \" d USING \"+dbschema+\".temp_updatechildftp_\" + str(parameterID) + \" sc ON d.member=sc.member WHEN MATCHED THEN UPDATE SET d.column27=COALESCE(sc.column27,'')\")\n",
    "    \n",
    "    spark.sql(\"INSERT INTO \"+dbschema+\".temp_tbfinaldd_\" + str(parameterID) + \"(PeriodType,Period,userglcode,usergldescription,currency,closingbalance,source,\"+alldimcols+\") SELECT PeriodType,a.Period, a.userglcode, a.usergldescription, currency, Sum(closingbalance) AS closingbalance, a.source, \"+alldimcols+\" FROM ( SELECT PeriodType,left(Period,6) as Period, userglcode, '' as usergldescription, closingbalance, CurrencyID_fk AS currency, 'TB' source, \"+alldimcols+\" FROM \"+dbschema+\".temp_trialbalance_\" + str(parameterID) + \" vt INNER JOIN \"+dbschema+\".temp_legacyDatesLite_\" + str(parameterID) + \" ld ON vt.Period = ld.LegacyPeriod AND EntityID_fk = \"+str(EntityID)+\" AND AccountingStdID_fk = \"+str(AccountingStandardID)+\" UNION  SELECT PeriodType,left(Period,6) as Period, userglcode, '' as usergldescription, amount, CurrencyID_fk, 'Adjustments' source, \"+alldimcols+\" FROM \"+dbschema+\".temp_standaloneadj_\" + str(parameterID) + \" vt INNER JOIN \"+dbschema+\".temp_legacyDatesLite_\" + str(parameterID) + \" ld ON vt.Period = ld.LegacyPeriod AND EntityID_fk = \"+str(EntityID)+\" AND AccountingStdID_fk = \"+str(AccountingStandardID)+\" AND IsUserGLCodeAdjustment=1 INNER JOIN \"+dbschema+\".temp_tempcategory_\" + str(parameterID) + \" tc ON tc.categoryid = vt.CategoryID_fk UNION  SELECT PeriodType,LEFT(convertedperiod, 6) AS Period, userglcode, '' as usergldescription, amount, CurrencyID_fk, 'Adjustments' source, \"+alldimcols+\" FROM \"+dbschema+\".temp_ladjjfinal_\" + str(parameterID) + \" WHERE ismappedglcode = 1 ) a GROUP BY PeriodType,Period, userglcode, usergldescription, currency, Source,\"+alldimcols+\"\")\n",
    "    \n",
    "    spark.sql(\"INSERT INTO \"+dbschema+\".temp_tbfinalddytd_\" + str(parameterID) + \"(PeriodType,Period,userglcode,usergldescription,currency,closingbalance,source,\"+alldimcols+\") SELECT PeriodType,a.Period, a.userglcode, a.usergldescription, currency, Sum(closingbalance) AS closingbalance, a.source, \"+alldimcols+\" FROM ( SELECT PeriodType,left(Period,6) as Period, userglcode, '' as usergldescription, Sum(closingbalance) as closingbalance, CurrencyID_fk AS currency, 'TB' source, \"+alldimcols+\" FROM \"+dbschema+\".temp_trialbalance_\" + str(parameterID) + \" vt INNER JOIN \"+dbschema+\".temp_ytddates_\" + str(parameterID) + \" yt ON vt.Period = yt.FTPValue AND EntityID_fk = \"+str(EntityID)+\" AND AccountingStdID_fk = \"+str(AccountingStandardID)+\"  GROUP BY PeriodType,Period, userglcode, CurrencyID_fk, \"+alldimcols+\" UNION  SELECT PeriodType,left(Period,6) as Period, userglcode, '' as usergldescription,sum(amount) as amount, CurrencyID_fk, 'Adjustments' source, \"+alldimcols+\" FROM \"+dbschema+\".temp_standaloneadj_\" + str(parameterID) + \" vt INNER JOIN \"+dbschema+\".temp_ytddates_\" + str(parameterID) + \" yt ON vt.Period = yt.FTPValue AND EntityID_fk = \"+str(EntityID)+\" AND AccountingStdID_fk = \"+str(AccountingStandardID)+\" AND IsUserGLCodeAdjustment=1 INNER JOIN \"+dbschema+\".temp_tempcategory_\" + str(parameterID) + \" tc ON tc.categoryid = vt.CategoryID_fk GROUP BY PeriodType,Period,userglcode,CurrencyID_fk, \"+alldimcols+\" UNION  SELECT PeriodType,LEFT(convertedperiod, 6) AS Period, userglcode, '' as usergldescription, amount, CurrencyID_fk, 'Adjustments' source, \"+alldimcols+\" FROM \"+dbschema+\".temp_ladjjfinal_\" + str(parameterID) + \" WHERE ismappedglcode = 1 ) a GROUP BY PeriodType,Period, userglcode, usergldescription, currency, Source, \"+alldimcols+\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4f3c00d0-07d4-4748-976d-f91ed1eea511",
     "showTitle": true,
     "title": "Standalone FTP & YTD data processing"
    }
   },
   "outputs": [],
   "source": [
    "queryString=\"DROP TABLE IF EXISTS  {vSQLDB}.temp_mappedsdtbytd_\" + str(parameterID) + \";DROP TABLE IF EXISTS  {vSQLDB}.temp_mappedsdtbftp_\" + str(parameterID) + \";DROP TABLE IF EXISTS  {vSQLDB}.temp_mappedtbddftp_\" + str(parameterID) + \";DROP TABLE IF EXISTS  {vSQLDB}.temp_mappedtbddytd_\" + str(parameterID) + \";DROP TABLE IF EXISTS  {vSQLDB}.temp_mappedsdtbytddaterange_\" + str(parameterID) + \";DROP TABLE IF EXISTS  {vSQLDB}.temp_mappedtbddytddaterange_\" + str(parameterID) + \";DROP TABLE IF EXISTS  {vSQLDB}.temp_LegacyDatesPeriodTypeLite_\" + str(parameterID) + \";DROP TABLE IF EXISTS  {vSQLDB}.temp_LegacyDatesNotPeriodTypeLite_\" + str(parameterID) + \";DROP TABLE IF EXISTS  {vSQLDB}.temp_ISNoData_\" + str(parameterID) + \";\" \n",
    "\n",
    "queryString=queryString.replace(\"{vSQLDB}\",f\"{dbschema}\")\n",
    "\n",
    "sqlwarehouse_queryExecutor(queryString)\n",
    "\n",
    "spark.sql(\"MERGE INTO \"+dbschema+\".temp_tbfinal_\" + str(parameterID) + \" mb USING  \"+dbschema+\".temp_legacysddates_\" + str(parameterID) + \" ld ON  mb.period = ld.currentvalue AND mb.PeriodType = ld.periodtype WHEN MATCHED THEN UPDATE SET Period = ld.pcurrentvalue\")\n",
    "\n",
    "spark.sql(\"MERGE INTO \"+dbschema+\".temp_tbfinalytd_\" + str(parameterID) + \" mb USING  \"+dbschema+\".temp_ytddates_\" + str(parameterID) + \" yt ON   mb.period = LEFT(yt.ftpvalue, 6) AND    mb.PeriodType = yt.periodtype WHEN MATCHED THEN UPDATE SET  Period = yt.currentvalue\")\n",
    "\n",
    "spark.sql(\"MERGE INTO \"+dbschema+\".temp_tbfinaldd_\" + str(parameterID) + \" mb USING  \"+dbschema+\".temp_legacysddates_\" + str(parameterID) + \" ld ON mb.period = ld.currentvalue AND    mb.PeriodType = ld.periodtype WHEN MATCHED THEN UPDATE SET  Period = ld.pcurrentvalue\")\n",
    "\n",
    "spark.sql(\"MERGE INTO \"+dbschema+\".temp_tbfinalddytd_\" + str(parameterID) + \" mb USING  \"+dbschema+\".temp_ytddates_\" + str(parameterID) + \" yt ON   mb.period = LEFT(yt.ftpvalue, 6) and  mb.PeriodType = yt.periodtype WHEN MATCHED THEN UPDATE SET  Period = yt.currentvalue\")\n",
    "\n",
    "spark.sql(\"MERGE INTO \"+dbschema+\".temp_disclosures_\" + str(parameterID) + \" mb USING  \"+dbschema+\".temp_legacysddates_\" + str(parameterID) + \" ld ON  mb.period = ld.currentvalue AND   mb.PeriodType = ld.periodtype  WHEN MATCHED THEN UPDATE SET  Period = ld.pcurrentvalue\")\n",
    "\n",
    "spark.sql(\"MERGE INTO \"+dbschema+\".temp_disclosuresytd_\" + str(parameterID) + \" mb USING  \"+dbschema+\".temp_ytddates_\" + str(parameterID) + \" yt ON mb.period = LEFT(yt.ftpvalue, 6) AND    mb.PeriodType = yt.periodtype WHEN MATCHED THEN UPDATE SET  Period = yt.currentvalue\")\n",
    "\n",
    "spark.sql(\"CREATE TABLE \"+dbschema+\".temp_LegacyDatesPeriodTypeLite_\" + str(parameterID) + \"(pcurrentvalue VARCHAR(50),PeriodType VARCHAR(500))\")\n",
    "\n",
    "spark.sql(\"INSERT INTO \"+dbschema+\".temp_LegacyDatesPeriodTypeLite_\" + str(parameterID) + \" SELECT DISTINCT pcurrentvalue,PeriodType FROM \"+dbschema+\".temp_legacysddates_\" + str(parameterID) + \" WHERE PeriodType LIKE '%YTD' OR PeriodType LIKE '%Year'\")\n",
    "\n",
    "spark.sql(\"CREATE TABLE \"+dbschema+\".temp_LegacyDatesNotPeriodTypeLite_\" + str(parameterID) + \"(pcurrentvalue VARCHAR(50),PeriodType VARCHAR(500))\")\n",
    "\n",
    "spark.sql(\"INSERT INTO \"+dbschema+\".temp_LegacyDatesNotPeriodTypeLite_\" + str(parameterID) + \" SELECT DISTINCT pcurrentvalue,PeriodType FROM \"+dbschema+\".temp_legacysddates_\" + str(parameterID) + \" WHERE PeriodType NOT LIKE '%YTD' AND PeriodType NOT LIKE '%Year'\")\n",
    "\n",
    "### YTD ###\n",
    "\n",
    "spark.sql(\"CREATE TABLE \"+dbschema+\".temp_mappedsdtbytd_\" + str(parameterID) + \" AS SELECT tb.PeriodType, tb.Period AS Period, tb.userglcode AS UserGLCode, tb.currency, Sum(closingbalance) closingbalance, mt.categoryid AS categoryid, mt.eyglcode AS member, 'Y' ytdtype, \"+alldimcols+\"  FROM \"+dbschema+\".temp_tbfinalytd_\" + str(parameterID) + \" TB INNER JOIN \"+dbschema+\".temp_scoamappingtable_\" + str(parameterID) + \" MT ON tb.userglcode = mt.userglcode  JOIN \"+dbschema+\".temp_LegacyDatesPeriodTypeLite_\" + str(parameterID) + \" ld ON ld.pcurrentvalue = tb.Period AND ld.PeriodType = tb.PeriodType WHERE ultimateparent = 'IS_GROUP' GROUP BY tb.PeriodType, tb.Period, tb.userglcode, tb.currency, mt.categoryid, mt.eyglcode, \"+alldimcols+\" UNION  SELECT tb.PeriodType, tb.Period AS Period, tb.userglcode AS UserGLCode, tb.currency, Sum(closingbalance), mt.categoryid AS categoryid, mt.eyglcode AS member, 'Y' ytdtype, \"+alldimcols+\" FROM \"+dbschema+\".temp_tbfinal_\" + str(parameterID) + \" TB INNER JOIN \"+dbschema+\".temp_scoamappingtable_\" + str(parameterID) + \" MT ON tb.userglcode = mt.userglcode JOIN \"+dbschema+\".temp_LegacyDatesPeriodTypeLite_\" + str(parameterID) + \" ld ON ld.pcurrentvalue = tb.Period AND ld.PeriodType = tb.PeriodType WHERE ultimateparent = 'BS_GROUP' GROUP BY tb.PeriodType, tb.Period, tb.userglcode, tb.currency, mt.categoryid, mt.eyglcode, \"+alldimcols+\" UNION  SELECT PeriodType, Period, NULL AS UserGLCode, currencyid AS currency, Sum(amount) AS closingbalance, categoryid, member, 'Y', \"+alldimcols+\" FROM \"+dbschema+\".temp_disclosuresytd_\" + str(parameterID) + \" WHERE ultimateparent NOT IN ('BS_GROUP', 'IS_GROUP') AND column27 = 'FTP'  GROUP BY PeriodType, Period, currencyid, categoryid, member, ultimateparent, column27, \"+alldimcols+\" UNION  SELECT disc.PeriodType, Period, NULL AS UserGLCode, currencyid AS currency, Sum(amount) AS closingbalance, categoryid, member, 'Y', \"+alldimcols+\" FROM \"+dbschema+\".temp_disclosures_\" + str(parameterID) + \" disc JOIN \"+dbschema+\".temp_LegacyDatesPeriodTypeLite_\" + str(parameterID) + \" ld ON ld.pcurrentvalue = disc.Period AND ld.PeriodType = disc.PeriodType WHERE ultimateparent NOT IN ('BS_GROUP', 'IS_GROUP') AND column27 <> 'FTP' GROUP BY disc.PeriodType, Period, currencyid, categoryid, member, ultimateparent, column27, \"+alldimcols+\" \")\n",
    "\n",
    "### FTP ###\n",
    "\n",
    "spark.sql(\"CREATE TABLE \"+dbschema+\".temp_mappedsdtbftp_\" + str(parameterID) + \" AS SELECT tb.PeriodType, tb.Period AS Period, tb.userglcode AS UserGLCode, tb.currency, closingbalance, mt.categoryid AS categoryid, mt.eyglcode AS member, 'N' ytdtype, \"+alldimcols+\"  FROM \"+dbschema+\".temp_tbfinal_\" + str(parameterID) + \" TB INNER JOIN \"+dbschema+\".temp_scoamappingtable_\" + str(parameterID) + \" MT ON tb.userglcode = mt.userglcode JOIN \"+dbschema+\".temp_LegacyDatesNotPeriodTypeLite_\" + str(parameterID) + \" ld ON ld.pcurrentvalue = tb.Period AND ld.PeriodType = tb.PeriodType  UNION ALL SELECT d.PeriodType, Period, NULL AS UserGLCode, currencyid AS currency, amount AS closingbalance, categoryid, member, 'N', \"+alldimcols+\" FROM \"+dbschema+\".temp_disclosures_\" + str(parameterID) + \" d JOIN \"+dbschema+\".temp_LegacyDatesNotPeriodTypeLite_\" + str(parameterID) + \" ld ON ld.pcurrentvalue = d.Period AND ld.PeriodType = d.PeriodType  UNION ALL SELECT ld.PeriodType, LEFT(Period, 6) Period, NULL AS UserGLCode, currencyid_fk currencyid, amount AS closingbalance, tc.categoryid, '' member, 'N', \"+alldimcols+\" FROM \"+dbschema+\".temp_standaloneadj_\" + str(parameterID) + \" sd JOIN ( SELECT DISTINCT LegacyPeriod, PeriodType FROM \"+dbschema+\".temp_legacysddates_\" + str(parameterID) + \" where lower(periodtype) not like '%ytd' and   lower(periodtype) not like '%year' )ld ON sd.Period=ld.LegacyPeriod INNER JOIN \"+dbschema+\".temp_tempcategory_\" + str(parameterID) + \" tc ON tc.categoryid = sd.CategoryID_fk WHERE EntityID_fk = \"+str(EntityID)+\" AND AccountingStdID_fk = \"+str(AccountingStandardID)+\"  AND IsUserGLCodeAdjustment = 1 UNION ALL SELECT PeriodType, LEFT(convertedperiod, 6) AS Period, NULL userglcode, currencyid_fk currencyid, amount AS closingbalance, categoryid, '' member, 'N', \"+alldimcols+\" FROM \"+dbschema+\".temp_ladjjfinal_\" + str(parameterID) + \" WHERE ismappedglcode=1\")\n",
    "\n",
    "### drilldown YTD & FTP data ###\n",
    "### YTD ###\n",
    "\n",
    "spark.sql(\"CREATE TABLE \"+dbschema+\".temp_mappedtbddytd_\" + str(parameterID) + \" SELECT tb.PeriodType, tb.Period AS Period, tb.userglcode AS UserGLCode, tb.UserGLDescription AS UserGLDescription, Sum(closingbalance) closingbalance, mt.eyglcode AS member, tb.Source, 'Y' ytdtype, \"+alldimcols+\" FROM \"+dbschema+\".temp_tbfinalddytd_\" + str(parameterID) + \" TB INNER JOIN \"+dbschema+\".temp_scoamappingtable_\" + str(parameterID) + \" MT ON tb.userglcode = mt.userglcode JOIN ( SELECT pcurrentvalue, PeriodType FROM \"+dbschema+\".temp_legacysddates_\" + str(parameterID) + \" WHERE PeriodType LIKE '%YTD' OR PeriodType LIKE '%Year')ld ON ld.pcurrentvalue = tb.Period AND ld.PeriodType = tb.PeriodType WHERE ultimateparent = 'IS_GROUP' GROUP BY tb.PeriodType, tb.Period, tb.userglcode, tb.UserGLDescription, mt.eyglcode, tb.Source, \"+alldimcols+\" UNION SELECT tb.PeriodType, tb.Period AS Period, tb.userglcode AS UserGLCode, tb.UserGLDescription AS UserGLDescription, Sum(closingbalance) closingbalance, mt.eyglcode AS member, tb.Source, 'Y' ytdtype, \"+alldimcols+\" FROM \"+dbschema+\".temp_tbfinaldd_\" + str(parameterID) + \" TB INNER JOIN \"+dbschema+\".temp_scoamappingtable_\" + str(parameterID) + \" MT ON tb.userglcode = mt.userglcode JOIN ( SELECT pcurrentvalue, PeriodType FROM \"+dbschema+\".temp_legacysddates_\" + str(parameterID) + \" WHERE PeriodType LIKE '%YTD' OR PeriodType LIKE '%Year')ld ON ld.pcurrentvalue = tb.Period AND ld.PeriodType = tb.PeriodType WHERE ultimateparent = 'BS_GROUP' GROUP BY tb.PeriodType, tb.Period, tb.userglcode, tb.UserGLDescription, mt.eyglcode, tb.Source, \"+alldimcols+\" UNION SELECT PeriodType, Period, NULL AS UserGLCode, NULL AS UserGLDescription, Sum(amount) AS closingbalance, member, 'Disclosures', 'Y', \"+alldimcols+\" FROM \"+dbschema+\".temp_disclosuresytd_\" + str(parameterID) + \" WHERE ultimateparent NOT IN ('BS_GROUP', 'IS_GROUP') AND column27 = 'FTP' GROUP BY PeriodType, Period, member, ultimateparent, column27, \"+alldimcols+\" UNION SELECT disc.PeriodType, Period, NULL AS UserGLCode, NULL AS UserGLDescription, Sum(amount) AS closingbalance, member, 'Disclosures', 'Y', \"+alldimcols+\" FROM \"+dbschema+\".temp_disclosures_\" + str(parameterID) + \" disc JOIN ( SELECT pcurrentvalue, PeriodType FROM \"+dbschema+\".temp_legacysddates_\" + str(parameterID) + \" WHERE PeriodType LIKE '%YTD' OR PeriodType LIKE '%Year')ld ON ld.pcurrentvalue = disc.Period AND ld.PeriodType = disc.PeriodType WHERE ultimateparent NOT IN ('BS_GROUP', 'IS_GROUP') AND column27 <> 'FTP' GROUP BY disc.PeriodType, Period, member, ultimateparent, column27, \"+alldimcols+\" \")\n",
    "\n",
    "### FTP ###\n",
    "\n",
    "spark.sql(\" CREATE TABLE \"+dbschema+\".temp_mappedtbddftp_\" + str(parameterID) + \" SELECT tb.PeriodType, tb.Period AS Period, tb.userglcode AS UserGLCode, tb.UserGLDescription AS UserGLDescription, closingbalance, mt.eyglcode AS member, tb.Source, 'N' ytdtype, \"+alldimcols+\" FROM \"+dbschema+\".temp_tbfinaldd_\" + str(parameterID) + \" TB INNER JOIN \"+dbschema+\".temp_scoamappingtable_\" + str(parameterID) + \" MT ON tb.userglcode = mt.userglcode JOIN \"+dbschema+\".temp_LegacyDatesNotPeriodTypeLite_\" + str(parameterID) + \" ld ON ld.pcurrentvalue = tb.Period AND ld.PeriodType = tb.PeriodType UNION SELECT d.PeriodType, Period, NULL AS UserGLCode, NULL AS UserGLDescription, amount AS closingbalance, member, 'Disclosures', 'N', \"+alldimcols+\" FROM \"+dbschema+\".temp_disclosures_\" + str(parameterID) + \" d JOIN \"+dbschema+\".temp_LegacyDatesNotPeriodTypeLite_\" + str(parameterID) + \" ld ON ld.pcurrentvalue = d.Period AND ld.PeriodType = d.PeriodType UNION SELECT PeriodType, LEFT(Period, 6) Period, NULL AS UserGLCode, NULL AS UserGLDescription, amount AS closingbalance, '' member, 'Adjustments', 'N', \"+alldimcols+\" FROM \"+dbschema+\".temp_standaloneadj_\" + str(parameterID) + \" sd JOIN \"+dbschema+\".temp_legacysddates_\" + str(parameterID) + \" ld ON sd.Period=ld.LegacyPeriod and (lower(ld.periodtype) not like '%ytd' and   lower(ld.periodtype) not like '%year') INNER JOIN \"+dbschema+\".temp_tempcategory_\" + str(parameterID) + \" tc ON tc.categoryid = sd.CategoryID_fk WHERE EntityID_fk = \"+str(EntityID)+\" AND AccountingStdID_fk = \"+str(AccountingStandardID)+\" AND IsUserGLCodeAdjustment = 1 UNION SELECT PeriodType, LEFT(convertedperiod, 6) AS Period, NULL userglcode, NULL AS UserGLDescription, amount AS closingbalance, '' member, 'Adjustments', 'N', \"+alldimcols+\" FROM \"+dbschema+\".temp_ladjjfinal_\" + str(parameterID) + \" WHERE ismappedglcode=1\")\n",
    "\n",
    "if PeriodValueInYHQM == 0 :\n",
    "    spark.sql(\"UPDATE \"+dbschema+\".temp_mappedsdtbftp_\" + str(parameterID) + \" SET periodtype = REPLACE(periodtype, '_YTD', '')\")\n",
    "    spark.sql(\"UPDATE \"+dbschema+\".temp_mappedsdtbytd_\" + str(parameterID) + \" SET periodtype = REPLACE(periodtype, '_YTD', '')\")\n",
    "    spark.sql(\"UPDATE \"+dbschema+\".temp_mappedtbddftp_\" + str(parameterID) + \" SET periodtype = REPLACE(periodtype, '_YTD', '')\")\n",
    "    spark.sql(\"UPDATE \"+dbschema+\".temp_mappedtbddytd_\" + str(parameterID) + \" SET periodtype = REPLACE(periodtype, '_YTD', '')\")\n",
    "    spark.sql(\"UPDATE \"+dbschema+\".temp_legacysddates_\" + str(parameterID) + \" SET periodtype = REPLACE(periodtype, '_YTD', '')\")\n",
    "\n",
    "### IS Period having prev period data ###\n",
    "spark.sql(\"CREATE TABLE \"+dbschema+\".temp_ISNoData_\" + str(parameterID) + \" AS select lg.Periodtype,lg.legacyperiod from (select periodtype ,legacyperiod from \"+dbschema+\".temp_legacysddates_\" + str(parameterID) + \" where pcurrentvalue<>currentvalue)lg left join \"+dbschema+\".temp_ytddates_\" + str(parameterID) + \" yt on yt.FTPValue=lg.legacyperiod and yt.PeriodType=lg.periodtype where yt.Periodtype is null\")\n",
    "\n",
    "spark.sql(\"MERGE INTO \"+dbschema+\".temp_mappedsdtbftp_\" + str(parameterID) + \" as t USING ( SELECT mf.PeriodType FROM \"+dbschema+\".temp_mappedsdtbftp_\" + str(parameterID) + \" mf JOIN \"+dbschema+\".temp_SCOAMappingTable_\" + str(parameterID) + \" sc ON sc.EYGLCode = mf.member JOIN \"+dbschema+\".temp_ISNoData_\" + str(parameterID) + \" nd ON mf.PeriodType = nd.periodtype WHERE sc.UltimateParent = 'IS_GROUP' ) AS s ON t.PeriodType = s.PeriodType WHEN MATCHED THEN DELETE\")\n",
    "\n",
    "spark.sql(\"MERGE INTO \"+dbschema+\".temp_mappedtbddftp_\" + str(parameterID) + \" as t USING ( SELECT mf.PeriodType FROM \"+dbschema+\".temp_mappedtbddftp_\" + str(parameterID) + \" mf JOIN \"+dbschema+\".temp_SCOAMappingTable_\" + str(parameterID) + \" sc ON sc.EYGLCode = mf.member JOIN \"+dbschema+\".temp_ISNoData_\" + str(parameterID) + \" nd ON mf.PeriodType = nd.periodtype WHERE sc.UltimateParent = 'IS_GROUP' ) AS s ON t.PeriodType = s.PeriodType WHEN MATCHED THEN DELETE\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4aa5d0ff-6cac-42e7-99d2-b1474b851be7",
     "showTitle": true,
     "title": "Allperioddata"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "spark.sql(\"DROP TABLE IF EXISTS \"+dbschema+\".\"+temp_staging_tbl+\" \")\n",
    "spark.sql(\"DROP TABLE IF EXISTS \"+dbschema+\".\"+temp_drilldown_tbl+\" \")\n",
    "\n",
    "spark.sql(\"CREATE TABLE  \"+dbschema+\".\"+temp_staging_tbl+\" ( id INT DEFAULT NULL, PeriodType VARCHAR(50) DEFAULT NULL, Period VARCHAR(50) DEFAULT NULL, userglcode VARCHAR(100) DEFAULT NULL, currency INT DEFAULT NULL, closingbalance DECIMAL(28, 8) DEFAULT NULL, categoryid INT DEFAULT NULL, member VARCHAR(1000) DEFAULT NULL, ytdtype CHAR(10) DEFAULT NULL, usergldescription VARCHAR(200) DEFAULT NULL, Source VARCHAR(50) DEFAULT NULL, Value VARCHAR(5) DEFAULT NULL, pcurrentvalue VARCHAR(50) DEFAULT NULL, legacyperiod VARCHAR(20) DEFAULT NULL, financialyear VARCHAR(10) DEFAULT NULL, openingperiodvalue VARCHAR(20) DEFAULT NULL, castingperiod VARCHAR(20) DEFAULT NULL, p VARCHAR(20) DEFAULT NULL, ppp VARCHAR(20) DEFAULT NULL, py VARCHAR(20) DEFAULT NULL, pypp VARCHAR(20) DEFAULT NULL, ppy VARCHAR(20) DEFAULT NULL, p3y VARCHAR(20) DEFAULT NULL, p4y VARCHAR(20) DEFAULT NULL, CastingAdjustmentsID BIGINT GENERATED ALWAYS AS IDENTITY (START WITH 1 INCREMENT BY 1), Fk_AccountingStandardID INT, Fk_ValidationID INT DEFAULT NULL, PeriodValue VARCHAR(30), Fk_EntityID INT, EntityName VARCHAR(50), JournalNumber INT, JournalDate DATE DEFAULT NULL, StandardGLCode VARCHAR(100) DEFAULT NULL, StandardGLDescription VARCHAR(250) DEFAULT NULL, DebitCredit VARCHAR(50) DEFAULT NULL, Fk_CurrencyID INT, Amount DECIMAL(18, 4) DEFAULT NULL, JournalType VARCHAR(50) DEFAULT NULL, Narration VARCHAR(250) DEFAULT NULL, Fk_BusinessUnitID INT DEFAULT NULL, Roundoff INT DEFAULT NULL, AmountsIn VARCHAR(30) DEFAULT NULL, ReportType VARCHAR(60) DEFAULT NULL, CreatedDate DATE DEFAULT NULL, CreatedBy VARCHAR(250) DEFAULT NULL, ModifiedDate DATE DEFAULT NULL, ModifiedBy VARCHAR(250) DEFAULT NULL, IsUserGLCode INT DEFAULT NULL, RoundTypeLevel VARCHAR(20) DEFAULT NULL, typeofdata INT, \"+alldimcols_dt+\" ) TBLPROPERTIES('delta.feature.allowColumnDefaults' = 'supported') \")\n",
    "\n",
    "spark.sql(\"CREATE TABLE  \"+dbschema+\".\"+temp_drilldown_tbl+\" ( id INT DEFAULT NULL, PeriodType VARCHAR(50) DEFAULT NULL, Period VARCHAR(50) DEFAULT NULL, userglcode VARCHAR(100) DEFAULT NULL, currency INT DEFAULT NULL, closingbalance DECIMAL(28, 8) DEFAULT NULL, categoryid INT DEFAULT NULL, member VARCHAR(1000) DEFAULT NULL, ytdtype CHAR(10) DEFAULT NULL, usergldescription VARCHAR(200) DEFAULT NULL, Source VARCHAR(50) DEFAULT NULL, Value VARCHAR(5) DEFAULT NULL, pcurrentvalue VARCHAR(50) DEFAULT NULL, legacyperiod VARCHAR(20) DEFAULT NULL, financialyear VARCHAR(10) DEFAULT NULL, openingperiodvalue VARCHAR(20) DEFAULT NULL, castingperiod VARCHAR(20) DEFAULT NULL, p VARCHAR(20) DEFAULT NULL, ppp VARCHAR(20) DEFAULT NULL, py VARCHAR(20) DEFAULT NULL, pypp VARCHAR(20) DEFAULT NULL, ppy VARCHAR(20) DEFAULT NULL, p3y VARCHAR(20) DEFAULT NULL, p4y VARCHAR(20) DEFAULT NULL, CastingAdjustmentsID BIGINT GENERATED ALWAYS AS IDENTITY (START WITH 1 INCREMENT BY 1), Fk_AccountingStandardID INT, Fk_ValidationID INT DEFAULT NULL, PeriodValue VARCHAR(30), Fk_EntityID INT, EntityName VARCHAR(50), JournalNumber INT, JournalDate DATE DEFAULT NULL, StandardGLCode VARCHAR(100) DEFAULT NULL, StandardGLDescription VARCHAR(250) DEFAULT NULL, DebitCredit VARCHAR(50) DEFAULT NULL, Fk_CurrencyID INT, Amount DECIMAL(18, 4) DEFAULT NULL, JournalType VARCHAR(50) DEFAULT NULL, Narration VARCHAR(250) DEFAULT NULL, Fk_BusinessUnitID INT DEFAULT NULL, Roundoff INT DEFAULT NULL, AmountsIn VARCHAR(30) DEFAULT NULL, ReportType VARCHAR(60) DEFAULT NULL, CreatedDate DATE DEFAULT NULL, CreatedBy VARCHAR(250) DEFAULT NULL, ModifiedDate DATE DEFAULT NULL, ModifiedBy VARCHAR(250) DEFAULT NULL, IsUserGLCode INT DEFAULT NULL, RoundTypeLevel VARCHAR(20) DEFAULT NULL, typeofdata INT, \"+alldimcols_dt+\" ) TBLPROPERTIES('delta.feature.allowColumnDefaults' = 'supported') \")\n",
    "\n",
    "spark.sql(\"INSERT INTO \"+dbschema+\".\"+temp_staging_tbl+\" ( typeofdata, id, PeriodType, Value, pcurrentvalue, legacyperiod, categoryid , financialyear, openingperiodvalue, castingperiod, p, ppp, py, pypp, ppy, p3y, p4y ) SELECT 1, id, PeriodType, Value, pcurrentvalue, legacyperiod, categoryid, financialyear, openingperiodvalue, castingperiod, p, ppp, py, pypp, ppy, p3y, p4y FROM \"+dbschema+\".temp_legacysddates_\" + str(parameterID) + \"\")\n",
    "\n",
    "spark.sql(\"INSERT INTO \"+dbschema+\".\"+temp_drilldown_tbl+\" ( typeofdata, id, PeriodType, Value, pcurrentvalue, legacyperiod, categoryid , financialyear, openingperiodvalue, castingperiod, p, ppp, py, pypp, ppy, p3y, p4y ) SELECT 1, id, PeriodType, Value, pcurrentvalue, legacyperiod, categoryid, financialyear, openingperiodvalue, castingperiod, p, ppp, py, pypp, ppy, p3y, p4y FROM \"+dbschema+\".temp_legacysddates_\" + str(parameterID) + \"\")\n",
    "\n",
    "spark.sql(\"INSERT INTO \"+dbschema+\".\"+temp_staging_tbl+\" ( typeofdata,id,Fk_AccountingStandardID,Fk_ValidationID,PeriodValue,FinancialYear,Fk_EntityID ,EntityName,JournalNumber,JournalDate,UserGLCode,UserGLDescription,StandardGLCode,StandardGLDescription,DebitCredit,Fk_CurrencyID,Amount,JournalType,Narration,Fk_BusinessUnitID,CategoryID,Roundoff,AmountsIn,ReportType,CreatedDate,CreatedBy,ModifiedDate,ModifiedBy,IsUserGLCode,RoundTypeLevel,\"+alldimcols+\" ) SELECT DISTINCT 2,castingadjustmentsid,fk_accountingstandardid,fk_validationid,periodvalue,financialyear,fk_entityid,entityname,journalnumber,journaldate,userglcode,usergldescription,standardglcode,standardgldescription,debitcredit,fk_currencyid,amount,journaltype,narration,fk_businessunitid,categoryid,roundoff,amountsin,reporttype,createddate,createdby,modifieddate,modifiedby,isuserglcode,roundtypelevel,\"+alldimcols+\" FROM  \"+dbschema+\".temp_castingsdadjustment1_\" + str(parameterID) + \"\")\n",
    "\n",
    "spark.sql(\"INSERT INTO \"+dbschema+\".\"+temp_drilldown_tbl+\" ( typeofdata,id,Fk_AccountingStandardID,Fk_ValidationID,PeriodValue,FinancialYear,Fk_EntityID ,EntityName,JournalNumber,JournalDate,UserGLCode,UserGLDescription,StandardGLCode,StandardGLDescription,DebitCredit,Fk_CurrencyID,Amount,JournalType,Narration,Fk_BusinessUnitID,CategoryID,Roundoff,AmountsIn,ReportType,CreatedDate,CreatedBy,ModifiedDate,ModifiedBy,IsUserGLCode,RoundTypeLevel,\"+alldimcols+\" ) SELECT DISTINCT 2,castingadjustmentsid,fk_accountingstandardid,fk_validationid,periodvalue,financialyear,fk_entityid,entityname,journalnumber,journaldate,userglcode,usergldescription,standardglcode,standardgldescription,debitcredit,fk_currencyid,amount,journaltype,narration,fk_businessunitid,categoryid,roundoff,amountsin,reporttype,createddate,createdby,modifieddate,modifiedby,isuserglcode,roundtypelevel,\"+alldimcols+\" FROM  \"+dbschema+\".temp_castingsdadjustment1_\" + str(parameterID) + \"\")\n",
    "\n",
    "df_tbl_count_01=spark.sql(\"SELECT * FROM system.information_schema.tables where lower(concat(table_catalog,'.',table_schema))=lower('\"+dbschema+\"') and table_name in ('temp_mappedsdtbftp_\" + str(parameterID) + \"','temp_mappedsdtbytd_\" + str(parameterID) + \"')\")\n",
    "df_tbl_count_02=spark.sql(\"SELECT * FROM system.information_schema.tables where lower(concat(table_catalog,'.',table_schema))=lower('\"+dbschema+\"') and table_name in ('temp_mappedtbddftp_\" + str(parameterID) + \"','temp_mappedtbddytd_\" + str(parameterID) + \"')\")\n",
    "\n",
    "if df_tbl_count_01.count() == 2 :\n",
    "    spark.sql(\"INSERT INTO \"+dbschema+\".\"+temp_staging_tbl+\" ( typeofdata, PeriodType, Period, userglcode, currency, closingbalance, categoryid, member, ytdtype, \"+alldimcols+\" ) SELECT 0, PeriodType, Period, userglcode, currency, closingbalance, categoryid, member, ytdtype,\"+alldimcols+\" FROM \"+dbschema+\".temp_mappedsdtbftp_\" + str(parameterID) + \" UNION SELECT 0, PeriodType, Period, userglcode, currency, closingbalance, categoryid, member, ytdtype, \"+alldimcols+\" FROM \"+dbschema+\".temp_mappedsdtbytd_\" + str(parameterID) + \" \")\n",
    "\n",
    "if df_tbl_count_02.count() == 2 :\n",
    "    spark.sql(\"INSERT INTO \"+dbschema+\".\"+temp_drilldown_tbl+\" ( typeofdata, PeriodType, Period, userglcode, usergldescription, closingbalance, member, source, ytdtype, \"+alldimcols+\" ) SELECT 0, PeriodType, Period, userglcode, usergldescription, closingbalance, member, source, ytdtype,\"+alldimcols+\" FROM \"+dbschema+\".temp_mappedtbddftp_\" + str(parameterID) + \" UNION SELECT 0, PeriodType, Period, userglcode, usergldescription, closingbalance, member, source, ytdtype, \"+alldimcols+\" FROM \"+dbschema+\".temp_mappedtbddytd_\" + str(parameterID) + \"\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "GetMultiPeriodAllSourceData",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
