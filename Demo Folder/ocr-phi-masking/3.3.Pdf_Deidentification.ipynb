{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e62bdefe-1301-40b7-b310-c662283c45f7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from johnsnowlabs import nlp, visual, medical\n",
    "\n",
    "spark = nlp.start(visual=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ea5a66d3-adcf-4d71-9bcb-7247690cf6be",
     "showTitle": false,
     "title": ""
    },
    "id": "c2f9a3b6"
   },
   "source": [
    "![JohnSnowLabs](https://nlp.johnsnowlabs.com/assets/images/logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "106c3509-eb7b-421f-b3e9-39b9462240df",
     "showTitle": false,
     "title": ""
    },
    "collapsed": false,
    "id": "f8cb0c2ac214dc7"
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/visual-nlp/3.3.Pdf_Deidentification.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1efef366-d663-42ff-87f6-49dec0fee9da",
     "showTitle": false,
     "title": ""
    },
    "id": "b91c2868"
   },
   "source": [
    "## Blogposts and videos\n",
    "\n",
    "- [Text Detection in Spark OCR](https://medium.com/spark-nlp/text-detection-in-spark-ocr-dcd8002bdc97)\n",
    "\n",
    "- [Table Detection & Extraction in Spark OCR](https://medium.com/spark-nlp/table-detection-extraction-in-spark-ocr-50765c6cedc9)\n",
    "\n",
    "- [Extract Tabular Data from PDF in Spark OCR](https://medium.com/spark-nlp/extract-tabular-data-from-pdf-in-spark-ocr-b02136bc0fcb)\n",
    "\n",
    "- [Signature Detection in Spark OCR](https://medium.com/spark-nlp/signature-detection-in-spark-ocr-32f9e6f91e3c)\n",
    "\n",
    "- [GPU image pre-processing in Spark OCR](https://medium.com/spark-nlp/gpu-image-pre-processing-in-spark-ocr-3-1-0-6fc27560a9bb)\n",
    "\n",
    "- [How to Setup Spark OCR on UBUNTU - Video](https://www.youtube.com/watch?v=cmt4WIcL0nI)\n",
    "\n",
    "\n",
    "**More examples here**\n",
    "\n",
    "https://github.com/JohnSnowLabs/spark-ocr-workshop\n",
    "\n",
    "For get the trial license please go to:\n",
    "\n",
    "https://www.johnsnowlabs.com/install/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ff632f4b-a935-452c-887d-d4963e724be5",
     "showTitle": false,
     "title": ""
    },
    "id": "kugCG0c5Ez81"
   },
   "source": [
    "### Colab Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03553e56-cbc4-495e-90a5-36c31428f739",
     "showTitle": false,
     "title": ""
    },
    "id": "rsiwo3AbE2dD"
   },
   "outputs": [],
   "source": [
    "# Install the johnsnowlabs library to access Spark-OCR and Spark-NLP for Healthcare, Finance, and Legal.\n",
    "!pip install -q johnsnowlabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95c8cc22-8868-4b1e-acad-79adc60896e4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install google-colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e892c24c-7b8e-48aa-9f6e-3a22939f82f2",
     "showTitle": false,
     "title": ""
    },
    "id": "dss3UzbNE6xN"
   },
   "outputs": [],
   "source": [
    "print('Please Upload your John Snow Labs License using the button below')\n",
    "license_keys = \"/dbfs/Volumes/yash_gupta_hackerspace/ingestorinator/archimedes/spark_nlp_for_healthcare_spark_ocr_9379.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "152b70e0-59e1-4bdf-9014-7e6c9429a9de",
     "showTitle": false,
     "title": ""
    },
    "id": "ZHraeqXwFmdD"
   },
   "outputs": [],
   "source": [
    "from johnsnowlabs import nlp, visual\n",
    "\n",
    "# After uploading your license run this to install all licensed Python Wheels and pre-download Jars the Spark Session JVM\n",
    "nlp.install(refresh_install=True, visual=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fc50ed1c-2202-40db-bbf5-cea48bd2f5e9",
     "showTitle": false,
     "title": ""
    },
    "id": "EdEI65a1K8-B"
   },
   "source": [
    "## Start Visual NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a044a03a-846c-471b-84d3-91375c022616",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_jksffwsFtgJ",
    "outputId": "b652190f-1a3d-4ede-a1f4-78ee78464fa8"
   },
   "outputs": [],
   "source": [
    "from johnsnowlabs import nlp, visual, medical\n",
    "\n",
    "# Automatically load license data and start a session with all jars user has access to\n",
    "spark = nlp.start(visual=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e504227-a466-4ef6-8abb-6dbaa7b988f3",
     "showTitle": false,
     "title": ""
    },
    "id": "f11b97d3"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import PipelineModel, Pipeline\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4139bbb4-3c85-4477-babe-fbb2327e332c",
     "showTitle": false,
     "title": ""
    },
    "id": "9d693534"
   },
   "source": [
    "## Define de-identification  NLP pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d069d7a3-306b-4782-a303-37030e465ff4",
     "showTitle": false,
     "title": ""
    },
    "id": "f002ffcc"
   },
   "outputs": [],
   "source": [
    "def deidentification_nlp_pipeline(input_column, prefix = \"\", model=\"ner_deid_large\"):\n",
    "    document_assembler = nlp.DocumentAssembler() \\\n",
    "        .setInputCol(input_column) \\\n",
    "        .setOutputCol(prefix + \"document_raw\")\n",
    "\n",
    "    cleanUpPatterns = [\"<[^>]>\"]\n",
    "    documentNormalizer = nlp.DocumentNormalizer() \\\n",
    "      .setInputCols(prefix + \"document_raw\") \\\n",
    "      .setOutputCol(prefix + \"document\") \\\n",
    "      .setAction(\"clean\") \\\n",
    "      .setPatterns(cleanUpPatterns) \\\n",
    "      .setReplacement(\" \") \\\n",
    "      .setPolicy(\"pretty_all\")\n",
    "\n",
    "    # Sentence Detector annotator, processes various sentences per line\n",
    "    sentence_detector = nlp.SentenceDetector() \\\n",
    "        .setInputCols([prefix + \"document\"]) \\\n",
    "        .setOutputCol(prefix + \"sentence\")\n",
    "\n",
    "    tokenizer = nlp.Tokenizer() \\\n",
    "        .setInputCols([prefix + \"sentence\"]) \\\n",
    "        .setOutputCol(prefix + \"token\")\n",
    "\n",
    "    # Clinical word embeddings\n",
    "    word_embeddings = nlp.WordEmbeddingsModel.pretrained(\"embeddings_clinical\", \"en\", \"clinical/models\") \\\n",
    "        .setInputCols([prefix + \"sentence\", prefix + \"token\"]) \\\n",
    "        .setOutputCol(prefix + \"embeddings\") \\\n",
    "        .setEnableInMemoryStorage(True)\n",
    "\n",
    "    clinical_ner = medical.NerModel.pretrained(model, \"en\", \"clinical/models\") \\\n",
    "        .setInputCols([prefix + \"sentence\", prefix + \"token\", prefix + \"embeddings\"]) \\\n",
    "        .setOutputCol(prefix + \"ner\")\n",
    "\n",
    "    custom_ner_converter = nlp.NerConverter() \\\n",
    "        .setInputCols([prefix + \"sentence\", prefix + \"token\", prefix + \"ner\"]) \\\n",
    "        .setOutputCol(prefix + \"ner_chunk\") \\\n",
    "        .setWhiteList(['NAME', 'AGE', 'CONTACT', 'ID',\n",
    "                   'LOCATION', 'PROFESSION', 'PERSON', 'DATE', 'DOCTOR'])\n",
    "\n",
    "    nlp_pipeline = Pipeline(stages=[\n",
    "            document_assembler,\n",
    "            documentNormalizer,\n",
    "            sentence_detector,\n",
    "            tokenizer,\n",
    "            word_embeddings,\n",
    "            clinical_ner,\n",
    "            custom_ner_converter\n",
    "        ])\n",
    "    empty_data = spark.createDataFrame([[\"\"]]).toDF(input_column)\n",
    "    nlp_model = nlp_pipeline.fit(empty_data)\n",
    "    return nlp_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd69ef2c-4d63-42b0-b6df-f714eb944bda",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "input_column=\"text\"\n",
    "prefix=\"\"\n",
    "model=\"ner_deid_generic_augmented\"\n",
    "\n",
    "document_assembler = nlp.DocumentAssembler() \\\n",
    "    .setInputCol(input_column) \\\n",
    "    .setOutputCol(prefix + \"document_raw\")\n",
    "\n",
    "cleanUpPatterns = [\"<[^>]>\"]\n",
    "documentNormalizer = nlp.DocumentNormalizer() \\\n",
    "  .setInputCols(prefix + \"document_raw\") \\\n",
    "  .setOutputCol(prefix + \"document\") \\\n",
    "  .setAction(\"clean\") \\\n",
    "  .setPatterns(cleanUpPatterns) \\\n",
    "  .setReplacement(\" \") \\\n",
    "  .setPolicy(\"pretty_all\")\n",
    "\n",
    "# Sentence Detector annotator, processes various sentences per line\n",
    "sentence_detector = nlp.SentenceDetector() \\\n",
    "    .setInputCols([prefix + \"document\"]) \\\n",
    "    .setOutputCol(prefix + \"sentence\")\n",
    "\n",
    "tokenizer = nlp.Tokenizer() \\\n",
    "    .setInputCols([prefix + \"sentence\"]) \\\n",
    "    .setOutputCol(prefix + \"token\")\n",
    "\n",
    "# Clinical word embeddings\n",
    "word_embeddings = nlp.WordEmbeddingsModel.pretrained(\"embeddings_clinical\", \"en\", \"clinical/models\") \\\n",
    "    .setInputCols([prefix + \"sentence\", prefix + \"token\"]) \\\n",
    "    .setOutputCol(prefix + \"embeddings\") \\\n",
    "    .setEnableInMemoryStorage(True)\n",
    "\n",
    "clinical_ner = medical.NerModel.pretrained(model, \"en\", \"clinical/models\") \\\n",
    "    .setInputCols([prefix + \"sentence\", prefix + \"token\", prefix + \"embeddings\"]) \\\n",
    "    .setOutputCol(prefix + \"ner\")\n",
    "\n",
    "custom_ner_converter = nlp.NerConverter() \\\n",
    "    .setInputCols([prefix + \"sentence\", prefix + \"token\", prefix + \"ner\"]) \\\n",
    "    .setOutputCol(prefix + \"ner_chunk\") \\\n",
    "    .setWhiteList(['NAME', 'AGE', 'CONTACT', 'ID',\n",
    "               'LOCATION', 'PROFESSION', 'PERSON', 'DATE', 'DOCTOR'])\n",
    "\n",
    "nlp_pipeline = Pipeline(stages=[\n",
    "        document_assembler,\n",
    "        documentNormalizer,\n",
    "        sentence_detector,\n",
    "        tokenizer,\n",
    "        word_embeddings,\n",
    "        clinical_ner,\n",
    "        custom_ner_converter\n",
    "    ])\n",
    "empty_data = spark.createDataFrame([[\"\"]]).toDF(input_column)\n",
    "nlp_model = nlp_pipeline.fit(empty_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "98a8a96b-0f2c-4842-b485-92b12e34faab",
     "showTitle": false,
     "title": ""
    },
    "id": "6434dd05"
   },
   "source": [
    "## Define OCR transformers and pipeline for image deidentification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "410651b4-e885-4f30-bb49-4f82590bcba2",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a7814858",
    "outputId": "17e3051f-2028-4d78-f926-c51196a295d7"
   },
   "outputs": [],
   "source": [
    "from sparkocr.transformers import *\n",
    "\n",
    "pdf_to_image = visual.PdfToImage() \\\n",
    "    .setInputCol(\"content\") \\\n",
    "    .setOutputCol(\"image_raw\") \\\n",
    "    .setPartitionNum(16)\\\n",
    "    .setSplitNumBatch(2)\\\n",
    "    .setPartitionNumAfterSplit(2) \\\n",
    "    .setSplittingStategy(visual.SplittingStrategy.FIXED_NUMBER_OF_PARTITIONS) \\\n",
    "    .setKeepInput(False)\n",
    "\n",
    "ocr = visual.ImageToText() \\\n",
    "    .setInputCol(\"image_raw\") \\\n",
    "    .setOutputCol(\"text\") \\\n",
    "    .setIgnoreResolution(False) \\\n",
    "    .setPageIteratorLevel(visual.PageIteratorLevel.SYMBOL) \\\n",
    "    .setPageSegMode(visual.PageSegmentationMode.SPARSE_TEXT) \\\n",
    "    .setConfidenceThreshold(70)\n",
    "\n",
    "# Found coordinates of sensitive data\n",
    "position_finder = visual.PositionFinder() \\\n",
    "    .setInputCols([\"ner_chunk\"]) \\\n",
    "    .setOutputCol(\"regions\") \\\n",
    "    .setPageMatrixCol(\"positions\") \\\n",
    "    .setIgnoreSchema(True) \\\n",
    "    .setOcrScaleFactor(1.0)\n",
    "\n",
    "#Draw filled rectangle for hide sensitive data\n",
    "draw_regions = visual.ImageDrawRegions() \\\n",
    "    .setInputCol(\"image_raw\") \\\n",
    "    .setInputRegionsCol(\"regions\") \\\n",
    "    .setOutputCol(\"cleaned_images\") \\\n",
    "    .setFilledRect(True) \\\n",
    "    .setRotated(False)\n",
    "\n",
    "image_to_pdf = visual.ImageToPdf() \\\n",
    "    .setInputCol(\"cleaned_images\") \\\n",
    "    .setOutputCol(\"pdf\")\n",
    "\n",
    "deidentification_nlp_pipeline(input_column=\"text\", prefix=\"\", model=\"ner_deid_generic_augmented\")\n",
    "\n",
    "# OCR pipeline\n",
    "# pipeline = PipelineModel(stages=[\n",
    "#     pdf_to_image,\n",
    "#     ocr,\n",
    "#     deidentification_nlp_pipeline(input_column=\"text\", prefix=\"\", model=\"ner_deid_generic_augmented\"),\n",
    "#     position_finder,\n",
    "#     draw_regions\n",
    "# ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d3210182-826a-4f09-bac3-b168a40a7e85",
     "showTitle": false,
     "title": ""
    },
    "id": "cffda3aa"
   },
   "source": [
    "## Read PDF file and display it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f75fd8ae-fc41-4fba-bbdf-665e00621649",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "6232403d",
    "outputId": "e835734e-501d-4512-ef4d-ee16b20addda"
   },
   "outputs": [],
   "source": [
    "pdf_path = visual.pkg_resources.resource_filename('sparkocr', 'resources/ocr/pdfs/test_document.pdf')\n",
    "\n",
    "pdf_df = spark.read.format(\"binaryFile\").load(pdf_path).cache()\n",
    "\n",
    "visual.display_pdf(pdf_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "786d5f38-ced4-4689-920a-4657c3d00aaf",
     "showTitle": false,
     "title": ""
    },
    "id": "a7c95233-3557-47f4-b81b-5bd45d24eb70"
   },
   "source": [
    "## Run de-id pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2cc91b8f-66ed-4d4b-90fa-4b5ac9054535",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d1d56634-9542-4280-8e8d-d8272808a158",
    "outputId": "91237a39-0a81-46fd-883a-41976c8d1f5d"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "OUTPUT_PATH = \"./de-id/\"\n",
    "\n",
    "def get_name(path, keep_subfolder_level=0):\n",
    "    path = path.split(\"/\")\n",
    "    path[-1] = \".\".join(path[-1].split('.')[:-1])\n",
    "    return \"/\".join(path[-keep_subfolder_level-1:])\n",
    "\n",
    "\n",
    "pages = pipeline.transform(pdf_df) \\\n",
    "    .cache() \\\n",
    "    .orderBy(\"pagenum\")\n",
    "\n",
    "image_to_pdf.transform(pages) \\\n",
    "    .withColumn(\"fileName\", F.udf(get_name, StringType())(F.col(\"path\"))) \\\n",
    "    .write \\\n",
    "    .format(\"binaryFormat\") \\\n",
    "    .option(\"type\", \"pdf\") \\\n",
    "    .option(\"field\", \"pdf\") \\\n",
    "    .option(\"nameField\", \"fileName\") \\\n",
    "    .option(\"extension\", \"pdf\") \\\n",
    "    .option(\"prefix\", \"\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .save(OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6b40c417-3d85-4cfc-b77b-109965255996",
     "showTitle": false,
     "title": ""
    },
    "id": "f17fd76c-6f01-4d6b-a05c-73777629b17a"
   },
   "source": [
    "## Check results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0eeaed40-055c-4e1a-a78b-eaba4644eaec",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6facc328-7b3c-477d-9d4a-028885e0be45",
    "outputId": "afa0c59a-4154-4a4b-a7dc-8e61e1494ea1"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "ls ./de-id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e8c54f9e-233f-45a6-8f91-ec4ecb5960ea",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "ea5962ce-324f-46b9-b65e-c86e54f8f9b2",
    "outputId": "e3b60122-655b-44bf-f9e9-8665d07078cd"
   },
   "outputs": [],
   "source": [
    "result_pdf_df = spark.read.format(\"binaryFile\").load(\"./de-id/test_document.pdf\")\n",
    "visual.display_pdf(result_pdf_df)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "3.3.Pdf_Deidentification",
   "widgets": {}
  },
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
